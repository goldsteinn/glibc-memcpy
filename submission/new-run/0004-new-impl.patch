From 45afa848b2cdae16a282155d452a046acb870e62 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Sat, 21 Aug 2021 00:59:55 -0400
Subject: [PATCH 4/5] new impl

---
 sysdeps/x86/sysdep.h                          |   6 +-
 .../x86_64/multiarch/memmove-vec-unaligned.S  | 615 +++++++++++-------
 2 files changed, 375 insertions(+), 246 deletions(-)

diff --git a/sysdeps/x86/sysdep.h b/sysdeps/x86/sysdep.h
index cac1d762fb..ef997f980f 100644
--- a/sysdeps/x86/sysdep.h
+++ b/sysdeps/x86/sysdep.h
@@ -78,15 +78,17 @@ enum cf_protection_level
 #define ASM_SIZE_DIRECTIVE(name) .size name,.-name;
 
 /* Define an entry point visible from C.  */
-#define	ENTRY(name)							      \
+#define	P2ALIGN_ENTRY(name, alignment)                              \
   .globl C_SYMBOL_NAME(name);						      \
   .type C_SYMBOL_NAME(name),@function;					      \
-  .align ALIGNARG(4);							      \
+  .align ALIGNARG(alignment);							      \
   C_LABEL(name)								      \
   cfi_startproc;							      \
   _CET_ENDBR;								      \
   CALL_MCOUNT
 
+#define	ENTRY(name) P2ALIGN_ENTRY(name, 4)
+
 #undef	END
 #define END(name)							      \
   cfi_endproc;								      \
diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S
index 51c890a6ce..d3944f44d7 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S
@@ -60,6 +60,33 @@
 # define MEMMOVE_CHK_SYMBOL(p,s)	MEMMOVE_SYMBOL(p,	s)
 #endif
 
+
+#define ALIGN_MOVSB	1
+#define ALIGN_MOVSB_EXCLUSIVE	0
+#define LOOP_ALIGN_CONF	64
+#define MOVSB_ALIGN_CONF	64
+
+
+#if VEC_SIZE < LOOP_ALIGN_CONF
+# define LOOP_4X_ALIGN_TO	(VEC_SIZE	*	2)
+#else
+# define LOOP_4X_ALIGN_TO	(VEC_SIZE)
+#endif
+	/* Avoid short distance rep movsb only with non-SSE vector.  */
+#if ALIGN_MOVSB
+# if VEC_SIZE < MOVSB_ALIGN_CONF
+#  define MOVSB_ALIGN_TO	(VEC_SIZE	*	2)
+# else
+#  define MOVSB_ALIGN_TO	(VEC_SIZE)
+# endif
+#else
+# define MOVSB_ALIGN_TO	0
+#endif
+#if MOVSB_ALIGN_TO > LOOP_4X_ALIGN_TO
+# error "Invalid MOVSB Alignment"
+#endif
+
+
 #ifndef XMM0
 # define XMM0	xmm0
 #endif
@@ -165,6 +192,19 @@
 # error Invalid LARGE_LOAD_SIZE
 #endif
 
+#define COPY_BLOCK(mov_inst,	src_reg,	dst_reg,	size_reg,	len,	tmp_reg0,	tmp_reg1)	\
+	mov_inst (%src_reg), %tmp_reg0; \
+	mov_inst -(len)(%src_reg, %size_reg), %tmp_reg1; \
+	mov_inst %tmp_reg0, (%dst_reg); \
+	mov_inst %tmp_reg1, -(len)(%dst_reg, %size_reg);
+
+
+#define COPY_4_8	COPY_BLOCK(movl,	rsi,	rdi,	rdx,	4,	ecx,	esi)
+#define COPY_8_16	COPY_BLOCK(movq,	rsi,	rdi,	rdx,	8,	rcx,	rsi)
+#define COPY_16_32	COPY_BLOCK(vmovdqu,	rsi,	rdi,	rdx,	16,	xmm0,	xmm1)
+#define COPY_32_64	COPY_BLOCK(vmovdqu64,	rsi,	rdi,	rdx,	32,	ymm16,	ymm17)
+
+
 #ifndef SECTION
 # error SECTION is not defined!
 #endif
@@ -242,7 +282,7 @@ END (__memmove_erms)
 	strong_alias (__memmove_chk_erms, __memcpy_chk_erms)
 #endif
 
-ENTRY (MEMMOVE_SYMBOL (__memmove, unaligned))
+P2ALIGN_ENTRY (MEMMOVE_SYMBOL (__memmove, unaligned), 6)
 	movq	%rdi, %rax
 L(start):
 #ifdef __ILP32__
@@ -250,15 +290,23 @@ L(start):
 	movl	%edx, %edx
 #endif
 	cmp	$VEC_SIZE, %RDX_LP
-	jb	L(less_vec)
+#if VEC_SIZE > 16
+	jbe	L(leq_1x)
+#else
+	jb	L(leq_1x)
+#endif
+#if MOVSB_ALIGN_TO >= VEC_SIZE
+	VMOVU	(%rsi), %VEC(0)
+#endif
 	cmp	$(VEC_SIZE * 2), %RDX_LP
-	ja	L(more_2x_vec)
-L(last_2x_vec):
+	ja	L(gt_2x)
 	/* From VEC and to 2 * VEC.  No branch when size == VEC_SIZE.  */
+#if MOVSB_ALIGN_TO < VEC_SIZE
 	VMOVU	(%rsi), %VEC(0)
-	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(1)
-	VMOVU	%VEC(0), (%rdi)
-	VMOVU	%VEC(1), -VEC_SIZE(%rdi, %rdx)
+#endif
+	VMOVU	(VEC_SIZE * -1)(%rsi, %rdx), %VEC(15)
+	VMOVU	%VEC(0), (VEC_SIZE * 0)(%rdi)
+	VMOVU	%VEC(15), (VEC_SIZE * -1)(%rdi, %rdx)
 L(return):
 #if VEC_SIZE > 16
 	ZERO_UPPER_VEC_REGISTERS_RETURN
@@ -266,296 +314,373 @@ L(return):
 	ret
 #endif
 
-#ifdef USE_WITH_ERMS
-L(movsb):
-	cmp	__x86_rep_movsb_stop_threshold(%rip), %RDX_LP
-	jae	L(more_8x_vec)
-	cmpq	%rsi, %rdi
-	jb	1f
-	/* Source == destination is less common.  */
-	je	L(nop)
-	leaq	(%rsi, %rdx), %r9
-	cmpq	%r9, %rdi
-	/* Avoid slow backward REP MOVSB.  */
-	jb	L(more_8x_vec_backward)
-# if AVOID_SHORT_DISTANCE_REP_MOVSB
-	andl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
-	jz	3f
-	movq	%rdi, %rcx
-	subq	%rsi, %rcx
-	jmp	2f
-# endif
-1:
-# if AVOID_SHORT_DISTANCE_REP_MOVSB
-	andl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
-	jz	3f
-	movq	%rsi, %rcx
-	subq	%rdi, %rcx
-2:
-	/* Avoid "rep movsb" if RCX, the distance between source and
-	   destination, is N*4GB + [1..63] with N >= 0.  */
-	cmpl	$63, %ecx
-	jbe	L(more_2x_vec_no_movsb)
-	/* Avoid "rep movsb" if ECX <= 63.  */
-3:
-# endif
-	mov	%RDX_LP, %RCX_LP
-	rep	movsb
+#if VEC_SIZE == 64
+L(copy_8_15):
+	COPY_8_16
+	ret
+
+L(copy_32_63):
+	vmovdqu64 (%rsi), %ymm16
+	vmovdqu64 -32(%rsi, %rdx), %ymm17
+	vmovdqu64 %ymm16, (%rdi)
+	vmovdqu64 %ymm17, -32(%rdi, %rdx)
 	ret
 #endif
 
-L(less_vec):
-	/* Less than 1 VEC.  */
+	.p2align 4,, 6
+L(leq_1x):
 #if VEC_SIZE != 16 && VEC_SIZE != 32 && VEC_SIZE != 64
 # error Unsupported VEC_SIZE!
 #endif
-#if VEC_SIZE > 32
-	cmpb	$32, %dl
-	jae	L(between_32_63)
+	cmpl	$(VEC_SIZE / 4), %edx
+	jb	L(lt_quarter_x)
+
+	cmpl	$(VEC_SIZE / 2), %edx
+#if VEC_SIZE == 64
+	ja	L(copy_32_63)
+	COPY_16_32
+#elif VEC_SIZE == 32
+	jb	L(copy_8_15)
+	COPY_16_32
+#else
+	jb	L(copy_4_7)
+	COPY_8_16
 #endif
-#if VEC_SIZE > 16
-	cmpb	$16, %dl
-	jae	L(between_16_31)
-#endif
-	cmpb	$8, %dl
-	jae	L(between_8_15)
-	cmpb	$4, %dl
-	jae	L(between_4_7)
-	cmpb	$1, %dl
-	ja	L(between_2_3)
-	jb	1f
+	ret
+
+
+	.p2align 4,, 6
+L(copy_4_7):
+	COPY_4_8
+	ret
+
+L(copy_1):
 	movzbl	(%rsi), %ecx
 	movb	%cl, (%rdi)
-1:
-L(nop):
 	ret
+
+	/* Colder copy case for [0, 7].  */
+L(lt_quarter_x):
 #if VEC_SIZE > 32
-L(between_32_63):
-	/* From 32 to 63.  No branch when size == 32.  */
-	VMOVU	(%rsi), %YMM0
-	VMOVU	-32(%rsi, %rdx), %YMM1
-	VMOVU	%YMM0, (%rdi)
-	VMOVU	%YMM1, -32(%rdi, %rdx)
-	VZEROUPPER_RETURN
+	cmpl	$8, %edx
+	jae	L(copy_8_15)
 #endif
 #if VEC_SIZE > 16
-	/* From 16 to 31.  No branch when size == 16.  */
-L(between_16_31):
-	VMOVU	(%rsi), %XMM0
-	VMOVU	-16(%rsi, %rdx), %XMM1
-	VMOVU	%XMM0, (%rdi)
-	VMOVU	%XMM1, -16(%rdi, %rdx)
-	VZEROUPPER_RETURN
-#endif
-L(between_8_15):
-	/* From 8 to 15.  No branch when size == 8.  */
-	movq	-8(%rsi, %rdx), %rcx
-	movq	(%rsi), %rsi
-	movq	%rcx, -8(%rdi, %rdx)
-	movq	%rsi, (%rdi)
+	cmpl	$4, %edx
+	jae	L(copy_4_7)
+#endif
+	cmpl	$1, %edx
+	je	L(copy_1)
+	jb	L(copy_0)
+	/* Fall through into copy [2, 3] as it is more common than [0, 1].  */
+	movzwl	(%rsi), %ecx
+	movzbl	-1(%rsi, %rdx), %esi
+	movw	%cx, (%rdi)
+	movb	%sil, -1(%rdi, %rdx)
+L(copy_0):
 	ret
-L(between_4_7):
-	/* From 4 to 7.  No branch when size == 4.  */
-	movl	-4(%rsi, %rdx), %ecx
-	movl	(%rsi), %esi
-	movl	%ecx, -4(%rdi, %rdx)
-	movl	%esi, (%rdi)
-	ret
-L(between_2_3):
-	/* From 2 to 3.  No branch when size == 2.  */
-	movzwl	-2(%rsi, %rdx), %ecx
-	movzwl	(%rsi), %esi
-	movw	%cx, -2(%rdi, %rdx)
-	movw	%si, (%rdi)
+
+	.p2align 4
+#if VEC_SIZE == 32
+L(copy_8_15):
+	COPY_8_16
 	ret
-    
-L(more_2x_vec):
-#ifdef USE_WITH_ERMS
-	cmp	__x86_rep_movsb_threshold(%rip), %RDX_LP
-	ja	L(movsb)
 #endif
-L(more_2x_vec_no_movsb):
-	/* More than 2 * VEC and there may be overlap between destination and
-	   source.  */
+
+
+L(gt_2x):
+#if MOVSB_ALIGN_TO >= (VEC_SIZE * 2)
+	VMOVU	(VEC_SIZE * 1)(%rsi), %VEC(1)
+#endif
 	cmpq	$(VEC_SIZE * 8), %rdx
-	ja	L(more_8x_vec)
-	cmpq	$(VEC_SIZE * 4), %rdx
-	jbe	L(last_4x_vec)
-	/* Copy from 4 * VEC + 1 to 8 * VEC, inclusively.  */
-	VMOVU	(%rsi), %VEC(0)
-	VMOVU	VEC_SIZE(%rsi), %VEC(1)
+	ja	L(gt_8x)
+#if MOVSB_ALIGN_TO < (VEC_SIZE)
+	VMOVU	(VEC_SIZE * 0)(%rsi), %VEC(0)
+#endif
+#if MOVSB_ALIGN_TO < (VEC_SIZE * 2)
+	VMOVU	(VEC_SIZE * 1)(%rsi), %VEC(1)
+#endif
+	cmpl	$(VEC_SIZE * 4), %edx
+	jbe	L(copy_gt_2x_leq_4x)
+
 	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(2)
 	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(3)
-	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(4)
-	VMOVU	-(VEC_SIZE * 2)(%rsi, %rdx), %VEC(5)
-	VMOVU	-(VEC_SIZE * 3)(%rsi, %rdx), %VEC(6)
-	VMOVU	-(VEC_SIZE * 4)(%rsi, %rdx), %VEC(7)
-	VMOVU	%VEC(0), (%rdi)
-	VMOVU	%VEC(1), VEC_SIZE(%rdi)
+	VMOVU	(VEC_SIZE * -4)(%rsi, %rdx), %VEC(12)
+	VMOVU	(VEC_SIZE * -3)(%rsi, %rdx), %VEC(13)
+	VMOVU	(VEC_SIZE * -2)(%rsi, %rdx), %VEC(14)
+	VMOVU	(VEC_SIZE * -1)(%rsi, %rdx), %VEC(15)
+	VMOVU	%VEC(0), (VEC_SIZE * 0)(%rdi)
+	VMOVU	%VEC(1), (VEC_SIZE * 1)(%rdi)
 	VMOVU	%VEC(2), (VEC_SIZE * 2)(%rdi)
 	VMOVU	%VEC(3), (VEC_SIZE * 3)(%rdi)
-	VMOVU	%VEC(4), -VEC_SIZE(%rdi, %rdx)
-	VMOVU	%VEC(5), -(VEC_SIZE * 2)(%rdi, %rdx)
-	VMOVU	%VEC(6), -(VEC_SIZE * 3)(%rdi, %rdx)
-	VMOVU	%VEC(7), -(VEC_SIZE * 4)(%rdi, %rdx)
+
+	VMOVU	%VEC(12), (VEC_SIZE * -4)(%rdi, %rdx)
+	VMOVU	%VEC(13), (VEC_SIZE * -3)(%rdi, %rdx)
+	VMOVU	%VEC(14), (VEC_SIZE * -2)(%rdi, %rdx)
+	VMOVU	%VEC(15), (VEC_SIZE * -1)(%rdi, %rdx)
 	VZEROUPPER_RETURN
-L(last_4x_vec):
-	/* Copy from 2 * VEC + 1 to 4 * VEC, inclusively.  */
-	VMOVU	(%rsi), %VEC(0)
-	VMOVU	VEC_SIZE(%rsi), %VEC(1)
-	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(2)
-	VMOVU	-(VEC_SIZE * 2)(%rsi, %rdx), %VEC(3)
-	VMOVU	%VEC(0), (%rdi)
-	VMOVU	%VEC(1), VEC_SIZE(%rdi)
-	VMOVU	%VEC(2), -VEC_SIZE(%rdi, %rdx)
-	VMOVU	%VEC(3), -(VEC_SIZE * 2)(%rdi, %rdx)
+
+	.p2align 4
+L(copy_gt_2x_leq_4x):
+	VMOVU	(VEC_SIZE * -2)(%rsi, %rdx), %VEC(14)
+	VMOVU	(VEC_SIZE * -1)(%rsi, %rdx), %VEC(15)
+
+	VMOVU	%VEC(0), (VEC_SIZE * 0)(%rdi)
+	VMOVU	%VEC(1), (VEC_SIZE * 1)(%rdi)
+	VMOVU	%VEC(14), (VEC_SIZE * -2)(%rdi, %rdx)
+	VMOVU	%VEC(15), (VEC_SIZE * -1)(%rdi, %rdx)
 	VZEROUPPER_RETURN
+#if VEC_SIZE != 16
+	.p2align 4
+#endif
 
-L(more_8x_vec):
-	/* Check if non-temporal move candidate.  */
-#if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
+	.p2align 4
+L(gt_8x):
+	movq	%rdi, %rcx
+	subq	%rsi, %rcx
+	movq	%rdi, %r8
+	cmpq	%rdx, %rcx
+	jb	L(gt_8x_backward)
+
+#ifdef USE_WITH_ERMS
+	cmpq	__x86_rep_movsb_threshold(%rip), %rdx
+	ja	L(movsb)
+#elif (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
 	/* Check non-temporal store threshold.  */
 	cmp	__x86_shared_non_temporal_threshold(%rip), %RDX_LP
 	ja	L(large_memcpy_2x)
 #endif
 	/* Entry if rdx is greater than non-temporal threshold but there is
 	   overlap.  */
-L(more_8x_vec_check):
-	cmpq	%rsi, %rdi
-	ja	L(more_8x_vec_backward)
-	/* Source == destination is less common.  */
-	je	L(nop)
-	/* Load the first VEC and last 4 * VEC to support overlapping addresses.
-	 */
-	VMOVU	(%rsi), %VEC(4)
-	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(5)
-	VMOVU	-(VEC_SIZE * 2)(%rsi, %rdx), %VEC(6)
-	VMOVU	-(VEC_SIZE * 3)(%rsi, %rdx), %VEC(7)
-	VMOVU	-(VEC_SIZE * 4)(%rsi, %rdx), %VEC(8)
-	/* Save start and stop of the destination buffer.  */
-	movq	%rdi, %r11
-	leaq	-VEC_SIZE(%rdi, %rdx), %rcx
-	/* Align destination for aligned stores in the loop.  Compute how much
-	   destination is misaligned.  */
-	movq	%rdi, %r8
-	andq	$(VEC_SIZE - 1), %r8
-	/* Get the negative of offset for alignment.  */
-	subq	$VEC_SIZE, %r8
-	/* Adjust source.  */
-	subq	%r8, %rsi
-	/* Adjust destination which should be aligned now.  */
-	subq	%r8, %rdi
-	/* Adjust length.  */
-	addq	%r8, %rdx
+L(gt_8x_forward):
+#if MOVSB_ALIGN_TO < (VEC_SIZE)
+	VMOVU	(VEC_SIZE * 0)(%rsi), %VEC(0)
+#endif
+#if MOVSB_ALIGN_TO < (VEC_SIZE * 2) && (LOOP_4X_ALIGN_TO > VEC_SIZE)
+	VMOVU	(VEC_SIZE * 1)(%rsi), %VEC(1)
+#endif
+
+	VMOVU	(VEC_SIZE * -1)(%rsi, %rdx), %VEC(15)
+	VMOVU	(VEC_SIZE * -2)(%rsi, %rdx), %VEC(14)
+	VMOVU	(VEC_SIZE * -3)(%rsi, %rdx), %VEC(13)
+	VMOVU	(VEC_SIZE * -4)(%rsi, %rdx), %VEC(12)
+	subq	%rdi, %rsi
+	leaq	(VEC_SIZE * -4)(%rdi, %rdx), %rdx
+	orq	$(LOOP_4X_ALIGN_TO - 1), %rdi
+	leaq	1(%rdi, %rsi), %rsi
+	incq	%rdi
+	/* Loop 4x VEC at a time copy forward from src (rsi) to dst (rdi).  */
+	.p2align 4
+L(loop_4x_forward):
+	VMOVU	(VEC_SIZE * 0)(%rsi), %VEC(4)
+	VMOVU	(VEC_SIZE * 1)(%rsi), %VEC(5)
+	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(6)
+	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(7)
+	subq	$(VEC_SIZE * -4), %rsi
+	VMOVA	%VEC(4), (VEC_SIZE * 0)(%rdi)
+	VMOVA	%VEC(5), (VEC_SIZE * 1)(%rdi)
+	VMOVA	%VEC(6), (VEC_SIZE * 2)(%rdi)
+	VMOVA	%VEC(7), (VEC_SIZE * 3)(%rdi)
+
+	subq	$(VEC_SIZE * -4), %rdi
+	cmpq	%rdi, %rdx
+	ja	L(loop_4x_forward)
+	VMOVU	%VEC(12), (VEC_SIZE * 0)(%rdx)
+	VMOVU	%VEC(13), (VEC_SIZE * 1)(%rdx)
+	VMOVU	%VEC(14), (VEC_SIZE * 2)(%rdx)
+	VMOVU	%VEC(15), (VEC_SIZE * 3)(%rdx)
+
+	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
+#if (LOOP_4X_ALIGN_TO > VEC_SIZE)
+	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
+#endif
+L(nop):
+	VZEROUPPER_RETURN
 
 	.p2align 4
-L(loop_4x_vec_forward):
-	/* Copy 4 * VEC a time forward.  */
-	VMOVU	(%rsi), %VEC(0)
-	VMOVU	VEC_SIZE(%rsi), %VEC(1)
+L(gt_8x_backward):
+#if MOVSB_ALIGN_TO < (VEC_SIZE)
+	VMOVU	(VEC_SIZE * 0)(%rsi), %VEC(0)
+#endif
+#if MOVSB_ALIGN_TO < (VEC_SIZE * 2)
+	VMOVU	(VEC_SIZE * 1)(%rsi), %VEC(1)
+#endif
 	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(2)
 	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(3)
-	subq	$-(VEC_SIZE * 4), %rsi
-	addq	$-(VEC_SIZE * 4), %rdx
-	VMOVA	%VEC(0), (%rdi)
-	VMOVA	%VEC(1), VEC_SIZE(%rdi)
-	VMOVA	%VEC(2), (VEC_SIZE * 2)(%rdi)
-	VMOVA	%VEC(3), (VEC_SIZE * 3)(%rdi)
-	subq	$-(VEC_SIZE * 4), %rdi
-	cmpq	$(VEC_SIZE * 4), %rdx
-	ja	L(loop_4x_vec_forward)
-	/* Store the last 4 * VEC.  */
-	VMOVU	%VEC(5), (%rcx)
-	VMOVU	%VEC(6), -VEC_SIZE(%rcx)
-	VMOVU	%VEC(7), -(VEC_SIZE * 2)(%rcx)
-	VMOVU	%VEC(8), -(VEC_SIZE * 3)(%rcx)
-	/* Store the first VEC.  */
-	VMOVU	%VEC(4), (%r11)
-	VZEROUPPER_RETURN
 
-L(more_8x_vec_backward):
-	/* Load the first 4 * VEC and last VEC to support overlapping addresses.
-	 */
-	VMOVU	(%rsi), %VEC(4)
-	VMOVU	VEC_SIZE(%rsi), %VEC(5)
-	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(6)
-	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(7)
-	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(8)
-	/* Save stop of the destination buffer.  */
-	leaq	-VEC_SIZE(%rdi, %rdx), %r11
-	/* Align destination end for aligned stores in the loop.  Compute how
-	   much destination end is misaligned.  */
-	leaq	-VEC_SIZE(%rsi, %rdx), %rcx
-	movq	%r11, %r9
-	movq	%r11, %r8
-	andq	$(VEC_SIZE - 1), %r8
-	/* Adjust source.  */
-	subq	%r8, %rcx
-	/* Adjust the end of destination which should be aligned now.  */
-	subq	%r8, %r9
-	/* Adjust length.  */
-	subq	%r8, %rdx
+	VMOVU	(VEC_SIZE * -1)(%rsi, %rdx), %VEC(15)
+#if (LOOP_4X_ALIGN_TO > VEC_SIZE)
+	VMOVU	(VEC_SIZE * -2)(%rsi, %rdx), %VEC(14)
+#endif
+
+	/* Subtract dst (rdi) from src (rsi) before aligning dst (rdi) so that
+	   src (rsi) can be properly adjusted.  */
+	subq	%rdi, %rsi
+	jz	L(nop)
+	/* Set dst (rdi) as end of region. Set 4x VEC from begining of region as
+	   those VECs where already loaded. -1 for aligning dst (rdi).  */
+	leaq	(VEC_SIZE * -4 + -1)(%rdi, %rdx), %rdi
+	/* Align dst (rdi).  */
+	andq	$-(LOOP_4X_ALIGN_TO), %rdi
+	/* Readjust src (rsi).  */
+	addq	%rdi, %rsi
+	/* Loop 4x VEC at a time copy backward from src (rsi) to dst (rdi).  */
+	.p2align 4
+L(loop_4x_backward):
+	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(4)
+	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(5)
+	VMOVU	(VEC_SIZE * 1)(%rsi), %VEC(6)
+	VMOVU	(VEC_SIZE * 0)(%rsi), %VEC(7)
+
+	addq	$(VEC_SIZE * -4), %rsi
+
+	VMOVA	%VEC(4), (VEC_SIZE * 3)(%rdi)
+	VMOVA	%VEC(5), (VEC_SIZE * 2)(%rdi)
+	VMOVA	%VEC(6), (VEC_SIZE * 1)(%rdi)
+	VMOVA	%VEC(7), (VEC_SIZE * 0)(%rdi)
+	addq	$(VEC_SIZE * -4), %rdi
+
+	cmpq	%rdi, %r8
+	jb	L(loop_4x_backward)
+
+	/* Store begining of region.  */
 
+	/* NB: rax was set to dst on function entry.  */
+	VMOVU	%VEC(3), (VEC_SIZE * 3)(%r8)
+	VMOVU	%VEC(2), (VEC_SIZE * 2)(%r8)
+	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
+	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
+
+	/* Store VECs loaded for aligning dst (rdi).  */
+#if (LOOP_4X_ALIGN_TO > VEC_SIZE)
+	VMOVU	%VEC(14), (VEC_SIZE * -2)(%r8, %rdx)
+#endif
+	VMOVU	%VEC(15), (VEC_SIZE * -1)(%r8, %rdx)
+	VZEROUPPER_RETURN
+
+#ifdef USE_WITH_ERMS
 	.p2align 4
-L(loop_4x_vec_backward):
-	/* Copy 4 * VEC a time backward.  */
-	VMOVU	(%rcx), %VEC(0)
-	VMOVU	-VEC_SIZE(%rcx), %VEC(1)
-	VMOVU	-(VEC_SIZE * 2)(%rcx), %VEC(2)
-	VMOVU	-(VEC_SIZE * 3)(%rcx), %VEC(3)
-	addq	$-(VEC_SIZE * 4), %rcx
-	addq	$-(VEC_SIZE * 4), %rdx
-	VMOVA	%VEC(0), (%r9)
-	VMOVA	%VEC(1), -VEC_SIZE(%r9)
-	VMOVA	%VEC(2), -(VEC_SIZE * 2)(%r9)
-	VMOVA	%VEC(3), -(VEC_SIZE * 3)(%r9)
-	addq	$-(VEC_SIZE * 4), %r9
-	cmpq	$(VEC_SIZE * 4), %rdx
-	ja	L(loop_4x_vec_backward)
-	/* Store the first 4 * VEC.  */
-	VMOVU	%VEC(4), (%rdi)
-	VMOVU	%VEC(5), VEC_SIZE(%rdi)
-	VMOVU	%VEC(6), (VEC_SIZE * 2)(%rdi)
-	VMOVU	%VEC(7), (VEC_SIZE * 3)(%rdi)
-	/* Store the last VEC.  */
-	VMOVU	%VEC(8), (%r11)
+L(movsb):
+	/* If size (rdx) > __x86_rep_movsb_stop_threshold go to large copy.  */
+
+	/* NB: __x86_rep_movsb_stop_threshold in range
+	   [__x86_rep_movsb_threshold, __x86_shared_non_temporal_threshold].  */
+	cmp	__x86_rep_movsb_stop_threshold(%rip), %RDX_LP
+# if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
+	jae	L(large_memcpy_2x)
+# else
+	jae	L(gt_8x_forward)
+# endif
+# if MOVSB_ALIGN_TO < VEC_SIZE
+#  if VEC_SIZE > 16
+	cmpl	$-64, %ecx
+	jae	L(gt_8x_forward)
+#  endif
+	movq	%rdx, %rcx
+	rep	movsb
+	VZEROUPPER_RETURN
+# elif ALIGN_MOVSB_EXCLUSIVE
+#  if VEC_SIZE > 16
+	cmpl	$-64, %ecx
+	jae	L(gt_8x_forward)
+#  endif
+	subq	%rdi, %rsi
+	leaq	(%rdi, %rdx), %rcx
+	addq	$(MOVSB_ALIGN_TO - 1), %rdi
+	andq	$-(MOVSB_ALIGN_TO), %rdi
+	addq	%rdi, %rsi
+	subq	%rdi, %rcx
+
+	rep	movsb
+
+	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
+#  if MOVSB_ALIGN_TO > VEC_SIZE
+	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
+#  endif
+	VZEROUPPER_RETURN
+# else
+	testl	$(PAGE_SIZE - 512), %ecx
+	jnz	L(movsb_align_dst)
+	movq	%rcx, %r9
+	leaq	-(1)(%rsi, %rdx), %rcx
+	orq	$(MOVSB_ALIGN_TO - 1), %rsi
+	leaq	1(%rsi, %r9), %rdi
+	subq	%rsi, %rcx
+	incq	%rsi
+	rep	movsb
+
+	/* Store aligning VECs.  */
+
+	/* NB: rax was set to dst on function entry.  */
+	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
+#  if MOVSB_ALIGN_TO > VEC_SIZE
+	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
+#  endif
+	VZEROUPPER_RETURN
+L(movsb_align_dst):
+	subq	%rdi, %rsi
+	leaq	-(1)(%rdi, %rdx), %rcx
+	orq	$(MOVSB_ALIGN_TO - 1), %rdi
+	leaq	1(%rdi, %rsi), %rsi
+	subq	%rdi, %rcx
+	incq	%rdi
+
+	rep	movsb
+
+	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
+#  if MOVSB_ALIGN_TO > VEC_SIZE
+	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
+#  endif
 	VZEROUPPER_RETURN
+# endif
+#endif
+
 
 #if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
 	.p2align 4
 L(large_memcpy_2x):
-	/* Compute absolute value of difference between source and destination.
-	 */
-	movq	%rdi, %r9
-	subq	%rsi, %r9
-	movq	%r9, %r8
-	leaq	-1(%r9), %rcx
-	sarq	$63, %r8
-	xorq	%r8, %r9
-	subq	%r8, %r9
-	/* Don't use non-temporal store if there is overlap between destination
-	   and source since destination may be in cache when source is loaded.  */
-	cmpq	%r9, %rdx
-	ja	L(more_8x_vec_check)
+# ifdef USE_WITH_ERMS
+	cmpq	__x86_shared_non_temporal_threshold(%rip), %rdx
+	jb	L(gt_8x_forward)
+# endif
+	/* We want to use temporal copies if there is overlap constructive cache
+	   interference between loading from src and storing to dst.  */
+
+	/* rcx current has dst - src. Above logic already checked that there is
+	   no overlap in the case of dst > src. Negating rcx will produce a
+	   negative value (or large positive value) if dst > src (which we already
+	   know has no overlap) and the src - dst if dst < src. If rdx is greater
+	   than negated rcx then there is overlap in the dst < src case (so do
+	   forward copy) or in the highly unlikely case that length > 2^64 the
+	   temporal forward copy will still work.  */
+	negq	%rcx
+	cmpq	%rcx, %rdx
+	ja	L(gt_8x_forward)
 
 	/* Cache align destination. First store the first 64 bytes then adjust
 	   alignments.  */
-	VMOVU	(%rsi), %VEC(8)
+# if MOVSB_ALIGN_TO < VEC_SIZE
+	VMOVU	(%rsi), %VEC(0)
+# endif
 # if VEC_SIZE < 64
-	VMOVU	VEC_SIZE(%rsi), %VEC(9)
+#  if MOVSB_ALIGN_TO < (2 * VEC_SIZE)
+	VMOVU	VEC_SIZE(%rsi), %VEC(1)
+#  endif
 #  if VEC_SIZE < 32
-	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(10)
-	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(11)
+	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(2)
+	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(3)
 #  endif
 # endif
-	VMOVU	%VEC(8), (%rdi)
+
+	VMOVU	%VEC(0), (%rdi)
 # if VEC_SIZE < 64
-	VMOVU	%VEC(9), VEC_SIZE(%rdi)
+	VMOVU	%VEC(1), VEC_SIZE(%rdi)
 #  if VEC_SIZE < 32
-	VMOVU	%VEC(10), (VEC_SIZE * 2)(%rdi)
-	VMOVU	%VEC(11), (VEC_SIZE * 3)(%rdi)
+	VMOVU	%VEC(2), (VEC_SIZE * 2)(%rdi)
+	VMOVU	%VEC(3), (VEC_SIZE * 3)(%rdi)
 #  endif
 # endif
+
 	/* Adjust source, destination, and size.  */
 	movq	%rdi, %r8
 	andq	$63, %r8
@@ -568,8 +693,10 @@ L(large_memcpy_2x):
 	/* Adjust length.  */
 	addq	%r8, %rdx
 
+
 	/* Test if source and destination addresses will alias. If they do the
 	   larger pipeline in large_memcpy_4x alleviated the performance drop.  */
+	incl	%ecx
 	testl	$(PAGE_SIZE - VEC_SIZE * 8), %ecx
 	jz	L(large_memcpy_4x)
 
-- 
2.25.1

