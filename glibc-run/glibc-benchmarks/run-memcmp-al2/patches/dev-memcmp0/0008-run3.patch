From 2c5caa1fec29d0089809d78df7f9c3925d7a45c9 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Sat, 18 Sep 2021 17:20:43 -0500
Subject: [PATCH 8/9] run3

---
 sysdeps/x86_64/multiarch/memcmp-avx2-movbe.S | 301 ++++++++++---------
 sysdeps/x86_64/multiarch/memcmp-evex-movbe.S |   3 +-
 2 files changed, 159 insertions(+), 145 deletions(-)

diff --git a/sysdeps/x86_64/multiarch/memcmp-avx2-movbe.S b/sysdeps/x86_64/multiarch/memcmp-avx2-movbe.S
index 79ba9df543..068f59bb6a 100644
--- a/sysdeps/x86_64/multiarch/memcmp-avx2-movbe.S
+++ b/sysdeps/x86_64/multiarch/memcmp-avx2-movbe.S
@@ -124,10 +124,11 @@ ENTRY_P2ALIGN (MEMCMP, 6)
 	   branches.  */
 
 	/* Load first two VEC from s2 before adjusting addresses.  */
-	vmovdqu	-(VEC_SIZE * 4)(%rsi, %rdx), %ymm1
-	vmovdqu	-(VEC_SIZE * 3)(%rsi, %rdx), %ymm2
-	leaq	-(4 * VEC_SIZE)(%rdi, %rdx), %rdi
 	leaq	-(4 * VEC_SIZE)(%rsi, %rdx), %rsi
+	vmovdqu	(%rsi), %ymm1
+	vmovdqu	VEC_SIZE(%rsi), %ymm2
+	leaq	-(4 * VEC_SIZE)(%rdi, %rdx), %rdi
+
 
 	/* Wait to load from s1 until addressed adjust due to unlamination
 	   of microfusion with complex address mode.  */
@@ -150,7 +151,116 @@ ENTRY_P2ALIGN (MEMCMP, 6)
 	VZEROUPPER_RETURN
 
 
+	.p2align 4
+L(more_8x_vec):
+	/* Set end of s1 in rdx.  */
+	leaq	-(VEC_SIZE * 4)(%rdi, %rdx), %rdx
+	/* rsi stores s2 - s1. This allows loop to only update one pointer.
+	 */
+	subq	%rdi, %rsi
+	/* Align s1 pointer.  */
+	andq	$-VEC_SIZE, %rdi
+	/* Adjust because first 4x vec where check already.  */
+	subq	$-(VEC_SIZE * 4), %rdi
+	.p2align 4
+L(loop_4x_vec):
+	/* rsi has s2 - s1 so get correct address by adding s1 (in rdi).  */
+	vmovdqu	(%rsi, %rdi), %ymm1
+	VPCMPEQ	(%rdi), %ymm1, %ymm1
+
+	vmovdqu	VEC_SIZE(%rsi, %rdi), %ymm2
+	VPCMPEQ	VEC_SIZE(%rdi), %ymm2, %ymm2
+
+	vmovdqu	(VEC_SIZE * 2)(%rsi, %rdi), %ymm3
+	VPCMPEQ	(VEC_SIZE * 2)(%rdi), %ymm3, %ymm3
+
+	vmovdqu	(VEC_SIZE * 3)(%rsi, %rdi), %ymm4
+	VPCMPEQ	(VEC_SIZE * 3)(%rdi), %ymm4, %ymm4
+
+	vpand	%ymm1, %ymm2, %ymm5
+	vpand	%ymm3, %ymm4, %ymm6
+	vpand	%ymm5, %ymm6, %ymm7
+	vpmovmskb %ymm7, %ecx
+	incl	%ecx
+	jnz	L(8x_return_vec_0_1_2_3)
+	subq	$-(VEC_SIZE * 4), %rdi
+	/* Check if s1 pointer at end.  */
+	cmpq	%rdx, %rdi
+	jb	L(loop_4x_vec)
+
+	subq	%rdx, %rdi
+	/* rdi has 4 * VEC_SIZE - remaining length.  */
+	cmpl	$(VEC_SIZE * 3), %edi
+	jae	L(8x_last_1x_vec)
+
+	/* Load regardless of branch.  */
+	vmovdqu	(VEC_SIZE * 2)(%rsi, %rdx), %ymm3
+	VPCMPEQ	(VEC_SIZE * 2)(%rdx), %ymm3, %ymm3
+	cmpl	$(VEC_SIZE * 2), %edi
+	jae	L(8x_last_2x_vec)
+
+	vmovdqu	(VEC_SIZE * 3)(%rsi, %rdx), %ymm4
+	VPCMPEQ	(VEC_SIZE * 3)(%rdx), %ymm4, %ymm4
+
+	/* Check last 4 VEC.  */
+	vmovdqu	(%rsi, %rdx), %ymm1
+	VPCMPEQ	(%rdx), %ymm1, %ymm1
+
+	vmovdqu	VEC_SIZE(%rsi, %rdx), %ymm2
+	VPCMPEQ	VEC_SIZE(%rdx), %ymm2, %ymm2
+
+
+	vpand	%ymm1, %ymm2, %ymm5
+	vpand	%ymm3, %ymm4, %ymm6
+	vpand	%ymm5, %ymm6, %ymm7
+	vpmovmskb %ymm7, %ecx
+	/* Restore s1 pointer to rdi.  */
+	movq	%rdx, %rdi
+	incl	%ecx
+	jnz	L(8x_return_vec_0_1_2_3)
+	/* NB: eax must be zero to reach here.  */
+	VZEROUPPER_RETURN
+
+	/* Only entry is from L(more_8x_vec).  */
 	.p2align 4,, 10
+L(8x_last_2x_vec):
+	/* Check second to last VEC. rdx store end pointer of s1 and ymm3
+	   has already been loaded with second to last VEC from s2.  */
+
+	vpmovmskb %ymm3, %eax
+	incl	%eax
+	jnz	L(8x_return_vec_2)
+	/* Check last VEC.  */
+	.p2align 4,, 6
+L(8x_last_1x_vec):
+	vmovdqu	(VEC_SIZE * 3)(%rsi, %rdx), %ymm4
+	VPCMPEQ	(VEC_SIZE * 3)(%rdx), %ymm4, %ymm4
+	vpmovmskb %ymm4, %eax
+	incl	%eax
+	jnz	L(8x_return_vec_3)
+	VZEROUPPER_RETURN
+
+	.p2align 4
+L(8x_return_vec_2):
+	subq	$VEC_SIZE, %rdx
+L(8x_return_vec_3):
+	tzcntl	%eax, %eax
+	addq	%rdx, %rax
+# ifdef USE_AS_WMEMCMP
+	movl	(VEC_SIZE * 3)(%rax), %ecx
+	xorl	%edx, %edx
+	cmpl	(VEC_SIZE * 3)(%rsi, %rax), %ecx
+	setg	%dl
+	leal	-1(%rdx, %rdx), %eax
+# else
+	movzbl	(VEC_SIZE * 3)(%rsi, %rax), %ecx
+	movzbl	(VEC_SIZE * 3)(%rax), %eax
+	subl	%ecx, %eax
+# endif
+	VZEROUPPER_RETURN
+
+
+	.p2align 4
 L(8x_return_vec_0_1_2_3):
 	/* Returning from L(more_8x_vec) requires restoring rsi.  */
 	addq	%rdi, %rsi
@@ -166,9 +276,9 @@ L(return_vec_0_1_2_3):
 	vpmovmskb %ymm3, %eax
 	incl	%eax
 	jnz	L(return_vec_2)
-	.p2align 4,, 4
+	.p2align 4,, 6
 L(return_vec_3):
-	tzcntl	%ecx, %ecx
+	bsfl	%ecx, %ecx
 # ifdef USE_AS_WMEMCMP
 	movl	(VEC_SIZE * 3)(%rdi, %rcx), %eax
 	xorl	%edx, %edx
@@ -184,9 +294,10 @@ L(return_vzeroupper):
 	ZERO_UPPER_VEC_REGISTERS_RETURN
 
 
+
 	.p2align 4
 L(return_vec_0):
-	tzcntl	%eax, %eax
+	bsfl	%eax, %eax
 # ifdef USE_AS_WMEMCMP
 	movl	(%rdi, %rax), %ecx
 	xorl	%edx, %edx
@@ -204,7 +315,7 @@ L(return_vec_0):
 
 	.p2align 4,, 10
 L(return_vec_1):
-	tzcntl	%eax, %eax
+	bsfl	%eax, %eax
 # ifdef USE_AS_WMEMCMP
 	movl	VEC_SIZE(%rdi, %rax), %ecx
 	xorl	%edx, %edx
@@ -218,9 +329,9 @@ L(return_vec_1):
 # endif
 	VZEROUPPER_RETURN
 
-	.p2align 4,, 9
+	.p2align 4,, 10
 L(return_vec_2):
-	tzcntl	%eax, %eax
+	bsfl	%eax, %eax
 # ifdef USE_AS_WMEMCMP
 	movl	(VEC_SIZE * 2)(%rdi, %rax), %ecx
 	xorl	%edx, %edx
@@ -248,7 +359,7 @@ L(less_vec):
 	orl	%esi, %eax
 	andl	$(PAGE_SIZE - 1), %eax
 	cmpl	$(PAGE_SIZE - VEC_SIZE), %eax
-	jg	L(page_cross_less_vec)
+	ja	L(page_cross_less_vec)
 
 	/* No page cross possible.  */
 	vmovdqu	(%rsi), %ymm2
@@ -257,10 +368,29 @@ L(less_vec):
 	incl	%eax
 	/* Result will be zero if s1 and s2 match. Otherwise first set bit
 	   will be first mismatch.  */
-	bzhil	%edx, %eax, %eax
+	bzhil	%edx, %eax, %edx
 	jnz	L(return_vec_0)
+	xorl	%eax, %eax
+	VZEROUPPER_RETURN
+	NOP2
+	.p2align 4
+L(last_2x_vec):
+	/* Check second to last VEC.  */
+	vmovdqu	-(VEC_SIZE * 2)(%rsi, %rdx), %ymm1
+	VPCMPEQ	-(VEC_SIZE * 2)(%rdi, %rdx), %ymm1, %ymm1
+	vpmovmskb %ymm1, %eax
+	incl	%eax
+	jnz	L(return_vec_1_end)
+	/* Check last VEC.  */
+L(last_1x_vec):
+	vmovdqu	-(VEC_SIZE * 1)(%rsi, %rdx), %ymm1
+	VPCMPEQ	-(VEC_SIZE * 1)(%rdi, %rdx), %ymm1, %ymm1
+	vpmovmskb %ymm1, %eax
+	incl	%eax
+	jnz	L(return_vec_0_end)
 	VZEROUPPER_RETURN
 
+	.p2align 4,, 10
 # ifdef USE_AS_WMEMCMP
 L(one_or_less):
 	jb	L(zero)
@@ -281,31 +411,15 @@ L(one_or_less):
 	/* No ymm register was touched.  */
 	ret
 # endif
+
 L(zero):
 	xorl	%eax, %eax
 	ret
 
 
 	.p2align 4
-L(last_2x_vec):
-	/* Check second to last VEC.  */
-	vmovdqu	-(VEC_SIZE * 2)(%rsi, %rdx), %ymm1
-	VPCMPEQ	-(VEC_SIZE * 2)(%rdi, %rdx), %ymm1, %ymm1
-	vpmovmskb %ymm1, %eax
-	incl	%eax
-	jnz	L(return_vec_1_end)
-	/* Check last VEC.  */
-L(last_1x_vec):
-	vmovdqu	-(VEC_SIZE * 1)(%rsi, %rdx), %ymm1
-	VPCMPEQ	-(VEC_SIZE * 1)(%rdi, %rdx), %ymm1, %ymm1
-	vpmovmskb %ymm1, %eax
-	incl	%eax
-	jnz	L(return_vec_0_end)
-	VZEROUPPER_RETURN
-
-	.p2align 4,,10
 L(return_vec_0_end):
-	tzcntl	%eax, %eax
+	bsfl	%eax, %eax
 	addl	%edx, %eax
 # ifdef USE_AS_WMEMCMP
 	movl	-VEC_SIZE(%rdi, %rax), %ecx
@@ -320,9 +434,10 @@ L(return_vec_0_end):
 # endif
 	VZEROUPPER_RETURN
 
+
 	.p2align 4,, 10
 L(return_vec_1_end):
-	tzcntl	%eax, %eax
+	bsfl	%eax, %eax
 	addl	%edx, %eax
 # ifdef USE_AS_WMEMCMP
 	movl	-(VEC_SIZE * 2)(%rdi, %rax), %ecx
@@ -337,113 +452,8 @@ L(return_vec_1_end):
 # endif
 	VZEROUPPER_RETURN
 
-	.p2align 4
-L(more_8x_vec):
-	/* Set end of s1 in rdx.  */
-	leaq	-(VEC_SIZE * 4)(%rdi, %rdx), %rdx
-	/* rsi stores s2 - s1. This allows loop to only update one pointer.
-	 */
-	subq	%rdi, %rsi
-	/* Align s1 pointer.  */
-	andq	$-VEC_SIZE, %rdi
-	/* Adjust because first 4x vec where check already.  */
-	subq	$-(VEC_SIZE * 4), %rdi
-	.p2align 4
-L(loop_4x_vec):
-	/* rsi has s2 - s1 so get correct address by adding s1 (in rdi).  */
-	vmovdqu	(%rsi, %rdi), %ymm1
-	VPCMPEQ	(%rdi), %ymm1, %ymm1
-
-	vmovdqu	VEC_SIZE(%rsi, %rdi), %ymm2
-	VPCMPEQ	VEC_SIZE(%rdi), %ymm2, %ymm2
 
-	vmovdqu	(VEC_SIZE * 2)(%rsi, %rdi), %ymm3
-	VPCMPEQ	(VEC_SIZE * 2)(%rdi), %ymm3, %ymm3
 
-	vmovdqu	(VEC_SIZE * 3)(%rsi, %rdi), %ymm4
-	VPCMPEQ	(VEC_SIZE * 3)(%rdi), %ymm4, %ymm4
-
-	vpand	%ymm1, %ymm2, %ymm5
-	vpand	%ymm3, %ymm4, %ymm6
-	vpand	%ymm5, %ymm6, %ymm7
-	vpmovmskb %ymm7, %ecx
-	incl	%ecx
-	jnz	L(8x_return_vec_0_1_2_3)
-	subq	$-(VEC_SIZE * 4), %rdi
-	/* Check if s1 pointer at end.  */
-	cmpq	%rdx, %rdi
-	jb	L(loop_4x_vec)
-
-	subq	%rdx, %rdi
-	/* rdi has 4 * VEC_SIZE - remaining length.  */
-	cmpl	$(VEC_SIZE * 3), %edi
-	jae	L(8x_last_1x_vec)
-	/* Load regardless of branch.  */
-	vmovdqu	(VEC_SIZE * 2)(%rsi, %rdx), %ymm3
-	cmpl	$(VEC_SIZE * 2), %edi
-	jae	L(8x_last_2x_vec)
-
-	/* Check last 4 VEC.  */
-	vmovdqu	(%rsi, %rdx), %ymm1
-	VPCMPEQ	(%rdx), %ymm1, %ymm1
-
-	vmovdqu	VEC_SIZE(%rsi, %rdx), %ymm2
-	VPCMPEQ	VEC_SIZE(%rdx), %ymm2, %ymm2
-
-	VPCMPEQ	(VEC_SIZE * 2)(%rdx), %ymm3, %ymm3
-
-	vmovdqu	(VEC_SIZE * 3)(%rsi, %rdx), %ymm4
-	VPCMPEQ	(VEC_SIZE * 3)(%rdx), %ymm4, %ymm4
-
-	vpand	%ymm1, %ymm2, %ymm5
-	vpand	%ymm3, %ymm4, %ymm6
-	vpand	%ymm5, %ymm6, %ymm7
-	vpmovmskb %ymm7, %ecx
-	/* Restore s1 pointer to rdi.  */
-	movq	%rdx, %rdi
-	incl	%ecx
-	jnz	L(8x_return_vec_0_1_2_3)
-	/* NB: eax must be zero to reach here.  */
-	VZEROUPPER_RETURN
-
-	/* Only entry is from L(more_8x_vec).  */
-	.p2align 4
-L(8x_last_2x_vec):
-	/* Check second to last VEC. rdx store end pointer of s1 and ymm3
-	   has already been loaded with second to last VEC from s2.  */
-	VPCMPEQ	(VEC_SIZE * 2)(%rdx), %ymm3, %ymm3
-	vpmovmskb %ymm3, %eax
-	incl	%eax
-	jnz	L(8x_return_vec_2)
-	/* Check last VEC.  */
-	.p2align 4
-L(8x_last_1x_vec):
-	vmovdqu	(VEC_SIZE * 3)(%rsi, %rdx), %ymm4
-	VPCMPEQ	(VEC_SIZE * 3)(%rdx), %ymm4, %ymm4
-	vpmovmskb %ymm4, %eax
-	incl	%eax
-	jnz	L(8x_return_vec_3)
-	VZEROUPPER_RETURN
-
-
-	.p2align 4
-L(8x_return_vec_2):
-	subq	$VEC_SIZE, %rdx
-L(8x_return_vec_3):
-	tzcntl	%eax, %eax
-	addq	%rdx, %rax
-# ifdef USE_AS_WMEMCMP
-	movl	(VEC_SIZE * 3)(%rax), %ecx
-	xorl	%edx, %edx
-	cmpl	(VEC_SIZE * 3)(%rsi, %rax), %ecx
-	setg	%dl
-	leal	-1(%rdx, %rdx), %eax
-# else
-	movzbl	(VEC_SIZE * 3)(%rsi, %rax), %ecx
-	movzbl	(VEC_SIZE * 3)(%rax), %eax
-	subl	%ecx, %eax
-# endif
-	VZEROUPPER_RETURN
 
 
 	.p2align 4
@@ -489,7 +499,7 @@ L(between_8_15):
 	vmovq	-8(%rdi, %rdx), %xmm1
 	vmovq	-8(%rsi, %rdx), %xmm2
 	VPCMPEQ	%xmm1, %xmm2, %xmm2
-    addl    $(VEC_SIZE - 8), %edx
+	addl	$(VEC_SIZE - 8), %edx
 	vpmovmskb %xmm2, %eax
 	subl	$0xffff, %eax
 	jnz	L(return_vec_0_end)
@@ -499,15 +509,17 @@ L(between_8_15):
 	.p2align 4,, 10
 L(between_16_31):
 	/* From 16 to 31 bytes.  No branch when size == 16.  */
-	vmovdqu	(%rsi), %xmm2
+
+	/* Use movups to save code size.  */
+	movups	(%rsi), %xmm2
 	VPCMPEQ	(%rdi), %xmm2, %xmm2
 	vpmovmskb %xmm2, %eax
 	subl	$0xffff, %eax
 	jnz	L(return_vec_0)
 	/* Use overlapping loads to avoid branches.  */
-	vmovdqu	-16(%rsi, %rdx), %xmm2
+	movups	-16(%rsi, %rdx), %xmm2
 	VPCMPEQ	-16(%rdi, %rdx), %xmm2, %xmm2
-    addl    $(VEC_SIZE - 16), %edx
+	addl	$(VEC_SIZE - 16), %edx
 	vpmovmskb %xmm2, %eax
 	subl	$0xffff, %eax
 	jnz	L(return_vec_0_end)
@@ -524,15 +536,16 @@ L(between_2_3):
 	shll	$8, %ecx
 	bswap	%eax
 	bswap	%ecx
-	movzbl	-1(%rdi, %rdx), %edi
-	movzbl	-1(%rsi, %rdx), %esi
-	orl	%edi, %eax
-	orl	%esi, %ecx
+	/* Partial register stall but any machine with AVX2 will also just
+	   insert merging uop. Doing this saves 6 bytes of code which causes
+	   memcmp() to take up one less cache line. Since page cross
+	   L(between_2_3) is relatively cold, this is likely worth it.  */
+	orb	-1(%rdi, %rdx), %al
+	orb	-1(%rsi, %rdx), %cl
 	/* Subtraction is okay because the upper 8 bits are zero.  */
 	subl	%ecx, %eax
 	/* No ymm register was touched.  */
 	ret
-
 # endif
 
 END (MEMCMP)
diff --git a/sysdeps/x86_64/multiarch/memcmp-evex-movbe.S b/sysdeps/x86_64/multiarch/memcmp-evex-movbe.S
index a215d0ab0f..b79ff19f3b 100644
--- a/sysdeps/x86_64/multiarch/memcmp-evex-movbe.S
+++ b/sysdeps/x86_64/multiarch/memcmp-evex-movbe.S
@@ -546,6 +546,7 @@ L(between_2_3):
 	/* Subtraction is okay because the upper 8 bits are zero.  */
 	subl	%ecx, %eax
 	ret
-# endif    
+# endif
+
 END (MEMCMP)
 #endif
-- 
2.25.1

