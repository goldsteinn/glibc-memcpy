From 3e9d1b03bed5bf2ba1c63dca83ed3586bee3142d Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Mon, 11 Oct 2021 20:34:23 -0500
Subject: [PATCH 5/8] tmp

---
 .../multiarch/memmove-vec-unaligned-erms.S    | 186 +++++-------------
 1 file changed, 52 insertions(+), 134 deletions(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
index 9c03a25393..d2bba2539e 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
@@ -389,6 +389,44 @@ L(between_32_63):
 
 
 
+	.p2align 4,, 10
+#if defined USE_MULTIARCH && IS_IN (libc)
+L(movsb):
+	movq	%rdi, %rcx
+	subq	%rsi, %rcx
+	/* Go to backwards temporal copy if overlap no matter what as
+	   backward REP MOVSB is slow and we don't want to use NT stores if
+	   there is overlap.  */
+	cmpq	%rdx, %rcx
+	/* L(more_8x_vec_backward_check_nop) checks for src == dst.  */
+	jb	L(more_8x_vec_backward_check_nop)
+	/* If above x86_rep_movsb_stop_threshold most likely is
+	   candidate for NT moves aswell.  */
+	cmp	__x86_rep_movsb_stop_threshold(%rip), %RDX_LP
+	jae	L(large_memcpy_2x_check)
+# if AVOID_SHORT_DISTANCE_REP_MOVSB
+	/* Only avoid short movsb if CPU has FSRM.  */
+	testl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
+	jz	L(skip_short_movsb_check)
+#  if AVOID_SHORT_DISTANCE_REP_MOVSB
+	/* Avoid "rep movsb" if RCX, the distance between source and
+	   destination, is N*4GB + [1..63] with N >= 0.  */
+
+	/* ecx contains dst - src. Early check for backward copy
+	   conditions means only case of slow movsb with src = dst + [0,
+	   63] is ecx in [-63, 0]. Use unsigned comparison with -64 check
+	   for that case.  */
+	cmpl	$-64, %ecx
+	ja	L(more_8x_vec_forward)
+#  endif
+# endif
+	/* !ALIGN_MOVSB.  */
+L(skip_short_movsb_check):
+	mov	%RDX_LP, %RCX_LP
+	rep	movsb
+	ret
+#endif
+
 	.p2align 4,, 12
 #if defined USE_MULTIARCH && IS_IN (libc)
 L(movsb_more_2x_vec):
@@ -420,9 +458,8 @@ L(more_2x_vec):
 	VMOVU	%VEC(5), -(VEC_SIZE * 2)(%rdi, %rdx)
 	VMOVU	%VEC(6), -(VEC_SIZE * 3)(%rdi, %rdx)
 	VMOVU	%VEC(7), -(VEC_SIZE * 4)(%rdi, %rdx)
+L(nop_backward):
 	VZEROUPPER_RETURN
-
-	.p2align 4,, 6
 L(last_4x_vec):
 	/* Copy from 2 * VEC + 1 to 4 * VEC, inclusively.  */
 	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(2)
@@ -431,9 +468,7 @@ L(last_4x_vec):
 	VMOVU	%VEC(1), VEC_SIZE(%rdi)
 	VMOVU	%VEC(2), -VEC_SIZE(%rdi, %rdx)
 	VMOVU	%VEC(3), -(VEC_SIZE * 2)(%rdi, %rdx)
-L(nop_backward):
 	VZEROUPPER_RETURN
-
 	.p2align 4
 L(more_8x_vec_backward_check_nop):
 	testq	%rcx, %rcx
@@ -494,7 +529,7 @@ L(more_8x_vec):
 	/* Check if non-temporal move candidate.  */
 #if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
 	/* Check non-temporal store threshold.  */
-    cmp	__x86_shared_non_temporal_threshold(%rip), %RDX_LP
+	cmp     __x86_shared_non_temporal_threshold(%rip), %RDX_LP
 	ja	L(large_memcpy_2x)
 #endif
 	/* To reach this point there cannot be overlap and dst > src. So
@@ -502,8 +537,8 @@ L(more_8x_vec):
 	   requires forward copy. Otherwise decide between backward/forward
 	   copy depending on address aliasing.  */
 
-	/* Entry if rdx is greater than __x86_rep_movsb_stop_threshold but
-	   less than __x86_shared_non_temporal_threshold.  */
+	/* Entry if rdx is greater than x86_rep_movsb_stop_threshold but
+	   less than x86_shared_non_temporal_threshold.  */
 L(more_8x_vec_check):
 	/* rcx contains dst - src. Add back length (rdx).  */
 	leaq	(%rcx, %rdx), %r8
@@ -520,7 +555,7 @@ L(more_8x_vec_check):
 	addl	%r8d, %ecx
 	jz	L(more_8x_vec_backward)
 
-	/* if rdx is greater than __x86_shared_non_temporal_threshold but
+	/* if rdx is greater than x86_shared_non_temporal_threshold but
 	   there is overlap, or from short distance movsb.  */
 L(more_8x_vec_forward):
 	/* Load first and last 4 * VEC to support overlapping addresses.
@@ -573,125 +608,8 @@ L(loop_4x_vec_forward):
 	/* Keep L(nop_backward) target close to jmp for 2-byte encoding.
 	 */
 	VZEROUPPER_RETURN
-	.p2align 5
-	/* NB: The size of the L(skip_short_movsb_check) and L(movsb)
-	   blocks impacts the alignment of the L(more_2x_vec) and
-	   L(more_8x_vec) cases. If this block is changes ensure no
-	   performance degragragation for sizes running out of those
-	   sections.  */
-#if ALIGN_MOVSB
-L(skip_short_movsb_check):
-	VMOVU	(%rsi), %VEC(4)
-# if MOVSB_ALIGN_TO > VEC_SIZE
-	VMOVU	VEC_SIZE(%rsi), %VEC(5)
-# endif
-# if MOVSB_ALIGN_TO > (VEC_SIZE * 2)
-#  error Unsupported MOVSB_ALIGN_TO
-# endif
-	/* If CPU does not have FSRM two options for aligning. Align src
-	   if dst and src 4k alias. Otherwise align dst.  */
-	testl	$(PAGE_SIZE - 512), %ecx
-	jnz	L(movsb_align_dst)
-	/* rcx already has dst - src.  */
-	movq	%rcx, %r9
-	/* Add src to len. Subtract back after src aligned. -1 because
-	   src is initially aligned to MOVSB_ALIGN_TO - 1.  */
-	leaq	-(1)(%rsi, %rdx), %rcx
-	/* Inclusively align src to MOVSB_ALIGN_TO - 1.  */
-	orq	$(MOVSB_ALIGN_TO - 1), %rsi
-	/* Restore dst and len adjusted with new values for aligned dst.
-	 */
-	leaq	1(%rsi, %r9), %rdi
-	subq	%rsi, %rcx
-	/* Finish aligning src.  */
-	incq	%rsi
-
-	rep	movsb
-
-	VMOVU	%VEC(4), (%r8)
-# if MOVSB_ALIGN_TO > VEC_SIZE
-	VMOVU	%VEC(5), VEC_SIZE(%r8)
-# endif
-	VZEROUPPER_RETURN
-#endif
 
-	.p2align 4,, 10
-#if defined USE_MULTIARCH && IS_IN (libc)
-L(movsb):
-	movq	%rdi, %rcx
-	subq	%rsi, %rcx
-	/* Go to backwards temporal copy if overlap no matter what as
-	   backward REP MOVSB is slow and we don't want to use NT stores if
-	   there is overlap.  */
-	cmpq	%rdx, %rcx
-	/* L(more_8x_vec_backward_check_nop) checks for src == dst.  */
-	jb	L(more_8x_vec_backward_check_nop)
-# if ALIGN_MOVSB
-	/* Save dest for storing aligning VECs later.  */
-	movq	%rdi, %r8
-# endif
-	/* If above __x86_rep_movsb_stop_threshold most likely is
-	   candidate for NT moves aswell.  */
-	cmp	__x86_rep_movsb_stop_threshold(%rip), %RDX_LP
-	jae	L(large_memcpy_2x_check)
-# if AVOID_SHORT_DISTANCE_REP_MOVSB || ALIGN_MOVSB
-	/* Only avoid short movsb if CPU has FSRM.  */
-	testl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
-	jz	L(skip_short_movsb_check)
-#  if AVOID_SHORT_DISTANCE_REP_MOVSB
-	/* Avoid "rep movsb" if RCX, the distance between source and
-	   destination, is N*4GB + [1..63] with N >= 0.  */
-
-	/* ecx contains dst - src. Early check for backward copy
-	   conditions means only case of slow movsb with src = dst + [0,
-	   63] is ecx in [-63, 0]. Use unsigned comparison with -64 check
-	   for that case.  */
-	cmpl	$-64, %ecx
-	ja	L(more_8x_vec_forward)
-#  endif
-# endif
-# if ALIGN_MOVSB
-	VMOVU	(%rsi), %VEC(4)
-#  if MOVSB_ALIGN_TO > VEC_SIZE
-	VMOVU	VEC_SIZE(%rsi), %VEC(5)
-#  endif
-#  if MOVSB_ALIGN_TO > (VEC_SIZE * 2)
-#   error Unsupported MOVSB_ALIGN_TO
-#  endif
-	/* Fall through means cpu has FSRM. In that case exclusively
-	   align destination.  */
-L(movsb_align_dst):
-	/* Subtract dst from src. Add back after dst aligned.  */
-	subq	%rdi, %rsi
-	/* Exclusively align dst to MOVSB_ALIGN_TO (64).  */
-	addq	$(MOVSB_ALIGN_TO - 1), %rdi
-	/* Add dst to len. Subtract back after dst aligned.  */
-	leaq	(%r8, %rdx), %rcx
-	/* Finish aligning dst.  */
-	andq	$-(MOVSB_ALIGN_TO), %rdi
-	/* Restore src and len adjusted with new values for aligned dst.
-	 */
-	addq	%rdi, %rsi
-	subq	%rdi, %rcx
-
-	rep	movsb
-
-	/* Store VECs loaded for aligning.  */
-	VMOVU	%VEC(4), (%r8)
-#  if MOVSB_ALIGN_TO > VEC_SIZE
-	VMOVU	%VEC(5), VEC_SIZE(%r8)
-#  endif
-	VZEROUPPER_RETURN
-# else
-	/* !ALIGN_MOVSB.  */
-L(skip_short_movsb_check):
-	mov	%RDX_LP, %RCX_LP
-	rep	movsb
-	ret
-# endif
-#endif
-
-	.p2align 4,, 10
+	.p2align 4
 #if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
 L(large_memcpy_2x_check):
 	cmp	__x86_shared_non_temporal_threshold(%rip), %RDX_LP
@@ -708,21 +626,21 @@ L(large_memcpy_2x):
 	/* Cache align destination. First store the first 64 bytes then
 	   adjust alignments.  */
 	VMOVU	(%rsi), %VEC(8)
-#if VEC_SIZE < 64
+# if VEC_SIZE < 64
 	VMOVU	VEC_SIZE(%rsi), %VEC(9)
-#if VEC_SIZE < 32
+#  if VEC_SIZE < 32
 	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(10)
 	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(11)
-#endif
-#endif
+#  endif
+# endif
 	VMOVU	%VEC(8), (%rdi)
-#if VEC_SIZE < 64
+# if VEC_SIZE < 64
 	VMOVU	%VEC(9), VEC_SIZE(%rdi)
-#if VEC_SIZE < 32
+#  if VEC_SIZE < 32
 	VMOVU	%VEC(10), (VEC_SIZE * 2)(%rdi)
 	VMOVU	%VEC(11), (VEC_SIZE * 3)(%rdi)
-#endif
-#endif
+#  endif
+# endif
 	/* Adjust source, destination, and size.  */
 	movq	%rdi, %r8
 	andq	$63, %r8
-- 
2.25.1

