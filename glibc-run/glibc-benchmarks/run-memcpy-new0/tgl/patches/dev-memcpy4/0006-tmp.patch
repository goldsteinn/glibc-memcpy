From d361bd2e7ef2e503843aab8fe0e1fed30f8c707a Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Mon, 11 Oct 2021 20:37:47 -0500
Subject: [PATCH 06/10] tmp

---
 .../multiarch/memmove-vec-unaligned-erms.S    | 179 +++++-------------
 1 file changed, 48 insertions(+), 131 deletions(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
index 629ef0da5b..3be4ebfba7 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
@@ -389,6 +389,43 @@ L(between_8_15):
 	ret
 
 	.p2align 4,, 10
+#if defined USE_MULTIARCH && IS_IN (libc)
+L(movsb):
+	movq	%rdi, %rcx
+	subq	%rsi, %rcx
+	/* Go to backwards temporal copy if overlap no matter what as
+	   backward REP MOVSB is slow and we don't want to use NT stores if
+	   there is overlap.  */
+	cmpq	%rdx, %rcx
+	/* L(more_8x_vec_backward_check_nop) checks for src == dst.  */
+	jb	L(more_8x_vec_backward_check_nop)
+	/* If above x86_rep_movsb_stop_threshold most likely is
+	   candidate for NT moves aswell.  */
+	cmp	__x86_rep_movsb_stop_threshold(%rip), %RDX_LP
+	jae	L(large_memcpy_2x_check)
+# if AVOID_SHORT_DISTANCE_REP_MOVSB
+	/* Only avoid short movsb if CPU has FSRM.  */
+	testl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
+	jz	L(skip_short_movsb_check)
+#  if AVOID_SHORT_DISTANCE_REP_MOVSB
+	/* Avoid "rep movsb" if RCX, the distance between source and
+	   destination, is N*4GB + [1..63] with N >= 0.  */
+
+	/* ecx contains dst - src. Early check for backward copy
+	   conditions means only case of slow movsb with src = dst + [0,
+	   63] is ecx in [-63, 0]. Use unsigned comparison with -64 check
+	   for that case.  */
+	cmpl	$-64, %ecx
+	ja	L(more_8x_vec_forward)
+#  endif
+# endif
+	/* !ALIGN_MOVSB.  */
+L(skip_short_movsb_check):
+	mov	%RDX_LP, %RCX_LP
+	rep	movsb
+	ret
+#endif
+	.p2align 4
 L(last_4x_vec):
 	/* Copy from 2 * VEC + 1 to 4 * VEC, inclusively.  */
 	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(2)
@@ -399,12 +436,10 @@ L(last_4x_vec):
 	VMOVU	%VEC(3), -(VEC_SIZE * 2)(%rdi, %rdx)
 	VZEROUPPER_RETURN
 
-
-
 	.p2align 4,, 12
 #if defined USE_MULTIARCH && IS_IN (libc)
 L(movsb_more_2x_vec):
-	cmp	__x86_rep_movsb_threshold(%rip), %RDX_LP
+	cmp     __x86_rep_movsb_threshold(%rip), %RDX_LP
 	ja	L(movsb)
 #endif
 L(more_2x_vec):
@@ -434,7 +469,7 @@ L(more_2x_vec):
 	VZEROUPPER_RETURN
 
 
-	.p2align 4,, 12
+	.p2align 4
 L(more_8x_vec):
 	movq	%rdi, %rcx
 	subq	%rsi, %rcx
@@ -447,7 +482,7 @@ L(more_8x_vec):
 	/* Check if non-temporal move candidate.  */
 #if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
 	/* Check non-temporal store threshold.  */
-    cmp	__x86_shared_non_temporal_threshold(%rip), %RDX_LP
+	cmp     __x86_shared_non_temporal_threshold(%rip), %RDX_LP
 	ja	L(large_memcpy_2x)
 #endif
 	/* To reach this point there cannot be overlap and dst > src. So
@@ -574,124 +609,6 @@ L(loop_4x_vec_backward):
 	VZEROUPPER_RETURN
 
 
-	.p2align 4,, 8
-	.p2align 6,, 16
-	/* NB: The size of the L(skip_short_movsb_check) and L(movsb)
-	   blocks impacts the alignment of the L(more_2x_vec) and
-	   L(more_8x_vec) cases. If this block is changes ensure no
-	   performance degragragation for sizes running out of those
-	   sections.  */
-#if ALIGN_MOVSB
-L(skip_short_movsb_check):
-# if MOVSB_ALIGN_TO > VEC_SIZE
-	VMOVU	VEC_SIZE(%rsi), %VEC(5)
-# endif
-# if MOVSB_ALIGN_TO > (VEC_SIZE * 2)
-#  error Unsupported MOVSB_ALIGN_TO
-# endif
-	/* If CPU does not have FSRM two options for aligning. Align src
-	   if dst and src 4k alias. Otherwise align dst.  */
-	testl	$(PAGE_SIZE - 512), %ecx
-	jnz	L(movsb_align_dst)
-	/* rcx already has dst - src.  */
-	movq	%rcx, %r9
-	/* Add src to len. Subtract back after src aligned. -1 because
-	   src is initially aligned to MOVSB_ALIGN_TO - 1.  */
-	leaq	-(1)(%rsi, %rdx), %rcx
-	/* Inclusively align src to MOVSB_ALIGN_TO - 1.  */
-	orq	$(MOVSB_ALIGN_TO - 1), %rsi
-	/* Restore dst and len adjusted with new values for aligned dst.
-	 */
-	leaq	1(%rsi, %r9), %rdi
-	subq	%rsi, %rcx
-	/* Finish aligning src.  */
-	incq	%rsi
-
-	rep	movsb
-
-	VMOVU	%VEC(0), (%r8)
-# if MOVSB_ALIGN_TO > VEC_SIZE
-	VMOVU	%VEC(5), VEC_SIZE(%r8)
-# endif
-	VZEROUPPER_RETURN
-#endif
-
-	.p2align 4,, 10
-	.p2align 6,, 16
-#if defined USE_MULTIARCH && IS_IN (libc)
-L(movsb):
-	movq	%rdi, %rcx
-	subq	%rsi, %rcx
-	/* Go to backwards temporal copy if overlap no matter what as
-	   backward REP MOVSB is slow and we don't want to use NT stores if
-	   there is overlap.  */
-	cmpq	%rdx, %rcx
-	/* L(more_8x_vec_backward_check_nop) checks for src == dst.  */
-	jb	L(more_8x_vec_backward_check_nop)
-# if ALIGN_MOVSB
-	/* Save dest for storing aligning VECs later.  */
-	movq	%rdi, %r8
-# endif
-	/* If above __x86_rep_movsb_stop_threshold most likely is
-	   candidate for NT moves aswell.  */
-	cmp	__x86_rep_movsb_stop_threshold(%rip), %RDX_LP
-	jae	L(large_memcpy_2x_check)
-# if AVOID_SHORT_DISTANCE_REP_MOVSB || ALIGN_MOVSB
-	/* Only avoid short movsb if CPU has FSRM.  */
-	testl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
-	jz	L(skip_short_movsb_check)
-#  if AVOID_SHORT_DISTANCE_REP_MOVSB
-	/* Avoid "rep movsb" if RCX, the distance between source and
-	   destination, is N*4GB + [1..63] with N >= 0.  */
-
-	/* ecx contains dst - src. Early check for backward copy
-	   conditions means only case of slow movsb with src = dst + [0,
-	   63] is ecx in [-63, 0]. Use unsigned comparison with -64 check
-	   for that case.  */
-	cmpl	$-64, %ecx
-	ja	L(more_8x_vec_forward)
-#  endif
-# endif
-# if ALIGN_MOVSB
-#  if MOVSB_ALIGN_TO > VEC_SIZE
-	VMOVU	VEC_SIZE(%rsi), %VEC(5)
-#  endif
-#  if MOVSB_ALIGN_TO > (VEC_SIZE * 2)
-#   error Unsupported MOVSB_ALIGN_TO
-#  endif
-	/* Fall through means cpu has FSRM. In that case exclusively
-	   align destination.  */
-L(movsb_align_dst):
-	/* Subtract dst from src. Add back after dst aligned.  */
-	subq	%rdi, %rsi
-	/* Exclusively align dst to MOVSB_ALIGN_TO (64).  */
-	addq	$(MOVSB_ALIGN_TO - 1), %rdi
-	/* Add dst to len. Subtract back after dst aligned.  */
-	leaq	(%r8, %rdx), %rcx
-	/* Finish aligning dst.  */
-	andq	$-(MOVSB_ALIGN_TO), %rdi
-	/* Restore src and len adjusted with new values for aligned dst.
-	 */
-	addq	%rdi, %rsi
-	subq	%rdi, %rcx
-
-	rep	movsb
-
-	/* Store VECs loaded for aligning.  */
-	VMOVU	%VEC(0), (%r8)
-#  if MOVSB_ALIGN_TO > VEC_SIZE
-	VMOVU	%VEC(5), VEC_SIZE(%r8)
-#  endif
-	VZEROUPPER_RETURN
-# else
-	/* !ALIGN_MOVSB.  */
-L(skip_short_movsb_check):
-	mov	%RDX_LP, %RCX_LP
-	rep	movsb
-	ret
-# endif
-#endif
-
 	.p2align 4,, 10
 #if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
 L(large_memcpy_2x_check):
@@ -708,21 +625,21 @@ L(large_memcpy_2x):
 
 	/* Cache align destination. First store the first 64 bytes then
 	   adjust alignments.  */
-#if VEC_SIZE < 64
+# if VEC_SIZE < 64
 	VMOVU	VEC_SIZE(%rsi), %VEC(9)
-#if VEC_SIZE < 32
+#  if VEC_SIZE < 32
 	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(10)
 	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(11)
-#endif
-#endif
+#  endif
+# endif
 	VMOVU	%VEC(0), (%rdi)
-#if VEC_SIZE < 64
+# if VEC_SIZE < 64
 	VMOVU	%VEC(9), VEC_SIZE(%rdi)
-#if VEC_SIZE < 32
+#  if VEC_SIZE < 32
 	VMOVU	%VEC(10), (VEC_SIZE * 2)(%rdi)
 	VMOVU	%VEC(11), (VEC_SIZE * 3)(%rdi)
-#endif
-#endif
+#  endif
+# endif
 	/* Adjust source, destination, and size.  */
 	movq	%rdi, %r8
 	andq	$63, %r8
-- 
2.25.1

