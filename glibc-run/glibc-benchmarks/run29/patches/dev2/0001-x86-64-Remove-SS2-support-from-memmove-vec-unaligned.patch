From daaa4ce3ea3b633ab276ffd86e1186031fe6e52f Mon Sep 17 00:00:00 2001
From: "H.J. Lu" <hjl.tools@gmail.com>
Date: Tue, 7 Sep 2021 11:26:00 -0700
Subject: [PATCH 01/10] x86-64: Remove SS2 support from
 memmove-vec-unaligned-erms.S

Remove SS2 support from memmove-vec-unaligned-erms.S and move it to
memmove.S so that memmove-vec-unaligned-erms.S can be further optimized
for AVX and AVX512 without changing SSE2 code path.

There are no changes in ld.so nor libc.so with and without multi-arch
support.
---
 sysdeps/x86_64/memmove.S                      | 648 +++++++++++++++++-
 .../multiarch/memmove-vec-unaligned-erms.S    | 133 +---
 2 files changed, 665 insertions(+), 116 deletions(-)

diff --git a/sysdeps/x86_64/memmove.S b/sysdeps/x86_64/memmove.S
index db106a7a1f..e1d59a3847 100644
--- a/sysdeps/x86_64/memmove.S
+++ b/sysdeps/x86_64/memmove.S
@@ -18,9 +18,7 @@
 
 #include <sysdep.h>
 
-#define VEC_SIZE	16
 #define VEC(i)		xmm##i
-#define PREFETCHNT	prefetchnta
 #define VMOVNT		movntdq
 /* Use movups and movaps for smaller code sizes.  */
 #define VMOVU		movups
@@ -47,7 +45,651 @@
 # define MEMMOVE_SYMBOL(p,s)		memmove
 #endif
 
-#include "multiarch/memmove-vec-unaligned-erms.S"
+/* memmove/memcpy/mempcpy is implemented as:
+   1. Use overlapping load and store to avoid branch.
+   2. Load all sources into registers and store them together to avoid
+      possible address overlap between source and destination.
+   3. If size is 8 * 16 bytes or less, load all sources into registers
+      and store them together.
+   4. If address of destination > address of source, backward copy
+      4 * 16 bytes at a time with unaligned load and aligned store.
+      Load the first 4 * 16 bytes and last 16 bytes before the loop
+      and store them after the loop to support overlapping addresses.
+   5. Otherwise, forward copy 4 * 16 bytes at a time with unaligned
+      load and aligned store.  Load the last 4 * 16 bytes and first
+      16 bytes before the loop and store them after the loop to support
+      overlapping addresses.
+   6. On machines with ERMS feature, if size greater than equal or to
+      __x86_rep_movsb_threshold and less than
+      __x86_rep_movsb_stop_threshold, then REP MOVSB will be used.
+   7. If size >= __x86_shared_non_temporal_threshold and there is no
+      overlap between destination and source, use non-temporal store
+      instead of aligned store copying from either 2 or 4 pages at
+      once.
+   8. For point 7) if size < 16 * __x86_shared_non_temporal_threshold
+      and source and destination do not page alias, copy from 2 pages
+      at once using non-temporal stores. Page aliasing in this case is
+      considered true if destination's page alignment - sources' page
+      alignment is less than 8 * 16 bytes.
+   9. If size >= 16 * __x86_shared_non_temporal_threshold or source
+      and destination do page alias copy from 4 pages at once using
+      non-temporal stores.  */
+
+#ifndef MEMCPY_SYMBOL
+# define MEMCPY_SYMBOL(p,s)		MEMMOVE_SYMBOL(p, s)
+#endif
+
+#ifndef MEMPCPY_SYMBOL
+# define MEMPCPY_SYMBOL(p,s)		MEMMOVE_SYMBOL(p, s)
+#endif
+
+#ifndef MEMMOVE_CHK_SYMBOL
+# define MEMMOVE_CHK_SYMBOL(p,s)	MEMMOVE_SYMBOL(p, s)
+#endif
+
+#ifndef PAGE_SIZE
+# define PAGE_SIZE 4096
+#endif
+
+#if PAGE_SIZE != 4096
+# error Unsupported PAGE_SIZE
+#endif
+
+#ifndef LOG_PAGE_SIZE
+# define LOG_PAGE_SIZE 12
+#endif
+
+#if PAGE_SIZE != (1 << LOG_PAGE_SIZE)
+# error Invalid LOG_PAGE_SIZE
+#endif
+
+/* Byte per page for large_memcpy inner loop.  */
+#define LARGE_LOAD_SIZE (16 * 4)
+
+/* Amount to shift rdx by to compare for memcpy_large_4x.  */
+#ifndef LOG_4X_MEMCPY_THRESH
+# define LOG_4X_MEMCPY_THRESH 4
+#endif
+
+#ifndef PREFETCH
+# define PREFETCH(addr) prefetcht0 addr
+#endif
+
+/* Assume 64-byte prefetch size.  */
+#ifndef PREFETCH_SIZE
+# define PREFETCH_SIZE 64
+#endif
+
+#define PREFETCHED_LOAD_SIZE (16 * 4)
+
+#if PREFETCH_SIZE == 64 && PREFETCHED_LOAD_SIZE == PREFETCH_SIZE
+# define PREFETCH_ONE_SET(dir, base, offset) \
+	PREFETCH ((offset)base)
+#else
+# error Unsupported PREFETCH_SIZE or PREFETCHED_LOAD_SIZE!
+#endif
+
+#define LOAD_ONE_SET(base, offset, vec0, vec1, vec2, vec3) \
+	VMOVU	(offset)base, vec0; \
+	VMOVU	((offset) + 16)base, vec1; \
+	VMOVU	((offset) + 16 * 2)base, vec2; \
+	VMOVU	((offset) + 16 * 3)base, vec3;
+#define STORE_ONE_SET(base, offset, vec0, vec1, vec2, vec3) \
+	VMOVNT	vec0, (offset)base; \
+	VMOVNT	vec1, ((offset) + 16)base; \
+	VMOVNT	vec2, ((offset) + 16 * 2)base; \
+	VMOVNT	vec3, ((offset) + 16 * 3)base;
+
+	.section SECTION(.text),"ax",@progbits
+#if defined SHARED && IS_IN (libc)
+ENTRY (MEMMOVE_CHK_SYMBOL (__mempcpy_chk, unaligned))
+	cmp	%RDX_LP, %RCX_LP
+	jb	HIDDEN_JUMPTARGET (__chk_fail)
+END (MEMMOVE_CHK_SYMBOL (__mempcpy_chk, unaligned))
+#endif
+
+ENTRY (MEMPCPY_SYMBOL (__mempcpy, unaligned))
+	mov	%RDI_LP, %RAX_LP
+	add	%RDX_LP, %RAX_LP
+	jmp	L(start)
+END (MEMPCPY_SYMBOL (__mempcpy, unaligned))
+
+#if defined SHARED && IS_IN (libc)
+ENTRY (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned))
+	cmp	%RDX_LP, %RCX_LP
+	jb	HIDDEN_JUMPTARGET (__chk_fail)
+END (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned))
+#endif
+
+ENTRY (MEMMOVE_SYMBOL (__memmove, unaligned))
+	movq	%rdi, %rax
+L(start):
+#ifdef __ILP32__
+	/* Clear the upper 32 bits.  */
+	movl	%edx, %edx
+#endif
+	cmp	$16, %RDX_LP
+	jb	L(less_vec)
+	cmp	$(16 * 2), %RDX_LP
+	ja	L(more_2x_vec)
+#if !defined USE_MULTIARCH || !IS_IN (libc)
+L(last_2x_vec):
+#endif
+	/* From 16 to 2 * 16.  No branch when size == 16.  */
+	VMOVU	(%rsi), %VEC(0)
+	VMOVU	-16(%rsi,%rdx), %VEC(1)
+	VMOVU	%VEC(0), (%rdi)
+	VMOVU	%VEC(1), -16(%rdi,%rdx)
+#if !defined USE_MULTIARCH || !IS_IN (libc)
+L(nop):
+#endif
+	ret
+#if defined USE_MULTIARCH && IS_IN (libc)
+END (MEMMOVE_SYMBOL (__memmove, unaligned))
+
+ENTRY (__mempcpy_chk_erms)
+	cmp	%RDX_LP, %RCX_LP
+	jb	HIDDEN_JUMPTARGET (__chk_fail)
+END (__mempcpy_chk_erms)
+
+/* Only used to measure performance of REP MOVSB.  */
+ENTRY (__mempcpy_erms)
+	mov	%RDI_LP, %RAX_LP
+	/* Skip zero length.  */
+	test	%RDX_LP, %RDX_LP
+	jz	2f
+	add	%RDX_LP, %RAX_LP
+	jmp	L(start_movsb)
+END (__mempcpy_erms)
+
+ENTRY (__memmove_chk_erms)
+	cmp	%RDX_LP, %RCX_LP
+	jb	HIDDEN_JUMPTARGET (__chk_fail)
+END (__memmove_chk_erms)
+
+ENTRY (__memmove_erms)
+	movq	%rdi, %rax
+	/* Skip zero length.  */
+	test	%RDX_LP, %RDX_LP
+	jz	2f
+L(start_movsb):
+	mov	%RDX_LP, %RCX_LP
+	cmp	%RSI_LP, %RDI_LP
+	jb	1f
+	/* Source == destination is less common.  */
+	je	2f
+	lea	(%rsi,%rcx), %RDX_LP
+	cmp	%RDX_LP, %RDI_LP
+	jb	L(movsb_backward)
+1:
+	rep movsb
+2:
+	ret
+L(movsb_backward):
+	leaq	-1(%rdi,%rcx), %rdi
+	leaq	-1(%rsi,%rcx), %rsi
+	std
+	rep movsb
+	cld
+	ret
+END (__memmove_erms)
+strong_alias (__memmove_erms, __memcpy_erms)
+strong_alias (__memmove_chk_erms, __memcpy_chk_erms)
+
+# ifdef SHARED
+ENTRY (MEMMOVE_CHK_SYMBOL (__mempcpy_chk, unaligned_erms))
+	cmp	%RDX_LP, %RCX_LP
+	jb	HIDDEN_JUMPTARGET (__chk_fail)
+END (MEMMOVE_CHK_SYMBOL (__mempcpy_chk, unaligned_erms))
+# endif
+
+ENTRY (MEMMOVE_SYMBOL (__mempcpy, unaligned_erms))
+	mov	%RDI_LP, %RAX_LP
+	add	%RDX_LP, %RAX_LP
+	jmp	L(start_erms)
+END (MEMMOVE_SYMBOL (__mempcpy, unaligned_erms))
+
+# ifdef SHARED
+ENTRY (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned_erms))
+	cmp	%RDX_LP, %RCX_LP
+	jb	HIDDEN_JUMPTARGET (__chk_fail)
+END (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned_erms))
+# endif
+
+ENTRY (MEMMOVE_SYMBOL (__memmove, unaligned_erms))
+	movq	%rdi, %rax
+L(start_erms):
+# ifdef __ILP32__
+	/* Clear the upper 32 bits.  */
+	movl	%edx, %edx
+# endif
+	cmp	$16, %RDX_LP
+	jb	L(less_vec)
+	cmp	$(16 * 2), %RDX_LP
+	ja	L(movsb_more_2x_vec)
+L(last_2x_vec):
+	/* From 16 to 2 * 16.  No branch when size == 16. */
+	VMOVU	(%rsi), %VEC(0)
+	VMOVU	-16(%rsi,%rdx), %VEC(1)
+	VMOVU	%VEC(0), (%rdi)
+	VMOVU	%VEC(1), -16(%rdi,%rdx)
+L(return):
+	ret
+
+L(movsb):
+	cmp     __x86_rep_movsb_stop_threshold(%rip), %RDX_LP
+	jae	L(more_8x_vec)
+	cmpq	%rsi, %rdi
+	jb	1f
+	/* Source == destination is less common.  */
+	je	L(nop)
+	leaq	(%rsi,%rdx), %r9
+	cmpq	%r9, %rdi
+	/* Avoid slow backward REP MOVSB.  */
+	jb	L(more_8x_vec_backward)
+1:
+	mov	%RDX_LP, %RCX_LP
+	rep movsb
+L(nop):
+	ret
+#endif
+
+L(less_vec):
+	/* Less than 16 bytes.  */
+	cmpb	$8, %dl
+	jae	L(between_8_15)
+	cmpb	$4, %dl
+	jae	L(between_4_7)
+	cmpb	$1, %dl
+	ja	L(between_2_3)
+	jb	1f
+	movzbl	(%rsi), %ecx
+	movb	%cl, (%rdi)
+1:
+	ret
+L(between_8_15):
+	/* From 8 to 15.  No branch when size == 8.  */
+	movq	-8(%rsi,%rdx), %rcx
+	movq	(%rsi), %rsi
+	movq	%rcx, -8(%rdi,%rdx)
+	movq	%rsi, (%rdi)
+	ret
+L(between_4_7):
+	/* From 4 to 7.  No branch when size == 4.  */
+	movl	-4(%rsi,%rdx), %ecx
+	movl	(%rsi), %esi
+	movl	%ecx, -4(%rdi,%rdx)
+	movl	%esi, (%rdi)
+	ret
+L(between_2_3):
+	/* From 2 to 3.  No branch when size == 2.  */
+	movzwl	-2(%rsi,%rdx), %ecx
+	movzwl	(%rsi), %esi
+	movw	%cx, -2(%rdi,%rdx)
+	movw	%si, (%rdi)
+	ret
+
+#if defined USE_MULTIARCH && IS_IN (libc)
+L(movsb_more_2x_vec):
+	cmp	__x86_rep_movsb_threshold(%rip), %RDX_LP
+	ja	L(movsb)
+#endif
+L(more_2x_vec):
+	/* More than 2 * 16 bytes and there may be overlap between
+	   destination and source.  */
+	cmpq	$(16 * 8), %rdx
+	ja	L(more_8x_vec)
+	cmpq	$(16 * 4), %rdx
+	jbe	L(last_4x_vec)
+	/* Copy from 4 * 16 + 1 to 8 * 16, inclusively. */
+	VMOVU	(%rsi), %VEC(0)
+	VMOVU	16(%rsi), %VEC(1)
+	VMOVU	(16 * 2)(%rsi), %VEC(2)
+	VMOVU	(16 * 3)(%rsi), %VEC(3)
+	VMOVU	-16(%rsi,%rdx), %VEC(4)
+	VMOVU	-(16 * 2)(%rsi,%rdx), %VEC(5)
+	VMOVU	-(16 * 3)(%rsi,%rdx), %VEC(6)
+	VMOVU	-(16 * 4)(%rsi,%rdx), %VEC(7)
+	VMOVU	%VEC(0), (%rdi)
+	VMOVU	%VEC(1), 16(%rdi)
+	VMOVU	%VEC(2), (16 * 2)(%rdi)
+	VMOVU	%VEC(3), (16 * 3)(%rdi)
+	VMOVU	%VEC(4), -16(%rdi,%rdx)
+	VMOVU	%VEC(5), -(16 * 2)(%rdi,%rdx)
+	VMOVU	%VEC(6), -(16 * 3)(%rdi,%rdx)
+	VMOVU	%VEC(7), -(16 * 4)(%rdi,%rdx)
+	ret
+L(last_4x_vec):
+	/* Copy from 2 * 16 bytes + 1 to 4 * 16 bytes, inclusively. */
+	VMOVU	(%rsi), %VEC(0)
+	VMOVU	16(%rsi), %VEC(1)
+	VMOVU	-16(%rsi,%rdx), %VEC(2)
+	VMOVU	-(16 * 2)(%rsi,%rdx), %VEC(3)
+	VMOVU	%VEC(0), (%rdi)
+	VMOVU	%VEC(1), 16(%rdi)
+	VMOVU	%VEC(2), -16(%rdi,%rdx)
+	VMOVU	%VEC(3), -(16 * 2)(%rdi,%rdx)
+	ret
+
+L(more_8x_vec):
+	/* Check if non-temporal move candidate.  */
+#if IS_IN (libc)
+	/* Check non-temporal store threshold.  */
+	cmp __x86_shared_non_temporal_threshold(%rip), %RDX_LP
+	ja	L(large_memcpy_2x)
+#endif
+	/* Entry if rdx is greater than non-temporal threshold but there
+       is overlap.  */
+L(more_8x_vec_check):
+	cmpq	%rsi, %rdi
+	ja	L(more_8x_vec_backward)
+	/* Source == destination is less common.  */
+	je	L(nop)
+	/* Load the first 16 bytes and last 4 * 16 bytes to support
+	   overlapping addresses.  */
+	VMOVU	(%rsi), %VEC(4)
+	VMOVU	-16(%rsi, %rdx), %VEC(5)
+	VMOVU	-(16 * 2)(%rsi, %rdx), %VEC(6)
+	VMOVU	-(16 * 3)(%rsi, %rdx), %VEC(7)
+	VMOVU	-(16 * 4)(%rsi, %rdx), %VEC(8)
+	/* Save start and stop of the destination buffer.  */
+	movq	%rdi, %r11
+	leaq	-16(%rdi, %rdx), %rcx
+	/* Align destination for aligned stores in the loop.  Compute
+	   how much destination is misaligned.  */
+	movq	%rdi, %r8
+	andq	$(16 - 1), %r8
+	/* Get the negative of offset for alignment.  */
+	subq	$16, %r8
+	/* Adjust source.  */
+	subq	%r8, %rsi
+	/* Adjust destination which should be aligned now.  */
+	subq	%r8, %rdi
+	/* Adjust length.  */
+	addq	%r8, %rdx
+
+	.p2align 4
+L(loop_4x_vec_forward):
+	/* Copy 4 * 16 bytes a time forward.  */
+	VMOVU	(%rsi), %VEC(0)
+	VMOVU	16(%rsi), %VEC(1)
+	VMOVU	(16 * 2)(%rsi), %VEC(2)
+	VMOVU	(16 * 3)(%rsi), %VEC(3)
+	subq	$-(16 * 4), %rsi
+	addq	$-(16 * 4), %rdx
+	VMOVA	%VEC(0), (%rdi)
+	VMOVA	%VEC(1), 16(%rdi)
+	VMOVA	%VEC(2), (16 * 2)(%rdi)
+	VMOVA	%VEC(3), (16 * 3)(%rdi)
+	subq	$-(16 * 4), %rdi
+	cmpq	$(16 * 4), %rdx
+	ja	L(loop_4x_vec_forward)
+	/* Store the last 4 * 16 bytes.  */
+	VMOVU	%VEC(5), (%rcx)
+	VMOVU	%VEC(6), -16(%rcx)
+	VMOVU	%VEC(7), -(16 * 2)(%rcx)
+	VMOVU	%VEC(8), -(16 * 3)(%rcx)
+	/* Store the first 16 bytes.  */
+	VMOVU	%VEC(4), (%r11)
+	ret
+
+L(more_8x_vec_backward):
+	/* Load the first 4 * 16 bytes and the last 16 bytes to support
+	   overlapping addresses.  */
+	VMOVU	(%rsi), %VEC(4)
+	VMOVU	16(%rsi), %VEC(5)
+	VMOVU	(16 * 2)(%rsi), %VEC(6)
+	VMOVU	(16 * 3)(%rsi), %VEC(7)
+	VMOVU	-16(%rsi,%rdx), %VEC(8)
+	/* Save stop of the destination buffer.  */
+	leaq	-16(%rdi, %rdx), %r11
+	/* Align destination end for aligned stores in the loop.  Compute
+	   how much destination end is misaligned.  */
+	leaq	-16(%rsi, %rdx), %rcx
+	movq	%r11, %r9
+	movq	%r11, %r8
+	andq	$(16 - 1), %r8
+	/* Adjust source.  */
+	subq	%r8, %rcx
+	/* Adjust the end of destination which should be aligned now.  */
+	subq	%r8, %r9
+	/* Adjust length.  */
+	subq	%r8, %rdx
+
+	.p2align 4
+L(loop_4x_vec_backward):
+	/* Copy 4 * 16 bytes a time backward.  */
+	VMOVU	(%rcx), %VEC(0)
+	VMOVU	-16(%rcx), %VEC(1)
+	VMOVU	-(16 * 2)(%rcx), %VEC(2)
+	VMOVU	-(16 * 3)(%rcx), %VEC(3)
+	addq	$-(16 * 4), %rcx
+	addq	$-(16 * 4), %rdx
+	VMOVA	%VEC(0), (%r9)
+	VMOVA	%VEC(1), -16(%r9)
+	VMOVA	%VEC(2), -(16 * 2)(%r9)
+	VMOVA	%VEC(3), -(16 * 3)(%r9)
+	addq	$-(16 * 4), %r9
+	cmpq	$(16 * 4), %rdx
+	ja	L(loop_4x_vec_backward)
+	/* Store the first 4 * 16 bytes.  */
+	VMOVU	%VEC(4), (%rdi)
+	VMOVU	%VEC(5), 16(%rdi)
+	VMOVU	%VEC(6), (16 * 2)(%rdi)
+	VMOVU	%VEC(7), (16 * 3)(%rdi)
+	/* Store the last 16 bytes.  */
+	VMOVU	%VEC(8), (%r11)
+	ret
+
+#if IS_IN (libc)
+	.p2align 4
+L(large_memcpy_2x):
+	/* Compute absolute value of difference between source and
+	   destination.  */
+	movq	%rdi, %r9
+	subq	%rsi, %r9
+	movq	%r9, %r8
+	leaq	-1(%r9), %rcx
+	sarq	$63, %r8
+	xorq	%r8, %r9
+	subq	%r8, %r9
+	/* Don't use non-temporal store if there is overlap between
+	   destination and source since destination may be in cache when
+	   source is loaded.  */
+	cmpq	%r9, %rdx
+	ja	L(more_8x_vec_check)
+
+	/* Cache align destination. First store the first 64 bytes then
+	   adjust alignments.  */
+	VMOVU	(%rsi), %VEC(8)
+	VMOVU	16(%rsi), %VEC(9)
+	VMOVU	(16 * 2)(%rsi), %VEC(10)
+	VMOVU	(16 * 3)(%rsi), %VEC(11)
+	VMOVU	%VEC(8), (%rdi)
+	VMOVU	%VEC(9), 16(%rdi)
+	VMOVU	%VEC(10), (16 * 2)(%rdi)
+	VMOVU	%VEC(11), (16 * 3)(%rdi)
+	/* Adjust source, destination, and size.  */
+	movq	%rdi, %r8
+	andq	$63, %r8
+	/* Get the negative of offset for alignment.  */
+	subq	$64, %r8
+	/* Adjust source.  */
+	subq	%r8, %rsi
+	/* Adjust destination which should be aligned now.  */
+	subq	%r8, %rdi
+	/* Adjust length.  */
+	addq	%r8, %rdx
+
+	/* Test if source and destination addresses will alias. If they do
+	   the larger pipeline in large_memcpy_4x alleviated the
+	   performance drop.  */
+	testl	$(PAGE_SIZE - 8 * 16), %ecx
+	jz	L(large_memcpy_4x)
+
+	movq	%rdx, %r10
+	shrq	$LOG_4X_MEMCPY_THRESH, %r10
+	cmp	__x86_shared_non_temporal_threshold(%rip), %r10
+	jae	L(large_memcpy_4x)
+
+	/* edx will store remainder size for copying tail.  */
+	andl	$(PAGE_SIZE * 2 - 1), %edx
+	/* r10 stores outer loop counter.  */
+	shrq	$((LOG_PAGE_SIZE + 1) - LOG_4X_MEMCPY_THRESH), %r10
+	/* Copy 4 * 16 bytes at a time from 2 pages.  */
+	.p2align 4
+L(loop_large_memcpy_2x_outer):
+	/* ecx stores inner loop counter.  */
+	movl	$(PAGE_SIZE / LARGE_LOAD_SIZE), %ecx
+L(loop_large_memcpy_2x_inner):
+	PREFETCH_ONE_SET(1, (%rsi), PREFETCHED_LOAD_SIZE)
+	PREFETCH_ONE_SET(1, (%rsi), PREFETCHED_LOAD_SIZE * 2)
+	PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE + PREFETCHED_LOAD_SIZE)
+	PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE + PREFETCHED_LOAD_SIZE * 2)
+	/* Load vectors from rsi.  */
+	LOAD_ONE_SET((%rsi), 0, %VEC(0), %VEC(1), %VEC(2), %VEC(3))
+	LOAD_ONE_SET((%rsi), PAGE_SIZE, %VEC(4), %VEC(5), %VEC(6), %VEC(7))
+	subq	$-LARGE_LOAD_SIZE, %rsi
+	/* Non-temporal store vectors to rdi.  */
+	STORE_ONE_SET((%rdi), 0, %VEC(0), %VEC(1), %VEC(2), %VEC(3))
+	STORE_ONE_SET((%rdi), PAGE_SIZE, %VEC(4), %VEC(5), %VEC(6), %VEC(7))
+	subq	$-LARGE_LOAD_SIZE, %rdi
+	decl	%ecx
+	jnz	L(loop_large_memcpy_2x_inner)
+	addq	$PAGE_SIZE, %rdi
+	addq	$PAGE_SIZE, %rsi
+	decq	%r10
+	jne	L(loop_large_memcpy_2x_outer)
+	sfence
+
+	/* Check if only last 4 loads are needed.  */
+	cmpl	$(16 * 4), %edx
+	jbe	L(large_memcpy_2x_end)
+
+	/* Handle the last 2 * PAGE_SIZE bytes.  */
+L(loop_large_memcpy_2x_tail):
+	/* Copy 4 * 16 bytes a time forward with non-temporal stores.  */
+	PREFETCH_ONE_SET (1, (%rsi), PREFETCHED_LOAD_SIZE)
+	PREFETCH_ONE_SET (1, (%rdi), PREFETCHED_LOAD_SIZE)
+	VMOVU	(%rsi), %VEC(0)
+	VMOVU	16(%rsi), %VEC(1)
+	VMOVU	(16 * 2)(%rsi), %VEC(2)
+	VMOVU	(16 * 3)(%rsi), %VEC(3)
+	subq	$-(16 * 4), %rsi
+	addl	$-(16 * 4), %edx
+	VMOVA	%VEC(0), (%rdi)
+	VMOVA	%VEC(1), 16(%rdi)
+	VMOVA	%VEC(2), (16 * 2)(%rdi)
+	VMOVA	%VEC(3), (16 * 3)(%rdi)
+	subq	$-(16 * 4), %rdi
+	cmpl	$(16 * 4), %edx
+	ja	L(loop_large_memcpy_2x_tail)
+
+L(large_memcpy_2x_end):
+	/* Store the last 4 * 16 bytes.  */
+	VMOVU	-(16 * 4)(%rsi, %rdx), %VEC(0)
+	VMOVU	-(16 * 3)(%rsi, %rdx), %VEC(1)
+	VMOVU	-(16 * 2)(%rsi, %rdx), %VEC(2)
+	VMOVU	-16(%rsi, %rdx), %VEC(3)
+
+	VMOVU	%VEC(0), -(16 * 4)(%rdi, %rdx)
+	VMOVU	%VEC(1), -(16 * 3)(%rdi, %rdx)
+	VMOVU	%VEC(2), -(16 * 2)(%rdi, %rdx)
+	VMOVU	%VEC(3), -16(%rdi, %rdx)
+	ret
+
+	.p2align 4
+L(large_memcpy_4x):
+	movq	%rdx, %r10
+	/* edx will store remainder size for copying tail.  */
+	andl	$(PAGE_SIZE * 4 - 1), %edx
+	/* r10 stores outer loop counter.  */
+	shrq	$(LOG_PAGE_SIZE + 2), %r10
+	/* Copy 4 * 16 bytes at a time from 4 pages.  */
+	.p2align 4
+L(loop_large_memcpy_4x_outer):
+	/* ecx stores inner loop counter.  */
+	movl	$(PAGE_SIZE / LARGE_LOAD_SIZE), %ecx
+L(loop_large_memcpy_4x_inner):
+	/* Only one prefetch set per page as doing 4 pages give more time
+	   for prefetcher to keep up.  */
+	PREFETCH_ONE_SET(1, (%rsi), PREFETCHED_LOAD_SIZE)
+	PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE + PREFETCHED_LOAD_SIZE)
+	PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE * 2 + PREFETCHED_LOAD_SIZE)
+	PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE * 3 + PREFETCHED_LOAD_SIZE)
+	/* Load vectors from rsi.  */
+	LOAD_ONE_SET((%rsi), 0, %VEC(0), %VEC(1), %VEC(2), %VEC(3))
+	LOAD_ONE_SET((%rsi), PAGE_SIZE, %VEC(4), %VEC(5), %VEC(6), %VEC(7))
+	LOAD_ONE_SET((%rsi), PAGE_SIZE * 2, %VEC(8), %VEC(9), %VEC(10), %VEC(11))
+	LOAD_ONE_SET((%rsi), PAGE_SIZE * 3, %VEC(12), %VEC(13), %VEC(14), %VEC(15))
+	subq	$-LARGE_LOAD_SIZE, %rsi
+	/* Non-temporal store vectors to rdi.  */
+	STORE_ONE_SET((%rdi), 0, %VEC(0), %VEC(1), %VEC(2), %VEC(3))
+	STORE_ONE_SET((%rdi), PAGE_SIZE, %VEC(4), %VEC(5), %VEC(6), %VEC(7))
+	STORE_ONE_SET((%rdi), PAGE_SIZE * 2, %VEC(8), %VEC(9), %VEC(10), %VEC(11))
+	STORE_ONE_SET((%rdi), PAGE_SIZE * 3, %VEC(12), %VEC(13), %VEC(14), %VEC(15))
+	subq	$-LARGE_LOAD_SIZE, %rdi
+	decl	%ecx
+	jnz	L(loop_large_memcpy_4x_inner)
+	addq	$(PAGE_SIZE * 3), %rdi
+	addq	$(PAGE_SIZE * 3), %rsi
+	decq	%r10
+	jne	L(loop_large_memcpy_4x_outer)
+	sfence
+	/* Check if only last 4 loads are needed.  */
+	cmpl	$(16 * 4), %edx
+	jbe	L(large_memcpy_4x_end)
+
+	/* Handle the last 4  * PAGE_SIZE bytes.  */
+L(loop_large_memcpy_4x_tail):
+	/* Copy 4 * 16 bytes a time forward with non-temporal stores.  */
+	PREFETCH_ONE_SET (1, (%rsi), PREFETCHED_LOAD_SIZE)
+	PREFETCH_ONE_SET (1, (%rdi), PREFETCHED_LOAD_SIZE)
+	VMOVU	(%rsi), %VEC(0)
+	VMOVU	16(%rsi), %VEC(1)
+	VMOVU	(16 * 2)(%rsi), %VEC(2)
+	VMOVU	(16 * 3)(%rsi), %VEC(3)
+	subq	$-(16 * 4), %rsi
+	addl	$-(16 * 4), %edx
+	VMOVA	%VEC(0), (%rdi)
+	VMOVA	%VEC(1), 16(%rdi)
+	VMOVA	%VEC(2), (16 * 2)(%rdi)
+	VMOVA	%VEC(3), (16 * 3)(%rdi)
+	subq	$-(16 * 4), %rdi
+	cmpl	$(16 * 4), %edx
+	ja	L(loop_large_memcpy_4x_tail)
+
+L(large_memcpy_4x_end):
+	/* Store the last 4 * 16 bytes.  */
+	VMOVU	-(16 * 4)(%rsi, %rdx), %VEC(0)
+	VMOVU	-(16 * 3)(%rsi, %rdx), %VEC(1)
+	VMOVU	-(16 * 2)(%rsi, %rdx), %VEC(2)
+	VMOVU	-16(%rsi, %rdx), %VEC(3)
+
+	VMOVU	%VEC(0), -(16 * 4)(%rdi, %rdx)
+	VMOVU	%VEC(1), -(16 * 3)(%rdi, %rdx)
+	VMOVU	%VEC(2), -(16 * 2)(%rdi, %rdx)
+	VMOVU	%VEC(3), -16(%rdi, %rdx)
+	ret
+#endif
+END (MEMMOVE_SYMBOL (__memmove, unaligned_erms))
+
+#if IS_IN (libc)
+# ifdef USE_MULTIARCH
+strong_alias (MEMMOVE_SYMBOL (__memmove, unaligned_erms),
+	      MEMMOVE_SYMBOL (__memcpy, unaligned_erms))
+#  ifdef SHARED
+strong_alias (MEMMOVE_SYMBOL (__memmove_chk, unaligned_erms),
+	      MEMMOVE_SYMBOL (__memcpy_chk, unaligned_erms))
+#  endif
+# endif
+# ifdef SHARED
+strong_alias (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned),
+	      MEMMOVE_CHK_SYMBOL (__memcpy_chk, unaligned))
+# endif
+#endif
+strong_alias (MEMMOVE_SYMBOL (__memmove, unaligned),
+	      MEMCPY_SYMBOL (__memcpy, unaligned))
 
 #ifndef USE_MULTIARCH
 libc_hidden_builtin_def (memmove)
diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
index abde8438d4..766d6ee4ac 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
@@ -69,11 +69,7 @@
 #endif
 
 #ifndef VZEROUPPER
-# if VEC_SIZE > 16
-#  define VZEROUPPER vzeroupper
-# else
-#  define VZEROUPPER
-# endif
+# define VZEROUPPER vzeroupper
 #endif
 
 #ifndef PAGE_SIZE
@@ -104,9 +100,9 @@
 # define LOG_4X_MEMCPY_THRESH 4
 #endif
 
-/* Avoid short distance rep movsb only with non-SSE vector.  */
+/* Avoid short distance rep movsb.  */
 #ifndef AVOID_SHORT_DISTANCE_REP_MOVSB
-# define AVOID_SHORT_DISTANCE_REP_MOVSB (VEC_SIZE > 16)
+# define AVOID_SHORT_DISTANCE_REP_MOVSB 1
 #else
 # define AVOID_SHORT_DISTANCE_REP_MOVSB 0
 #endif
@@ -170,7 +166,7 @@
 #endif
 
 	.section SECTION(.text),"ax",@progbits
-#if defined SHARED && IS_IN (libc)
+#ifdef SHARED
 ENTRY (MEMMOVE_CHK_SYMBOL (__mempcpy_chk, unaligned))
 	cmp	%RDX_LP, %RCX_LP
 	jb	HIDDEN_JUMPTARGET (__chk_fail)
@@ -183,7 +179,7 @@ ENTRY (MEMPCPY_SYMBOL (__mempcpy, unaligned))
 	jmp	L(start)
 END (MEMPCPY_SYMBOL (__mempcpy, unaligned))
 
-#if defined SHARED && IS_IN (libc)
+#ifdef SHARED
 ENTRY (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned))
 	cmp	%RDX_LP, %RCX_LP
 	jb	HIDDEN_JUMPTARGET (__chk_fail)
@@ -193,88 +189,28 @@ END (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned))
 ENTRY (MEMMOVE_SYMBOL (__memmove, unaligned))
 	movq	%rdi, %rax
 L(start):
-# ifdef __ILP32__
+#ifdef __ILP32__
 	/* Clear the upper 32 bits.  */
 	movl	%edx, %edx
-# endif
+#endif
 	cmp	$VEC_SIZE, %RDX_LP
 	jb	L(less_vec)
 	cmp	$(VEC_SIZE * 2), %RDX_LP
 	ja	L(more_2x_vec)
-#if !defined USE_MULTIARCH || !IS_IN (libc)
-L(last_2x_vec):
-#endif
 	/* From VEC and to 2 * VEC.  No branch when size == VEC_SIZE.  */
 	VMOVU	(%rsi), %VEC(0)
 	VMOVU	-VEC_SIZE(%rsi,%rdx), %VEC(1)
 	VMOVU	%VEC(0), (%rdi)
 	VMOVU	%VEC(1), -VEC_SIZE(%rdi,%rdx)
-#if !defined USE_MULTIARCH || !IS_IN (libc)
-L(nop):
-	ret
-#else
 	VZEROUPPER_RETURN
-#endif
-#if defined USE_MULTIARCH && IS_IN (libc)
 END (MEMMOVE_SYMBOL (__memmove, unaligned))
 
-# if VEC_SIZE == 16
-ENTRY (__mempcpy_chk_erms)
-	cmp	%RDX_LP, %RCX_LP
-	jb	HIDDEN_JUMPTARGET (__chk_fail)
-END (__mempcpy_chk_erms)
-
-/* Only used to measure performance of REP MOVSB.  */
-ENTRY (__mempcpy_erms)
-	mov	%RDI_LP, %RAX_LP
-	/* Skip zero length.  */
-	test	%RDX_LP, %RDX_LP
-	jz	2f
-	add	%RDX_LP, %RAX_LP
-	jmp	L(start_movsb)
-END (__mempcpy_erms)
-
-ENTRY (__memmove_chk_erms)
-	cmp	%RDX_LP, %RCX_LP
-	jb	HIDDEN_JUMPTARGET (__chk_fail)
-END (__memmove_chk_erms)
-
-ENTRY (__memmove_erms)
-	movq	%rdi, %rax
-	/* Skip zero length.  */
-	test	%RDX_LP, %RDX_LP
-	jz	2f
-L(start_movsb):
-	mov	%RDX_LP, %RCX_LP
-	cmp	%RSI_LP, %RDI_LP
-	jb	1f
-	/* Source == destination is less common.  */
-	je	2f
-	lea	(%rsi,%rcx), %RDX_LP
-	cmp	%RDX_LP, %RDI_LP
-	jb	L(movsb_backward)
-1:
-	rep movsb
-2:
-	ret
-L(movsb_backward):
-	leaq	-1(%rdi,%rcx), %rdi
-	leaq	-1(%rsi,%rcx), %rsi
-	std
-	rep movsb
-	cld
-	ret
-END (__memmove_erms)
-strong_alias (__memmove_erms, __memcpy_erms)
-strong_alias (__memmove_chk_erms, __memcpy_chk_erms)
-# endif
-
-# ifdef SHARED
+#ifdef SHARED
 ENTRY (MEMMOVE_CHK_SYMBOL (__mempcpy_chk, unaligned_erms))
 	cmp	%RDX_LP, %RCX_LP
 	jb	HIDDEN_JUMPTARGET (__chk_fail)
 END (MEMMOVE_CHK_SYMBOL (__mempcpy_chk, unaligned_erms))
-# endif
+#endif
 
 ENTRY (MEMMOVE_SYMBOL (__mempcpy, unaligned_erms))
 	mov	%RDI_LP, %RAX_LP
@@ -282,20 +218,20 @@ ENTRY (MEMMOVE_SYMBOL (__mempcpy, unaligned_erms))
 	jmp	L(start_erms)
 END (MEMMOVE_SYMBOL (__mempcpy, unaligned_erms))
 
-# ifdef SHARED
+#ifdef SHARED
 ENTRY (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned_erms))
 	cmp	%RDX_LP, %RCX_LP
 	jb	HIDDEN_JUMPTARGET (__chk_fail)
 END (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned_erms))
-# endif
+#endif
 
 ENTRY (MEMMOVE_SYMBOL (__memmove, unaligned_erms))
 	movq	%rdi, %rax
 L(start_erms):
-# ifdef __ILP32__
+#ifdef __ILP32__
 	/* Clear the upper 32 bits.  */
 	movl	%edx, %edx
-# endif
+#endif
 	cmp	$VEC_SIZE, %RDX_LP
 	jb	L(less_vec)
 	cmp	$(VEC_SIZE * 2), %RDX_LP
@@ -307,11 +243,7 @@ L(last_2x_vec):
 	VMOVU	%VEC(0), (%rdi)
 	VMOVU	%VEC(1), -VEC_SIZE(%rdi,%rdx)
 L(return):
-#if VEC_SIZE > 16
 	ZERO_UPPER_VEC_REGISTERS_RETURN
-#else
-	ret
-#endif
 
 L(movsb):
 	cmp     __x86_rep_movsb_stop_threshold(%rip), %RDX_LP
@@ -324,15 +256,15 @@ L(movsb):
 	cmpq	%r9, %rdi
 	/* Avoid slow backward REP MOVSB.  */
 	jb	L(more_8x_vec_backward)
-# if AVOID_SHORT_DISTANCE_REP_MOVSB
+#if AVOID_SHORT_DISTANCE_REP_MOVSB
 	testl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
 	jz	3f
 	movq	%rdi, %rcx
 	subq	%rsi, %rcx
 	jmp	2f
-# endif
+#endif
 1:
-# if AVOID_SHORT_DISTANCE_REP_MOVSB
+#if AVOID_SHORT_DISTANCE_REP_MOVSB
 	testl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
 	jz	3f
 	movq	%rsi, %rcx
@@ -343,26 +275,23 @@ L(movsb):
 	cmpl	$63, %ecx
 	jbe	L(more_2x_vec)	/* Avoid "rep movsb" if ECX <= 63.  */
 3:
-# endif
+#endif
 	mov	%RDX_LP, %RCX_LP
 	rep movsb
 L(nop):
 	ret
-#endif
 
 L(less_vec):
 	/* Less than 1 VEC.  */
-#if VEC_SIZE != 16 && VEC_SIZE != 32 && VEC_SIZE != 64
+#if VEC_SIZE != 32 && VEC_SIZE != 64
 # error Unsupported VEC_SIZE!
 #endif
 #if VEC_SIZE > 32
 	cmpb	$32, %dl
 	jae	L(between_32_63)
 #endif
-#if VEC_SIZE > 16
 	cmpb	$16, %dl
 	jae	L(between_16_31)
-#endif
 	cmpb	$8, %dl
 	jae	L(between_8_15)
 	cmpb	$4, %dl
@@ -383,7 +312,6 @@ L(between_32_63):
 	VMOVU	%YMM1, -32(%rdi,%rdx)
 	VZEROUPPER_RETURN
 #endif
-#if VEC_SIZE > 16
 	/* From 16 to 31.  No branch when size == 16.  */
 L(between_16_31):
 	VMOVU	(%rsi), %XMM0
@@ -391,7 +319,6 @@ L(between_16_31):
 	VMOVU	%XMM0, (%rdi)
 	VMOVU	%XMM1, -16(%rdi,%rdx)
 	VZEROUPPER_RETURN
-#endif
 L(between_8_15):
 	/* From 8 to 15.  No branch when size == 8.  */
 	movq	-8(%rsi,%rdx), %rcx
@@ -414,11 +341,9 @@ L(between_2_3):
 	movw	%si, (%rdi)
 	ret
 
-#if defined USE_MULTIARCH && IS_IN (libc)
 L(movsb_more_2x_vec):
 	cmp	__x86_rep_movsb_threshold(%rip), %RDX_LP
 	ja	L(movsb)
-#endif
 L(more_2x_vec):
 	/* More than 2 * VEC and there may be overlap between destination
 	   and source.  */
@@ -458,13 +383,11 @@ L(last_4x_vec):
 
 L(more_8x_vec):
 	/* Check if non-temporal move candidate.  */
-#if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
 	/* Check non-temporal store threshold.  */
 	cmp __x86_shared_non_temporal_threshold(%rip), %RDX_LP
 	ja	L(large_memcpy_2x)
-#endif
 	/* Entry if rdx is greater than non-temporal threshold but there
-       is overlap.  */
+	   is overlap.  */
 L(more_8x_vec_check):
 	cmpq	%rsi, %rdi
 	ja	L(more_8x_vec_backward)
@@ -566,7 +489,6 @@ L(loop_4x_vec_backward):
 	VMOVU	%VEC(8), (%r11)
 	VZEROUPPER_RETURN
 
-#if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
 	.p2align 4
 L(large_memcpy_2x):
 	/* Compute absolute value of difference between source and
@@ -589,18 +511,10 @@ L(large_memcpy_2x):
 	VMOVU	(%rsi), %VEC(8)
 #if VEC_SIZE < 64
 	VMOVU	VEC_SIZE(%rsi), %VEC(9)
-#if VEC_SIZE < 32
-	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(10)
-	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(11)
-#endif
 #endif
 	VMOVU	%VEC(8), (%rdi)
 #if VEC_SIZE < 64
 	VMOVU	%VEC(9), VEC_SIZE(%rdi)
-#if VEC_SIZE < 32
-	VMOVU	%VEC(10), (VEC_SIZE * 2)(%rdi)
-	VMOVU	%VEC(11), (VEC_SIZE * 3)(%rdi)
-#endif
 #endif
 	/* Adjust source, destination, and size.  */
 	movq	%rdi, %r8
@@ -764,22 +678,15 @@ L(large_memcpy_4x_end):
 	VMOVU	%VEC(2), -(VEC_SIZE * 2)(%rdi, %rdx)
 	VMOVU	%VEC(3), -VEC_SIZE(%rdi, %rdx)
 	VZEROUPPER_RETURN
-#endif
 END (MEMMOVE_SYMBOL (__memmove, unaligned_erms))
 
-#if IS_IN (libc)
-# ifdef USE_MULTIARCH
 strong_alias (MEMMOVE_SYMBOL (__memmove, unaligned_erms),
 	      MEMMOVE_SYMBOL (__memcpy, unaligned_erms))
-#  ifdef SHARED
+#ifdef SHARED
 strong_alias (MEMMOVE_SYMBOL (__memmove_chk, unaligned_erms),
 	      MEMMOVE_SYMBOL (__memcpy_chk, unaligned_erms))
-#  endif
-# endif
-# ifdef SHARED
 strong_alias (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned),
 	      MEMMOVE_CHK_SYMBOL (__memcpy_chk, unaligned))
-# endif
 #endif
 strong_alias (MEMMOVE_SYMBOL (__memmove, unaligned),
 	      MEMCPY_SYMBOL (__memcpy, unaligned))
-- 
2.25.1

