From 7e9470052d743249da4fa0be22bcb0dd7250db18 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Sun, 22 Aug 2021 07:32:33 -0400
Subject: [PATCH 2/9] only small vec

---
 .../multiarch/memmove-vec-unaligned-erms.S    | 1040 +++++++++--------
 1 file changed, 536 insertions(+), 504 deletions(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
index 9f02624375..1383f0d32e 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
@@ -49,23 +49,23 @@
 #include <sysdep.h>
 
 #ifndef MEMCPY_SYMBOL
-# define MEMCPY_SYMBOL(p,s)		MEMMOVE_SYMBOL(p, s)
+# define MEMCPY_SYMBOL(p,s) MEMMOVE_SYMBOL(p, s)
 #endif
 
 #ifndef MEMPCPY_SYMBOL
-# define MEMPCPY_SYMBOL(p,s)		MEMMOVE_SYMBOL(p, s)
+# define MEMPCPY_SYMBOL(p,s) MEMMOVE_SYMBOL(p, s)
 #endif
 
 #ifndef MEMMOVE_CHK_SYMBOL
-# define MEMMOVE_CHK_SYMBOL(p,s)	MEMMOVE_SYMBOL(p, s)
+# define MEMMOVE_CHK_SYMBOL(p,s) MEMMOVE_SYMBOL(p, s)
 #endif
 
 #ifndef XMM0
-# define XMM0				xmm0
+# define XMM0 xmm0
 #endif
 
 #ifndef YMM0
-# define YMM0				ymm0
+# define YMM0 ymm0
 #endif
 
 #ifndef VZEROUPPER
@@ -125,17 +125,17 @@
 #if PREFETCH_SIZE == 64
 # if PREFETCHED_LOAD_SIZE == PREFETCH_SIZE
 #  define PREFETCH_ONE_SET(dir, base, offset) \
-	PREFETCH ((offset)base)
+PREFETCH ((offset)base)
 # elif PREFETCHED_LOAD_SIZE == 2 * PREFETCH_SIZE
 #  define PREFETCH_ONE_SET(dir, base, offset) \
-	PREFETCH ((offset)base); \
-	PREFETCH ((offset + dir * PREFETCH_SIZE)base)
+PREFETCH ((offset)base); \
+PREFETCH ((offset + dir * PREFETCH_SIZE)base)
 # elif PREFETCHED_LOAD_SIZE == 4 * PREFETCH_SIZE
 #  define PREFETCH_ONE_SET(dir, base, offset) \
-	PREFETCH ((offset)base); \
-	PREFETCH ((offset + dir * PREFETCH_SIZE)base); \
-	PREFETCH ((offset + dir * PREFETCH_SIZE * 2)base); \
-	PREFETCH ((offset + dir * PREFETCH_SIZE * 3)base)
+PREFETCH ((offset)base); \
+PREFETCH ((offset + dir * PREFETCH_SIZE)base); \
+PREFETCH ((offset + dir * PREFETCH_SIZE * 2)base); \
+PREFETCH ((offset + dir * PREFETCH_SIZE * 3)base)
 # else
 #   error Unsupported PREFETCHED_LOAD_SIZE!
 # endif
@@ -145,22 +145,22 @@
 
 #if LARGE_LOAD_SIZE == (VEC_SIZE * 2)
 # define LOAD_ONE_SET(base, offset, vec0, vec1, ...) \
-	VMOVU	(offset)base, vec0; \
-	VMOVU	((offset) + VEC_SIZE)base, vec1;
+VMOVU (offset)base, vec0; \
+VMOVU ((offset) + VEC_SIZE)base, vec1;
 # define STORE_ONE_SET(base, offset, vec0, vec1, ...) \
-	VMOVNT  vec0, (offset)base; \
-	VMOVNT  vec1, ((offset) + VEC_SIZE)base;
+VMOVNT  vec0, (offset)base; \
+VMOVNT  vec1, ((offset) + VEC_SIZE)base;
 #elif LARGE_LOAD_SIZE == (VEC_SIZE * 4)
 # define LOAD_ONE_SET(base, offset, vec0, vec1, vec2, vec3) \
-	VMOVU	(offset)base, vec0; \
-	VMOVU	((offset) + VEC_SIZE)base, vec1; \
-	VMOVU	((offset) + VEC_SIZE * 2)base, vec2; \
-	VMOVU	((offset) + VEC_SIZE * 3)base, vec3;
+VMOVU (offset)base, vec0; \
+VMOVU ((offset) + VEC_SIZE)base, vec1; \
+VMOVU ((offset) + VEC_SIZE * 2)base, vec2; \
+VMOVU ((offset) + VEC_SIZE * 3)base, vec3;
 # define STORE_ONE_SET(base, offset, vec0, vec1, vec2, vec3) \
-	VMOVNT	vec0, (offset)base; \
-	VMOVNT	vec1, ((offset) + VEC_SIZE)base; \
-	VMOVNT	vec2, ((offset) + VEC_SIZE * 2)base; \
-	VMOVNT	vec3, ((offset) + VEC_SIZE * 3)base;
+VMOVNT vec0, (offset)base; \
+VMOVNT vec1, ((offset) + VEC_SIZE)base; \
+VMOVNT vec2, ((offset) + VEC_SIZE * 2)base; \
+VMOVNT vec3, ((offset) + VEC_SIZE * 3)base;
 #else
 # error Invalid LARGE_LOAD_SIZE
 #endif
@@ -169,101 +169,118 @@
 # error SECTION is not defined!
 #endif
 
-	.section SECTION(.text),"ax",@progbits
+#define COPY_BLOCK(mov_inst, src_reg, dst_reg, size_reg, len, tmp_reg0, tmp_reg1) \
+mov_inst (%src_reg), %tmp_reg0; \
+mov_inst -(len)(%src_reg, %size_reg), %tmp_reg1; \
+mov_inst %tmp_reg0, (%dst_reg); \
+mov_inst %tmp_reg1, -(len)(%dst_reg, %size_reg);
+
+
+#define COPY_4_8 COPY_BLOCK(movl, rsi, rdi, rdx, 4, ecx, esi)
+#define COPY_8_16 COPY_BLOCK(movq, rsi, rdi, rdx, 8, rcx, rsi)
+#define COPY_16_32 COPY_BLOCK(vmovdqu, rsi, rdi, rdx, 16, xmm0, xmm1)
+#define COPY_32_64 COPY_BLOCK(vmovdqu64, rsi, rdi, rdx, 32, ymm16, ymm17)
+
+
+.section SECTION(.text),"ax",@progbits
 #if defined SHARED && IS_IN (libc)
 ENTRY (MEMMOVE_CHK_SYMBOL (__mempcpy_chk, unaligned))
-	cmp	%RDX_LP, %RCX_LP
-	jb	HIDDEN_JUMPTARGET (__chk_fail)
+cmp %RDX_LP, %RCX_LP
+jb HIDDEN_JUMPTARGET (__chk_fail)
 END (MEMMOVE_CHK_SYMBOL (__mempcpy_chk, unaligned))
 #endif
 
 ENTRY (MEMPCPY_SYMBOL (__mempcpy, unaligned))
-	mov	%RDI_LP, %RAX_LP
-	add	%RDX_LP, %RAX_LP
-	jmp	L(start)
+mov %RDI_LP, %RAX_LP
+add %RDX_LP, %RAX_LP
+jmp L(start)
 END (MEMPCPY_SYMBOL (__mempcpy, unaligned))
 
 #if defined SHARED && IS_IN (libc)
 ENTRY (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned))
-	cmp	%RDX_LP, %RCX_LP
-	jb	HIDDEN_JUMPTARGET (__chk_fail)
+cmp %RDX_LP, %RCX_LP
+jb HIDDEN_JUMPTARGET (__chk_fail)
 END (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned))
 #endif
 
 ENTRY (MEMMOVE_SYMBOL (__memmove, unaligned))
-	movq	%rdi, %rax
+movq %rdi, %rax
 L(start):
 # ifdef __ILP32__
-	/* Clear the upper 32 bits.  */
-	movl	%edx, %edx
+/* Clear the upper 32 bits.  */
+movl %edx, %edx
 # endif
-	cmp	$VEC_SIZE, %RDX_LP
-	jb	L(less_vec)
-	cmp	$(VEC_SIZE * 2), %RDX_LP
-	ja	L(more_2x_vec)
+cmp $VEC_SIZE, %RDX_LP
+#if VEC_SIZE > 16
+jbe L(leq_1x)
+#else
+jb L(leq_1x)
+#endif
+cmp $(VEC_SIZE * 2), %RDX_LP
+ja L(more_2x_vec)
 #if !defined USE_MULTIARCH || !IS_IN (libc)
 L(last_2x_vec):
 #endif
-	/* From VEC and to 2 * VEC.  No branch when size == VEC_SIZE.  */
-	VMOVU	(%rsi), %VEC(0)
-	VMOVU	-VEC_SIZE(%rsi,%rdx), %VEC(1)
-	VMOVU	%VEC(0), (%rdi)
-	VMOVU	%VEC(1), -VEC_SIZE(%rdi,%rdx)
+/* From VEC and to 2 * VEC.  No branch when size == VEC_SIZE.  */
+VMOVU (%rsi), %VEC(0)
+VMOVU -VEC_SIZE(%rsi,%rdx), %VEC(1)
+VMOVU %VEC(0), (%rdi)
+VMOVU %VEC(1), -VEC_SIZE(%rdi,%rdx)
 #if !defined USE_MULTIARCH || !IS_IN (libc)
 L(nop):
-	ret
+ret
 #else
-	VZEROUPPER_RETURN
+VZEROUPPER_RETURN
 #endif
 #if defined USE_MULTIARCH && IS_IN (libc)
 END (MEMMOVE_SYMBOL (__memmove, unaligned))
 
 # if VEC_SIZE == 16
 ENTRY (__mempcpy_chk_erms)
-	cmp	%RDX_LP, %RCX_LP
-	jb	HIDDEN_JUMPTARGET (__chk_fail)
+cmp %RDX_LP, %RCX_LP
+jb HIDDEN_JUMPTARGET (__chk_fail)
 END (__mempcpy_chk_erms)
 
 /* Only used to measure performance of REP MOVSB.  */
 ENTRY (__mempcpy_erms)
-	mov	%RDI_LP, %RAX_LP
-	/* Skip zero length.  */
-	test	%RDX_LP, %RDX_LP
-	jz	2f
-	add	%RDX_LP, %RAX_LP
-	jmp	L(start_movsb)
+mov %RDI_LP, %RAX_LP
+/* Skip zero length.  */
+test %RDX_LP, %RDX_LP
+jz 2f
+add %RDX_LP, %RAX_LP
+jmp L(start_movsb)
 END (__mempcpy_erms)
 
 ENTRY (__memmove_chk_erms)
-	cmp	%RDX_LP, %RCX_LP
-	jb	HIDDEN_JUMPTARGET (__chk_fail)
+cmp %RDX_LP, %RCX_LP
+jb HIDDEN_JUMPTARGET (__chk_fail)
 END (__memmove_chk_erms)
 
 ENTRY (__memmove_erms)
-	movq	%rdi, %rax
-	/* Skip zero length.  */
-	test	%RDX_LP, %RDX_LP
-	jz	2f
+movq %rdi, %rax
+/* Skip zero length.  */
+test %RDX_LP, %RDX_LP
+jz 2f
 L(start_movsb):
-	mov	%RDX_LP, %RCX_LP
-	cmp	%RSI_LP, %RDI_LP
-	jb	1f
-	/* Source == destination is less common.  */
-	je	2f
-	lea	(%rsi,%rcx), %RDX_LP
-	cmp	%RDX_LP, %RDI_LP
-	jb	L(movsb_backward)
+mov %RDX_LP, %RCX_LP
+cmp %RSI_LP, %RDI_LP
+jb 1f
+/* Source == destination is less common.  */
+je 2f
+lea (%rsi,%rcx), %RDX_LP
+cmp %RDX_LP, %RDI_LP
+jb L(movsb_backward)
 1:
-	rep movsb
+rep movsb
 2:
-	ret
+ret
 L(movsb_backward):
-	leaq	-1(%rdi,%rcx), %rdi
-	leaq	-1(%rsi,%rcx), %rsi
-	std
-	rep movsb
-	cld
-	ret
+leaq -1(%rdi,%rcx), %rdi
+leaq -1(%rsi,%rcx), %rsi
+std
+rep movsb
+cld
+ret
 END (__memmove_erms)
 strong_alias (__memmove_erms, __memcpy_erms)
 strong_alias (__memmove_chk_erms, __memcpy_chk_erms)
@@ -271,515 +288,530 @@ strong_alias (__memmove_chk_erms, __memcpy_chk_erms)
 
 # ifdef SHARED
 ENTRY (MEMMOVE_CHK_SYMBOL (__mempcpy_chk, unaligned_erms))
-	cmp	%RDX_LP, %RCX_LP
-	jb	HIDDEN_JUMPTARGET (__chk_fail)
+cmp %RDX_LP, %RCX_LP
+jb HIDDEN_JUMPTARGET (__chk_fail)
 END (MEMMOVE_CHK_SYMBOL (__mempcpy_chk, unaligned_erms))
 # endif
 
 ENTRY (MEMMOVE_SYMBOL (__mempcpy, unaligned_erms))
-	mov	%RDI_LP, %RAX_LP
-	add	%RDX_LP, %RAX_LP
-	jmp	L(start_erms)
+mov %RDI_LP, %RAX_LP
+add %RDX_LP, %RAX_LP
+jmp L(start_erms)
 END (MEMMOVE_SYMBOL (__mempcpy, unaligned_erms))
 
 # ifdef SHARED
 ENTRY (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned_erms))
-	cmp	%RDX_LP, %RCX_LP
-	jb	HIDDEN_JUMPTARGET (__chk_fail)
+cmp %RDX_LP, %RCX_LP
+jb HIDDEN_JUMPTARGET (__chk_fail)
 END (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned_erms))
 # endif
 
-ENTRY (MEMMOVE_SYMBOL (__memmove, unaligned_erms))
-	movq	%rdi, %rax
+P2ALIGN_ENTRY (MEMMOVE_SYMBOL (__memmove, unaligned_erms), 6)
+movq %rdi, %rax
 L(start_erms):
 # ifdef __ILP32__
-	/* Clear the upper 32 bits.  */
-	movl	%edx, %edx
+/* Clear the upper 32 bits.  */
+movl %edx, %edx
 # endif
-	cmp	$VEC_SIZE, %RDX_LP
-	jb	L(less_vec)
-	cmp	$(VEC_SIZE * 2), %RDX_LP
-	ja	L(movsb_more_2x_vec)
+cmp $VEC_SIZE, %RDX_LP
+#if VEC_SIZE > 16
+jbe L(leq_1x)
+#else
+jb L(leq_1x)
+#endif
+cmp $(VEC_SIZE * 2), %RDX_LP
+ja L(movsb_more_2x_vec)
 L(last_2x_vec):
-	/* From VEC and to 2 * VEC.  No branch when size == VEC_SIZE. */
-	VMOVU	(%rsi), %VEC(0)
-	VMOVU	-VEC_SIZE(%rsi,%rdx), %VEC(1)
-	VMOVU	%VEC(0), (%rdi)
-	VMOVU	%VEC(1), -VEC_SIZE(%rdi,%rdx)
+/* From VEC and to 2 * VEC.  No branch when size == VEC_SIZE. */
+VMOVU (%rsi), %VEC(0)
+VMOVU (VEC_SIZE * -1)(%rsi, %rdx), %VEC(15)
+VMOVU %VEC(0), (VEC_SIZE * 0)(%rdi)
+VMOVU %VEC(15), (VEC_SIZE * -1)(%rdi, %rdx)
 L(return):
 #if VEC_SIZE > 16
-	ZERO_UPPER_VEC_REGISTERS_RETURN
+ZERO_UPPER_VEC_REGISTERS_RETURN
 #else
-	ret
+ret
+#endif
 #endif
+#if VEC_SIZE == 64
+L(copy_8_15):
+COPY_8_16
+ret
+
+L(copy_32_63):
+vmovdqu64 (%rsi), %ymm16
+vmovdqu64 -32(%rsi, %rdx), %ymm17
+vmovdqu64 %ymm16, (%rdi)
+vmovdqu64 %ymm17, -32(%rdi, %rdx)
+ret
+#endif
+
+.p2align 4,, 6
+L(leq_1x):
+#if VEC_SIZE != 16 && VEC_SIZE != 32 && VEC_SIZE != 64
+# error Unsupported VEC_SIZE!
+#endif
+cmpl $(VEC_SIZE / 4), %edx
+jb L(lt_quarter_x)
 
+cmpl $(VEC_SIZE / 2), %edx
+#if VEC_SIZE == 64
+ja L(copy_32_63)
+COPY_16_32
+#elif VEC_SIZE == 32
+jb L(copy_8_15)
+COPY_16_32
+#else
+jb L(copy_4_7)
+COPY_8_16
+#endif
+ret
+
+
+.p2align 4,, 6
+L(copy_4_7):
+COPY_4_8
+ret
+
+L(copy_1):
+movzbl (%rsi), %ecx
+movb %cl, (%rdi)
+ret
+
+/* Colder copy case for [0, 7].  */
+L(lt_quarter_x):
+#if VEC_SIZE > 32
+cmpl $8, %edx
+jae L(copy_8_15)
+#endif
+#if VEC_SIZE > 16
+cmpl $4, %edx
+jae L(copy_4_7)
+#endif
+cmpl $1, %edx
+je L(copy_1)
+jb L(copy_0)
+/* Fall through into copy [2, 3] as it is more common than [0, 1].  */
+movzwl (%rsi), %ecx
+movzbl -1(%rsi, %rdx), %esi
+movw %cx, (%rdi)
+movb %sil, -1(%rdi, %rdx)
+L(copy_0):
+ret
+
+.p2align 4
+#if VEC_SIZE == 32
+L(copy_8_15):
+COPY_8_16
+ret
+#endif
+
+#if defined USE_MULTIARCH && IS_IN (libc)
 L(movsb):
-	cmp     __x86_rep_movsb_stop_threshold(%rip), %RDX_LP
-	jae	L(more_8x_vec)
-	cmpq	%rsi, %rdi
-	jb	1f
-	/* Source == destination is less common.  */
-	je	L(nop)
-	leaq	(%rsi,%rdx), %r9
-	cmpq	%r9, %rdi
-	/* Avoid slow backward REP MOVSB.  */
-	jb	L(more_8x_vec_backward)
+cmp     __x86_rep_movsb_stop_threshold(%rip), %RDX_LP
+jae L(more_8x_vec)
+cmpq %rsi, %rdi
+jb 1f
+/* Source == destination is less common.  */
+je L(nop)
+leaq (%rsi,%rdx), %r9
+cmpq %r9, %rdi
+/* Avoid slow backward REP MOVSB.  */
+jb L(more_8x_vec_backward)
 # if AVOID_SHORT_DISTANCE_REP_MOVSB
-	andl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
-	jz	3f
-	movq	%rdi, %rcx
-	subq	%rsi, %rcx
-	jmp	2f
+andl $X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
+jz 3f
+movq %rdi, %rcx
+subq %rsi, %rcx
+jmp 2f
 # endif
 1:
 # if AVOID_SHORT_DISTANCE_REP_MOVSB
-	andl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
-	jz	3f
-	movq	%rsi, %rcx
-	subq	%rdi, %rcx
+andl $X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
+jz 3f
+movq %rsi, %rcx
+subq %rdi, %rcx
 2:
 /* Avoid "rep movsb" if RCX, the distance between source and destination,
    is N*4GB + [1..63] with N >= 0.  */
-	cmpl	$63, %ecx
-	jbe	L(more_2x_vec)	/* Avoid "rep movsb" if ECX <= 63.  */
+cmpl $63, %ecx
+jbe L(more_2x_vec) /* Avoid "rep movsb" if ECX <= 63.  */
 3:
 # endif
-	mov	%RDX_LP, %RCX_LP
-	rep movsb
+mov %RDX_LP, %RCX_LP
+rep movsb
 L(nop):
-	ret
+ret
 #endif
 
-L(less_vec):
-	/* Less than 1 VEC.  */
-#if VEC_SIZE != 16 && VEC_SIZE != 32 && VEC_SIZE != 64
-# error Unsupported VEC_SIZE!
-#endif
-#if VEC_SIZE > 32
-	cmpb	$32, %dl
-	jae	L(between_32_63)
-#endif
-#if VEC_SIZE > 16
-	cmpb	$16, %dl
-	jae	L(between_16_31)
-#endif
-	cmpb	$8, %dl
-	jae	L(between_8_15)
-	cmpb	$4, %dl
-	jae	L(between_4_7)
-	cmpb	$1, %dl
-	ja	L(between_2_3)
-	jb	1f
-	movzbl	(%rsi), %ecx
-	movb	%cl, (%rdi)
-1:
-	ret
-#if VEC_SIZE > 32
-L(between_32_63):
-	/* From 32 to 63.  No branch when size == 32.  */
-	VMOVU	(%rsi), %YMM0
-	VMOVU	-32(%rsi,%rdx), %YMM1
-	VMOVU	%YMM0, (%rdi)
-	VMOVU	%YMM1, -32(%rdi,%rdx)
-	VZEROUPPER_RETURN
-#endif
-#if VEC_SIZE > 16
-	/* From 16 to 31.  No branch when size == 16.  */
-L(between_16_31):
-	VMOVU	(%rsi), %XMM0
-	VMOVU	-16(%rsi,%rdx), %XMM1
-	VMOVU	%XMM0, (%rdi)
-	VMOVU	%XMM1, -16(%rdi,%rdx)
-	VZEROUPPER_RETURN
-#endif
-L(between_8_15):
-	/* From 8 to 15.  No branch when size == 8.  */
-	movq	-8(%rsi,%rdx), %rcx
-	movq	(%rsi), %rsi
-	movq	%rcx, -8(%rdi,%rdx)
-	movq	%rsi, (%rdi)
-	ret
-L(between_4_7):
-	/* From 4 to 7.  No branch when size == 4.  */
-	movl	-4(%rsi,%rdx), %ecx
-	movl	(%rsi), %esi
-	movl	%ecx, -4(%rdi,%rdx)
-	movl	%esi, (%rdi)
-	ret
-L(between_2_3):
-	/* From 2 to 3.  No branch when size == 2.  */
-	movzwl	-2(%rsi,%rdx), %ecx
-	movzwl	(%rsi), %esi
-	movw	%cx, -2(%rdi,%rdx)
-	movw	%si, (%rdi)
-	ret
 
 #if defined USE_MULTIARCH && IS_IN (libc)
 L(movsb_more_2x_vec):
-	cmp	__x86_rep_movsb_threshold(%rip), %RDX_LP
-	ja	L(movsb)
+cmp __x86_rep_movsb_threshold(%rip), %RDX_LP
+ja L(movsb)
 #endif
 L(more_2x_vec):
-	/* More than 2 * VEC and there may be overlap between destination
-	   and source.  */
-	cmpq	$(VEC_SIZE * 8), %rdx
-	ja	L(more_8x_vec)
-	cmpq	$(VEC_SIZE * 4), %rdx
-	jbe	L(last_4x_vec)
-	/* Copy from 4 * VEC + 1 to 8 * VEC, inclusively. */
-	VMOVU	(%rsi), %VEC(0)
-	VMOVU	VEC_SIZE(%rsi), %VEC(1)
-	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(2)
-	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(3)
-	VMOVU	-VEC_SIZE(%rsi,%rdx), %VEC(4)
-	VMOVU	-(VEC_SIZE * 2)(%rsi,%rdx), %VEC(5)
-	VMOVU	-(VEC_SIZE * 3)(%rsi,%rdx), %VEC(6)
-	VMOVU	-(VEC_SIZE * 4)(%rsi,%rdx), %VEC(7)
-	VMOVU	%VEC(0), (%rdi)
-	VMOVU	%VEC(1), VEC_SIZE(%rdi)
-	VMOVU	%VEC(2), (VEC_SIZE * 2)(%rdi)
-	VMOVU	%VEC(3), (VEC_SIZE * 3)(%rdi)
-	VMOVU	%VEC(4), -VEC_SIZE(%rdi,%rdx)
-	VMOVU	%VEC(5), -(VEC_SIZE * 2)(%rdi,%rdx)
-	VMOVU	%VEC(6), -(VEC_SIZE * 3)(%rdi,%rdx)
-	VMOVU	%VEC(7), -(VEC_SIZE * 4)(%rdi,%rdx)
-	VZEROUPPER_RETURN
+/* More than 2 * VEC and there may be overlap between destination
+  and source.  */
+cmpq $(VEC_SIZE * 8), %rdx
+ja L(more_8x_vec)
+cmpq $(VEC_SIZE * 4), %rdx
+jbe L(last_4x_vec)
+/* Copy from 4 * VEC + 1 to 8 * VEC, inclusively. */
+VMOVU (%rsi), %VEC(0)
+VMOVU VEC_SIZE(%rsi), %VEC(1)
+VMOVU (VEC_SIZE * 2)(%rsi), %VEC(2)
+VMOVU (VEC_SIZE * 3)(%rsi), %VEC(3)
+VMOVU -VEC_SIZE(%rsi,%rdx), %VEC(4)
+VMOVU -(VEC_SIZE * 2)(%rsi,%rdx), %VEC(5)
+VMOVU -(VEC_SIZE * 3)(%rsi,%rdx), %VEC(6)
+VMOVU -(VEC_SIZE * 4)(%rsi,%rdx), %VEC(7)
+VMOVU %VEC(0), (%rdi)
+VMOVU %VEC(1), VEC_SIZE(%rdi)
+VMOVU %VEC(2), (VEC_SIZE * 2)(%rdi)
+VMOVU %VEC(3), (VEC_SIZE * 3)(%rdi)
+VMOVU %VEC(4), -VEC_SIZE(%rdi,%rdx)
+VMOVU %VEC(5), -(VEC_SIZE * 2)(%rdi,%rdx)
+VMOVU %VEC(6), -(VEC_SIZE * 3)(%rdi,%rdx)
+VMOVU %VEC(7), -(VEC_SIZE * 4)(%rdi,%rdx)
+VZEROUPPER_RETURN
 L(last_4x_vec):
-	/* Copy from 2 * VEC + 1 to 4 * VEC, inclusively. */
-	VMOVU	(%rsi), %VEC(0)
-	VMOVU	VEC_SIZE(%rsi), %VEC(1)
-	VMOVU	-VEC_SIZE(%rsi,%rdx), %VEC(2)
-	VMOVU	-(VEC_SIZE * 2)(%rsi,%rdx), %VEC(3)
-	VMOVU	%VEC(0), (%rdi)
-	VMOVU	%VEC(1), VEC_SIZE(%rdi)
-	VMOVU	%VEC(2), -VEC_SIZE(%rdi,%rdx)
-	VMOVU	%VEC(3), -(VEC_SIZE * 2)(%rdi,%rdx)
-	VZEROUPPER_RETURN
+/* Copy from 2 * VEC + 1 to 4 * VEC, inclusively. */
+VMOVU (%rsi), %VEC(0)
+VMOVU VEC_SIZE(%rsi), %VEC(1)
+VMOVU -VEC_SIZE(%rsi,%rdx), %VEC(2)
+VMOVU -(VEC_SIZE * 2)(%rsi,%rdx), %VEC(3)
+VMOVU %VEC(0), (%rdi)
+VMOVU %VEC(1), VEC_SIZE(%rdi)
+VMOVU %VEC(2), -VEC_SIZE(%rdi,%rdx)
+VMOVU %VEC(3), -(VEC_SIZE * 2)(%rdi,%rdx)
+VZEROUPPER_RETURN
 
 L(more_8x_vec):
-	/* Check if non-temporal move candidate.  */
+/* Check if non-temporal move candidate.  */
 #if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
-	/* Check non-temporal store threshold.  */
-	cmp __x86_shared_non_temporal_threshold(%rip), %RDX_LP
-	ja	L(large_memcpy_2x)
+/* Check non-temporal store threshold.  */
+cmp __x86_shared_non_temporal_threshold(%rip), %RDX_LP
+ja L(large_memcpy_2x)
 #endif
-	/* Entry if rdx is greater than non-temporal threshold but there
+/* Entry if rdx is greater than non-temporal threshold but there
        is overlap.  */
 L(more_8x_vec_check):
-	cmpq	%rsi, %rdi
-	ja	L(more_8x_vec_backward)
-	/* Source == destination is less common.  */
-	je	L(nop)
-	/* Load the first VEC and last 4 * VEC to support overlapping
-	   addresses.  */
-	VMOVU	(%rsi), %VEC(4)
-	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(5)
-	VMOVU	-(VEC_SIZE * 2)(%rsi, %rdx), %VEC(6)
-	VMOVU	-(VEC_SIZE * 3)(%rsi, %rdx), %VEC(7)
-	VMOVU	-(VEC_SIZE * 4)(%rsi, %rdx), %VEC(8)
-	/* Save start and stop of the destination buffer.  */
-	movq	%rdi, %r11
-	leaq	-VEC_SIZE(%rdi, %rdx), %rcx
-	/* Align destination for aligned stores in the loop.  Compute
-	   how much destination is misaligned.  */
-	movq	%rdi, %r8
-	andq	$(VEC_SIZE - 1), %r8
-	/* Get the negative of offset for alignment.  */
-	subq	$VEC_SIZE, %r8
-	/* Adjust source.  */
-	subq	%r8, %rsi
-	/* Adjust destination which should be aligned now.  */
-	subq	%r8, %rdi
-	/* Adjust length.  */
-	addq	%r8, %rdx
-
-	.p2align 4
+cmpq %rsi, %rdi
+ja L(more_8x_vec_backward)
+/* Source == destination is less common.  */
+je L(nop)
+/* Load the first VEC and last 4 * VEC to support overlapping
+  addresses.  */
+VMOVU (%rsi), %VEC(4)
+VMOVU -VEC_SIZE(%rsi, %rdx), %VEC(5)
+VMOVU -(VEC_SIZE * 2)(%rsi, %rdx), %VEC(6)
+VMOVU -(VEC_SIZE * 3)(%rsi, %rdx), %VEC(7)
+VMOVU -(VEC_SIZE * 4)(%rsi, %rdx), %VEC(8)
+/* Save start and stop of the destination buffer.  */
+movq %rdi, %r11
+leaq -VEC_SIZE(%rdi, %rdx), %rcx
+/* Align destination for aligned stores in the loop.  Compute
+  how much destination is misaligned.  */
+movq %rdi, %r8
+andq $(VEC_SIZE - 1), %r8
+/* Get the negative of offset for alignment.  */
+subq $VEC_SIZE, %r8
+/* Adjust source.  */
+subq %r8, %rsi
+/* Adjust destination which should be aligned now.  */
+subq %r8, %rdi
+/* Adjust length.  */
+addq %r8, %rdx
+
+.p2align 4
 L(loop_4x_vec_forward):
-	/* Copy 4 * VEC a time forward.  */
-	VMOVU	(%rsi), %VEC(0)
-	VMOVU	VEC_SIZE(%rsi), %VEC(1)
-	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(2)
-	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(3)
-	subq	$-(VEC_SIZE * 4), %rsi
-	addq	$-(VEC_SIZE * 4), %rdx
-	VMOVA	%VEC(0), (%rdi)
-	VMOVA	%VEC(1), VEC_SIZE(%rdi)
-	VMOVA	%VEC(2), (VEC_SIZE * 2)(%rdi)
-	VMOVA	%VEC(3), (VEC_SIZE * 3)(%rdi)
-	subq	$-(VEC_SIZE * 4), %rdi
-	cmpq	$(VEC_SIZE * 4), %rdx
-	ja	L(loop_4x_vec_forward)
-	/* Store the last 4 * VEC.  */
-	VMOVU	%VEC(5), (%rcx)
-	VMOVU	%VEC(6), -VEC_SIZE(%rcx)
-	VMOVU	%VEC(7), -(VEC_SIZE * 2)(%rcx)
-	VMOVU	%VEC(8), -(VEC_SIZE * 3)(%rcx)
-	/* Store the first VEC.  */
-	VMOVU	%VEC(4), (%r11)
-	VZEROUPPER_RETURN
+/* Copy 4 * VEC a time forward.  */
+VMOVU (%rsi), %VEC(0)
+VMOVU VEC_SIZE(%rsi), %VEC(1)
+VMOVU (VEC_SIZE * 2)(%rsi), %VEC(2)
+VMOVU (VEC_SIZE * 3)(%rsi), %VEC(3)
+subq $-(VEC_SIZE * 4), %rsi
+addq $-(VEC_SIZE * 4), %rdx
+VMOVA %VEC(0), (%rdi)
+VMOVA %VEC(1), VEC_SIZE(%rdi)
+VMOVA %VEC(2), (VEC_SIZE * 2)(%rdi)
+VMOVA %VEC(3), (VEC_SIZE * 3)(%rdi)
+subq $-(VEC_SIZE * 4), %rdi
+cmpq $(VEC_SIZE * 4), %rdx
+ja L(loop_4x_vec_forward)
+/* Store the last 4 * VEC.  */
+VMOVU %VEC(5), (%rcx)
+VMOVU %VEC(6), -VEC_SIZE(%rcx)
+VMOVU %VEC(7), -(VEC_SIZE * 2)(%rcx)
+VMOVU %VEC(8), -(VEC_SIZE * 3)(%rcx)
+/* Store the first VEC.  */
+VMOVU %VEC(4), (%r11)
+VZEROUPPER_RETURN
 
 L(more_8x_vec_backward):
-	/* Load the first 4 * VEC and last VEC to support overlapping
-	   addresses.  */
-	VMOVU	(%rsi), %VEC(4)
-	VMOVU	VEC_SIZE(%rsi), %VEC(5)
-	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(6)
-	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(7)
-	VMOVU	-VEC_SIZE(%rsi,%rdx), %VEC(8)
-	/* Save stop of the destination buffer.  */
-	leaq	-VEC_SIZE(%rdi, %rdx), %r11
-	/* Align destination end for aligned stores in the loop.  Compute
-	   how much destination end is misaligned.  */
-	leaq	-VEC_SIZE(%rsi, %rdx), %rcx
-	movq	%r11, %r9
-	movq	%r11, %r8
-	andq	$(VEC_SIZE - 1), %r8
-	/* Adjust source.  */
-	subq	%r8, %rcx
-	/* Adjust the end of destination which should be aligned now.  */
-	subq	%r8, %r9
-	/* Adjust length.  */
-	subq	%r8, %rdx
-
-	.p2align 4
+/* Load the first 4 * VEC and last VEC to support overlapping
+  addresses.  */
+VMOVU (%rsi), %VEC(4)
+VMOVU VEC_SIZE(%rsi), %VEC(5)
+VMOVU (VEC_SIZE * 2)(%rsi), %VEC(6)
+VMOVU (VEC_SIZE * 3)(%rsi), %VEC(7)
+VMOVU -VEC_SIZE(%rsi,%rdx), %VEC(8)
+/* Save stop of the destination buffer.  */
+leaq -VEC_SIZE(%rdi, %rdx), %r11
+/* Align destination end for aligned stores in the loop.  Compute
+  how much destination end is misaligned.  */
+leaq -VEC_SIZE(%rsi, %rdx), %rcx
+movq %r11, %r9
+movq %r11, %r8
+andq $(VEC_SIZE - 1), %r8
+/* Adjust source.  */
+subq %r8, %rcx
+/* Adjust the end of destination which should be aligned now.  */
+subq %r8, %r9
+/* Adjust length.  */
+subq %r8, %rdx
+
+.p2align 4
 L(loop_4x_vec_backward):
-	/* Copy 4 * VEC a time backward.  */
-	VMOVU	(%rcx), %VEC(0)
-	VMOVU	-VEC_SIZE(%rcx), %VEC(1)
-	VMOVU	-(VEC_SIZE * 2)(%rcx), %VEC(2)
-	VMOVU	-(VEC_SIZE * 3)(%rcx), %VEC(3)
-	addq	$-(VEC_SIZE * 4), %rcx
-	addq	$-(VEC_SIZE * 4), %rdx
-	VMOVA	%VEC(0), (%r9)
-	VMOVA	%VEC(1), -VEC_SIZE(%r9)
-	VMOVA	%VEC(2), -(VEC_SIZE * 2)(%r9)
-	VMOVA	%VEC(3), -(VEC_SIZE * 3)(%r9)
-	addq	$-(VEC_SIZE * 4), %r9
-	cmpq	$(VEC_SIZE * 4), %rdx
-	ja	L(loop_4x_vec_backward)
-	/* Store the first 4 * VEC.  */
-	VMOVU	%VEC(4), (%rdi)
-	VMOVU	%VEC(5), VEC_SIZE(%rdi)
-	VMOVU	%VEC(6), (VEC_SIZE * 2)(%rdi)
-	VMOVU	%VEC(7), (VEC_SIZE * 3)(%rdi)
-	/* Store the last VEC.  */
-	VMOVU	%VEC(8), (%r11)
-	VZEROUPPER_RETURN
+/* Copy 4 * VEC a time backward.  */
+VMOVU (%rcx), %VEC(0)
+VMOVU -VEC_SIZE(%rcx), %VEC(1)
+VMOVU -(VEC_SIZE * 2)(%rcx), %VEC(2)
+VMOVU -(VEC_SIZE * 3)(%rcx), %VEC(3)
+addq $-(VEC_SIZE * 4), %rcx
+addq $-(VEC_SIZE * 4), %rdx
+VMOVA %VEC(0), (%r9)
+VMOVA %VEC(1), -VEC_SIZE(%r9)
+VMOVA %VEC(2), -(VEC_SIZE * 2)(%r9)
+VMOVA %VEC(3), -(VEC_SIZE * 3)(%r9)
+addq $-(VEC_SIZE * 4), %r9
+cmpq $(VEC_SIZE * 4), %rdx
+ja L(loop_4x_vec_backward)
+/* Store the first 4 * VEC.  */
+VMOVU %VEC(4), (%rdi)
+VMOVU %VEC(5), VEC_SIZE(%rdi)
+VMOVU %VEC(6), (VEC_SIZE * 2)(%rdi)
+VMOVU %VEC(7), (VEC_SIZE * 3)(%rdi)
+/* Store the last VEC.  */
+VMOVU %VEC(8), (%r11)
+VZEROUPPER_RETURN
 
 #if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
-	.p2align 4
+.p2align 4
 L(large_memcpy_2x):
-	/* Compute absolute value of difference between source and
-	   destination.  */
-	movq	%rdi, %r9
-	subq	%rsi, %r9
-	movq	%r9, %r8
-	leaq	-1(%r9), %rcx
-	sarq	$63, %r8
-	xorq	%r8, %r9
-	subq	%r8, %r9
-	/* Don't use non-temporal store if there is overlap between
-	   destination and source since destination may be in cache when
-	   source is loaded.  */
-	cmpq	%r9, %rdx
-	ja	L(more_8x_vec_check)
-
-	/* Cache align destination. First store the first 64 bytes then
-	   adjust alignments.  */
-	VMOVU	(%rsi), %VEC(8)
+/* Compute absolute value of difference between source and
+  destination.  */
+movq %rdi, %r9
+subq %rsi, %r9
+movq %r9, %r8
+leaq -1(%r9), %rcx
+sarq $63, %r8
+xorq %r8, %r9
+subq %r8, %r9
+/* Don't use non-temporal store if there is overlap between
+  destination and source since destination may be in cache when
+  source is loaded.  */
+cmpq %r9, %rdx
+ja L(more_8x_vec_check)
+
+/* Cache align destination. First store the first 64 bytes then
+  adjust alignments.  */
+VMOVU (%rsi), %VEC(8)
 #if VEC_SIZE < 64
-	VMOVU	VEC_SIZE(%rsi), %VEC(9)
+VMOVU VEC_SIZE(%rsi), %VEC(9)
 #if VEC_SIZE < 32
-	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(10)
-	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(11)
+VMOVU (VEC_SIZE * 2)(%rsi), %VEC(10)
+VMOVU (VEC_SIZE * 3)(%rsi), %VEC(11)
 #endif
 #endif
-	VMOVU	%VEC(8), (%rdi)
+VMOVU %VEC(8), (%rdi)
 #if VEC_SIZE < 64
-	VMOVU	%VEC(9), VEC_SIZE(%rdi)
+VMOVU %VEC(9), VEC_SIZE(%rdi)
 #if VEC_SIZE < 32
-	VMOVU	%VEC(10), (VEC_SIZE * 2)(%rdi)
-	VMOVU	%VEC(11), (VEC_SIZE * 3)(%rdi)
-#endif
-#endif
-	/* Adjust source, destination, and size.  */
-	movq	%rdi, %r8
-	andq	$63, %r8
-	/* Get the negative of offset for alignment.  */
-	subq	$64, %r8
-	/* Adjust source.  */
-	subq	%r8, %rsi
-	/* Adjust destination which should be aligned now.  */
-	subq	%r8, %rdi
-	/* Adjust length.  */
-	addq	%r8, %rdx
-
-	/* Test if source and destination addresses will alias. If they do
-	   the larger pipeline in large_memcpy_4x alleviated the
-	   performance drop.  */
-	testl	$(PAGE_SIZE - VEC_SIZE * 8), %ecx
-	jz	L(large_memcpy_4x)
-
-	movq	%rdx, %r10
-	shrq	$LOG_4X_MEMCPY_THRESH, %r10
-	cmp	__x86_shared_non_temporal_threshold(%rip), %r10
-	jae	L(large_memcpy_4x)
-
-	/* edx will store remainder size for copying tail.  */
-	andl	$(PAGE_SIZE * 2 - 1), %edx
-	/* r10 stores outer loop counter.  */
-	shrq	$((LOG_PAGE_SIZE + 1) - LOG_4X_MEMCPY_THRESH), %r10
-	/* Copy 4x VEC at a time from 2 pages.  */
-	.p2align 4
+VMOVU %VEC(10), (VEC_SIZE * 2)(%rdi)
+VMOVU %VEC(11), (VEC_SIZE * 3)(%rdi)
+#endif
+#endif
+/* Adjust source, destination, and size.  */
+movq %rdi, %r8
+andq $63, %r8
+/* Get the negative of offset for alignment.  */
+subq $64, %r8
+/* Adjust source.  */
+subq %r8, %rsi
+/* Adjust destination which should be aligned now.  */
+subq %r8, %rdi
+/* Adjust length.  */
+addq %r8, %rdx
+
+/* Test if source and destination addresses will alias. If they do
+  the larger pipeline in large_memcpy_4x alleviated the
+  performance drop.  */
+testl $(PAGE_SIZE - VEC_SIZE * 8), %ecx
+jz L(large_memcpy_4x)
+
+movq %rdx, %r10
+shrq $LOG_4X_MEMCPY_THRESH, %r10
+cmp __x86_shared_non_temporal_threshold(%rip), %r10
+jae L(large_memcpy_4x)
+
+/* edx will store remainder size for copying tail.  */
+andl $(PAGE_SIZE * 2 - 1), %edx
+/* r10 stores outer loop counter.  */
+shrq $((LOG_PAGE_SIZE + 1) - LOG_4X_MEMCPY_THRESH), %r10
+/* Copy 4x VEC at a time from 2 pages.  */
+.p2align 4
 L(loop_large_memcpy_2x_outer):
-	/* ecx stores inner loop counter.  */
-	movl	$(PAGE_SIZE / LARGE_LOAD_SIZE), %ecx
+/* ecx stores inner loop counter.  */
+movl $(PAGE_SIZE / LARGE_LOAD_SIZE), %ecx
 L(loop_large_memcpy_2x_inner):
-	PREFETCH_ONE_SET(1, (%rsi), PREFETCHED_LOAD_SIZE)
-	PREFETCH_ONE_SET(1, (%rsi), PREFETCHED_LOAD_SIZE * 2)
-	PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE + PREFETCHED_LOAD_SIZE)
-	PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE + PREFETCHED_LOAD_SIZE * 2)
-	/* Load vectors from rsi.  */
-	LOAD_ONE_SET((%rsi), 0, %VEC(0), %VEC(1), %VEC(2), %VEC(3))
-	LOAD_ONE_SET((%rsi), PAGE_SIZE, %VEC(4), %VEC(5), %VEC(6), %VEC(7))
-	subq	$-LARGE_LOAD_SIZE, %rsi
-	/* Non-temporal store vectors to rdi.  */
-	STORE_ONE_SET((%rdi), 0, %VEC(0), %VEC(1), %VEC(2), %VEC(3))
-	STORE_ONE_SET((%rdi), PAGE_SIZE, %VEC(4), %VEC(5), %VEC(6), %VEC(7))
-	subq	$-LARGE_LOAD_SIZE, %rdi
-	decl	%ecx
-	jnz	L(loop_large_memcpy_2x_inner)
-	addq	$PAGE_SIZE, %rdi
-	addq	$PAGE_SIZE, %rsi
-	decq	%r10
-	jne	L(loop_large_memcpy_2x_outer)
-	sfence
-
-	/* Check if only last 4 loads are needed.  */
-	cmpl	$(VEC_SIZE * 4), %edx
-	jbe	L(large_memcpy_2x_end)
-
-	/* Handle the last 2 * PAGE_SIZE bytes.  */
+PREFETCH_ONE_SET(1, (%rsi), PREFETCHED_LOAD_SIZE)
+PREFETCH_ONE_SET(1, (%rsi), PREFETCHED_LOAD_SIZE * 2)
+PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE + PREFETCHED_LOAD_SIZE)
+PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE + PREFETCHED_LOAD_SIZE * 2)
+/* Load vectors from rsi.  */
+LOAD_ONE_SET((%rsi), 0, %VEC(0), %VEC(1), %VEC(2), %VEC(3))
+LOAD_ONE_SET((%rsi), PAGE_SIZE, %VEC(4), %VEC(5), %VEC(6), %VEC(7))
+subq $-LARGE_LOAD_SIZE, %rsi
+/* Non-temporal store vectors to rdi.  */
+STORE_ONE_SET((%rdi), 0, %VEC(0), %VEC(1), %VEC(2), %VEC(3))
+STORE_ONE_SET((%rdi), PAGE_SIZE, %VEC(4), %VEC(5), %VEC(6), %VEC(7))
+subq $-LARGE_LOAD_SIZE, %rdi
+decl %ecx
+jnz L(loop_large_memcpy_2x_inner)
+addq $PAGE_SIZE, %rdi
+addq $PAGE_SIZE, %rsi
+decq %r10
+jne L(loop_large_memcpy_2x_outer)
+sfence
+
+/* Check if only last 4 loads are needed.  */
+cmpl $(VEC_SIZE * 4), %edx
+jbe L(large_memcpy_2x_end)
+
+/* Handle the last 2 * PAGE_SIZE bytes.  */
 L(loop_large_memcpy_2x_tail):
-	/* Copy 4 * VEC a time forward with non-temporal stores.  */
-	PREFETCH_ONE_SET (1, (%rsi), PREFETCHED_LOAD_SIZE)
-	PREFETCH_ONE_SET (1, (%rdi), PREFETCHED_LOAD_SIZE)
-	VMOVU	(%rsi), %VEC(0)
-	VMOVU	VEC_SIZE(%rsi), %VEC(1)
-	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(2)
-	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(3)
-	subq	$-(VEC_SIZE * 4), %rsi
-	addl	$-(VEC_SIZE * 4), %edx
-	VMOVA	%VEC(0), (%rdi)
-	VMOVA	%VEC(1), VEC_SIZE(%rdi)
-	VMOVA	%VEC(2), (VEC_SIZE * 2)(%rdi)
-	VMOVA	%VEC(3), (VEC_SIZE * 3)(%rdi)
-	subq	$-(VEC_SIZE * 4), %rdi
-	cmpl	$(VEC_SIZE * 4), %edx
-	ja	L(loop_large_memcpy_2x_tail)
+/* Copy 4 * VEC a time forward with non-temporal stores.  */
+PREFETCH_ONE_SET (1, (%rsi), PREFETCHED_LOAD_SIZE)
+PREFETCH_ONE_SET (1, (%rdi), PREFETCHED_LOAD_SIZE)
+VMOVU (%rsi), %VEC(0)
+VMOVU VEC_SIZE(%rsi), %VEC(1)
+VMOVU (VEC_SIZE * 2)(%rsi), %VEC(2)
+VMOVU (VEC_SIZE * 3)(%rsi), %VEC(3)
+subq $-(VEC_SIZE * 4), %rsi
+addl $-(VEC_SIZE * 4), %edx
+VMOVA %VEC(0), (%rdi)
+VMOVA %VEC(1), VEC_SIZE(%rdi)
+VMOVA %VEC(2), (VEC_SIZE * 2)(%rdi)
+VMOVA %VEC(3), (VEC_SIZE * 3)(%rdi)
+subq $-(VEC_SIZE * 4), %rdi
+cmpl $(VEC_SIZE * 4), %edx
+ja L(loop_large_memcpy_2x_tail)
 
 L(large_memcpy_2x_end):
-	/* Store the last 4 * VEC.  */
-	VMOVU	-(VEC_SIZE * 4)(%rsi, %rdx), %VEC(0)
-	VMOVU	-(VEC_SIZE * 3)(%rsi, %rdx), %VEC(1)
-	VMOVU	-(VEC_SIZE * 2)(%rsi, %rdx), %VEC(2)
-	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(3)
-
-	VMOVU	%VEC(0), -(VEC_SIZE * 4)(%rdi, %rdx)
-	VMOVU	%VEC(1), -(VEC_SIZE * 3)(%rdi, %rdx)
-	VMOVU	%VEC(2), -(VEC_SIZE * 2)(%rdi, %rdx)
-	VMOVU	%VEC(3), -VEC_SIZE(%rdi, %rdx)
-	VZEROUPPER_RETURN
-
-	.p2align 4
+/* Store the last 4 * VEC.  */
+VMOVU -(VEC_SIZE * 4)(%rsi, %rdx), %VEC(0)
+VMOVU -(VEC_SIZE * 3)(%rsi, %rdx), %VEC(1)
+VMOVU -(VEC_SIZE * 2)(%rsi, %rdx), %VEC(2)
+VMOVU -VEC_SIZE(%rsi, %rdx), %VEC(3)
+
+VMOVU %VEC(0), -(VEC_SIZE * 4)(%rdi, %rdx)
+VMOVU %VEC(1), -(VEC_SIZE * 3)(%rdi, %rdx)
+VMOVU %VEC(2), -(VEC_SIZE * 2)(%rdi, %rdx)
+VMOVU %VEC(3), -VEC_SIZE(%rdi, %rdx)
+VZEROUPPER_RETURN
+
+.p2align 4
 L(large_memcpy_4x):
-	movq	%rdx, %r10
-	/* edx will store remainder size for copying tail.  */
-	andl	$(PAGE_SIZE * 4 - 1), %edx
-	/* r10 stores outer loop counter.  */
-	shrq	$(LOG_PAGE_SIZE + 2), %r10
-	/* Copy 4x VEC at a time from 4 pages.  */
-	.p2align 4
+movq %rdx, %r10
+/* edx will store remainder size for copying tail.  */
+andl $(PAGE_SIZE * 4 - 1), %edx
+/* r10 stores outer loop counter.  */
+shrq $(LOG_PAGE_SIZE + 2), %r10
+/* Copy 4x VEC at a time from 4 pages.  */
+.p2align 4
 L(loop_large_memcpy_4x_outer):
-	/* ecx stores inner loop counter.  */
-	movl	$(PAGE_SIZE / LARGE_LOAD_SIZE), %ecx
+/* ecx stores inner loop counter.  */
+movl $(PAGE_SIZE / LARGE_LOAD_SIZE), %ecx
 L(loop_large_memcpy_4x_inner):
-	/* Only one prefetch set per page as doing 4 pages give more time
-	   for prefetcher to keep up.  */
-	PREFETCH_ONE_SET(1, (%rsi), PREFETCHED_LOAD_SIZE)
-	PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE + PREFETCHED_LOAD_SIZE)
-	PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE * 2 + PREFETCHED_LOAD_SIZE)
-	PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE * 3 + PREFETCHED_LOAD_SIZE)
-	/* Load vectors from rsi.  */
-	LOAD_ONE_SET((%rsi), 0, %VEC(0), %VEC(1), %VEC(2), %VEC(3))
-	LOAD_ONE_SET((%rsi), PAGE_SIZE, %VEC(4), %VEC(5), %VEC(6), %VEC(7))
-	LOAD_ONE_SET((%rsi), PAGE_SIZE * 2, %VEC(8), %VEC(9), %VEC(10), %VEC(11))
-	LOAD_ONE_SET((%rsi), PAGE_SIZE * 3, %VEC(12), %VEC(13), %VEC(14), %VEC(15))
-	subq	$-LARGE_LOAD_SIZE, %rsi
-	/* Non-temporal store vectors to rdi.  */
-	STORE_ONE_SET((%rdi), 0, %VEC(0), %VEC(1), %VEC(2), %VEC(3))
-	STORE_ONE_SET((%rdi), PAGE_SIZE, %VEC(4), %VEC(5), %VEC(6), %VEC(7))
-	STORE_ONE_SET((%rdi), PAGE_SIZE * 2, %VEC(8), %VEC(9), %VEC(10), %VEC(11))
-	STORE_ONE_SET((%rdi), PAGE_SIZE * 3, %VEC(12), %VEC(13), %VEC(14), %VEC(15))
-	subq	$-LARGE_LOAD_SIZE, %rdi
-	decl	%ecx
-	jnz	L(loop_large_memcpy_4x_inner)
-	addq	$(PAGE_SIZE * 3), %rdi
-	addq	$(PAGE_SIZE * 3), %rsi
-	decq	%r10
-	jne	L(loop_large_memcpy_4x_outer)
-	sfence
-	/* Check if only last 4 loads are needed.  */
-	cmpl	$(VEC_SIZE * 4), %edx
-	jbe	L(large_memcpy_4x_end)
-
-	/* Handle the last 4  * PAGE_SIZE bytes.  */
+/* Only one prefetch set per page as doing 4 pages give more time
+  for prefetcher to keep up.  */
+PREFETCH_ONE_SET(1, (%rsi), PREFETCHED_LOAD_SIZE)
+PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE + PREFETCHED_LOAD_SIZE)
+PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE * 2 + PREFETCHED_LOAD_SIZE)
+PREFETCH_ONE_SET(1, (%rsi), PAGE_SIZE * 3 + PREFETCHED_LOAD_SIZE)
+/* Load vectors from rsi.  */
+LOAD_ONE_SET((%rsi), 0, %VEC(0), %VEC(1), %VEC(2), %VEC(3))
+LOAD_ONE_SET((%rsi), PAGE_SIZE, %VEC(4), %VEC(5), %VEC(6), %VEC(7))
+LOAD_ONE_SET((%rsi), PAGE_SIZE * 2, %VEC(8), %VEC(9), %VEC(10), %VEC(11))
+LOAD_ONE_SET((%rsi), PAGE_SIZE * 3, %VEC(12), %VEC(13), %VEC(14), %VEC(15))
+subq $-LARGE_LOAD_SIZE, %rsi
+/* Non-temporal store vectors to rdi.  */
+STORE_ONE_SET((%rdi), 0, %VEC(0), %VEC(1), %VEC(2), %VEC(3))
+STORE_ONE_SET((%rdi), PAGE_SIZE, %VEC(4), %VEC(5), %VEC(6), %VEC(7))
+STORE_ONE_SET((%rdi), PAGE_SIZE * 2, %VEC(8), %VEC(9), %VEC(10), %VEC(11))
+STORE_ONE_SET((%rdi), PAGE_SIZE * 3, %VEC(12), %VEC(13), %VEC(14), %VEC(15))
+subq $-LARGE_LOAD_SIZE, %rdi
+decl %ecx
+jnz L(loop_large_memcpy_4x_inner)
+addq $(PAGE_SIZE * 3), %rdi
+addq $(PAGE_SIZE * 3), %rsi
+decq %r10
+jne L(loop_large_memcpy_4x_outer)
+sfence
+/* Check if only last 4 loads are needed.  */
+cmpl $(VEC_SIZE * 4), %edx
+jbe L(large_memcpy_4x_end)
+
+/* Handle the last 4  * PAGE_SIZE bytes.  */
 L(loop_large_memcpy_4x_tail):
-	/* Copy 4 * VEC a time forward with non-temporal stores.  */
-	PREFETCH_ONE_SET (1, (%rsi), PREFETCHED_LOAD_SIZE)
-	PREFETCH_ONE_SET (1, (%rdi), PREFETCHED_LOAD_SIZE)
-	VMOVU	(%rsi), %VEC(0)
-	VMOVU	VEC_SIZE(%rsi), %VEC(1)
-	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(2)
-	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(3)
-	subq	$-(VEC_SIZE * 4), %rsi
-	addl	$-(VEC_SIZE * 4), %edx
-	VMOVA	%VEC(0), (%rdi)
-	VMOVA	%VEC(1), VEC_SIZE(%rdi)
-	VMOVA	%VEC(2), (VEC_SIZE * 2)(%rdi)
-	VMOVA	%VEC(3), (VEC_SIZE * 3)(%rdi)
-	subq	$-(VEC_SIZE * 4), %rdi
-	cmpl	$(VEC_SIZE * 4), %edx
-	ja	L(loop_large_memcpy_4x_tail)
+/* Copy 4 * VEC a time forward with non-temporal stores.  */
+PREFETCH_ONE_SET (1, (%rsi), PREFETCHED_LOAD_SIZE)
+PREFETCH_ONE_SET (1, (%rdi), PREFETCHED_LOAD_SIZE)
+VMOVU (%rsi), %VEC(0)
+VMOVU VEC_SIZE(%rsi), %VEC(1)
+VMOVU (VEC_SIZE * 2)(%rsi), %VEC(2)
+VMOVU (VEC_SIZE * 3)(%rsi), %VEC(3)
+subq $-(VEC_SIZE * 4), %rsi
+addl $-(VEC_SIZE * 4), %edx
+VMOVA %VEC(0), (%rdi)
+VMOVA %VEC(1), VEC_SIZE(%rdi)
+VMOVA %VEC(2), (VEC_SIZE * 2)(%rdi)
+VMOVA %VEC(3), (VEC_SIZE * 3)(%rdi)
+subq $-(VEC_SIZE * 4), %rdi
+cmpl $(VEC_SIZE * 4), %edx
+ja L(loop_large_memcpy_4x_tail)
 
 L(large_memcpy_4x_end):
-	/* Store the last 4 * VEC.  */
-	VMOVU	-(VEC_SIZE * 4)(%rsi, %rdx), %VEC(0)
-	VMOVU	-(VEC_SIZE * 3)(%rsi, %rdx), %VEC(1)
-	VMOVU	-(VEC_SIZE * 2)(%rsi, %rdx), %VEC(2)
-	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(3)
-
-	VMOVU	%VEC(0), -(VEC_SIZE * 4)(%rdi, %rdx)
-	VMOVU	%VEC(1), -(VEC_SIZE * 3)(%rdi, %rdx)
-	VMOVU	%VEC(2), -(VEC_SIZE * 2)(%rdi, %rdx)
-	VMOVU	%VEC(3), -VEC_SIZE(%rdi, %rdx)
-	VZEROUPPER_RETURN
+/* Store the last 4 * VEC.  */
+VMOVU -(VEC_SIZE * 4)(%rsi, %rdx), %VEC(0)
+VMOVU -(VEC_SIZE * 3)(%rsi, %rdx), %VEC(1)
+VMOVU -(VEC_SIZE * 2)(%rsi, %rdx), %VEC(2)
+VMOVU -VEC_SIZE(%rsi, %rdx), %VEC(3)
+
+VMOVU %VEC(0), -(VEC_SIZE * 4)(%rdi, %rdx)
+VMOVU %VEC(1), -(VEC_SIZE * 3)(%rdi, %rdx)
+VMOVU %VEC(2), -(VEC_SIZE * 2)(%rdi, %rdx)
+VMOVU %VEC(3), -VEC_SIZE(%rdi, %rdx)
+VZEROUPPER_RETURN
 #endif
 END (MEMMOVE_SYMBOL (__memmove, unaligned_erms))
 
 #if IS_IN (libc)
 # ifdef USE_MULTIARCH
 strong_alias (MEMMOVE_SYMBOL (__memmove, unaligned_erms),
-	      MEMMOVE_SYMBOL (__memcpy, unaligned_erms))
+     MEMMOVE_SYMBOL (__memcpy, unaligned_erms))
 #  ifdef SHARED
 strong_alias (MEMMOVE_SYMBOL (__memmove_chk, unaligned_erms),
-	      MEMMOVE_SYMBOL (__memcpy_chk, unaligned_erms))
+     MEMMOVE_SYMBOL (__memcpy_chk, unaligned_erms))
 #  endif
 # endif
 # ifdef SHARED
 strong_alias (MEMMOVE_CHK_SYMBOL (__memmove_chk, unaligned),
-	      MEMMOVE_CHK_SYMBOL (__memcpy_chk, unaligned))
+     MEMMOVE_CHK_SYMBOL (__memcpy_chk, unaligned))
 # endif
 #endif
 strong_alias (MEMMOVE_SYMBOL (__memmove, unaligned),
-	      MEMCPY_SYMBOL (__memcpy, unaligned))
+     MEMCPY_SYMBOL (__memcpy, unaligned))
-- 
2.25.1

