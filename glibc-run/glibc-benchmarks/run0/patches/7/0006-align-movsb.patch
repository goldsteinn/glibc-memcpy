From 20217db29fa7f4480f11a2004795b5c7b06e4216 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Sun, 22 Aug 2021 08:07:12 -0400
Subject: [PATCH 6/6] align movsb

---
 .../multiarch/memmove-vec-unaligned-erms.S    | 65 ++++++++++++++++++-
 1 file changed, 62 insertions(+), 3 deletions(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
index db2e029927..f186e7609a 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
@@ -414,17 +414,75 @@ L(movsb):
 	cmpq	%rdx, %rcx
 	jb	L(more_8x_vec_backward_check_nop)
 	cmp	__x86_rep_movsb_stop_threshold(%rip), %RDX_LP
+
+	VMOVU	(%rsi), %VEC(0)
+# if VEC_SIZE != 64
+	VMOVU	VEC_SIZE(%rsi), %VEC(1)
+# endif
+	movq	%rdi, %r8
 # if AVOID_SHORT_DISTANCE_REP_MOVSB
 	andl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
 	jz	L(skip_short_movsb_check)
 	cmpl	$-64, %ecx
 	jae	L(more_8x_vec_forward)
+# endif
+
+	subq	%rdi, %rsi
+	leaq	(%rdi, %rdx), %rcx
+
+# if VEC_SIZE != 64
+	addq	$(VEC_SIZE * 2 - 1), %rdi
+	andq	$-(VEC_SIZE * 2), %rdi
+# else
+	addq	$(VEC_SIZE - 1), %rdi
+	andq	$-(VEC_SIZE), %rdi
+# endif
+	addq	%rdi, %rsi
+	subq	%rdi, %rcx
+
+	rep	movsb
+	VMOVU	%VEC(0), (%r8)
+# if VEC_SIZE != 64
+	VMOVU	%VEC(1), VEC_SIZE(%r8)
+# endif
+	VZEROUPPER_RETURN
+L(movsb_align_dst):
+	subq	%rdi, %rsi
+	leaq	-(1)(%rdi, %rdx), %rcx
+# if VEC_SIZE != 64
+	orq	$(VEC_SIZE * 2 - 1), %rdi
+# else
+	orq	$(VEC_SIZE - 1), %rdi
+# endif
+	leaq	1(%rdi, %rsi), %rsi
+	subq	%rdi, %rcx
+	incq	%rdi
+	rep	movsb
+	VMOVU	%VEC(0), (%r8)
+# if VEC_SIZE != 64
+	VMOVU	%VEC(1), VEC_SIZE(%r8)
+# endif
+	VZEROUPPER_RETURN
+
 L(skip_short_movsb_check):
+	testl	$(PAGE_SIZE - 512), %ecx
+	jnz	L(movsb_align_dst)
+	movq	%rcx, %r9
+	leaq	-(1)(%rsi, %rdx), %rcx
+# if VEC_SIZE != 64
+	orq	$(VEC_SIZE * 2 - 1), %rsi
+# else
+	orq	$(VEC_SIZE - 1), %rsi
 # endif
-	mov	%RDX_LP, %RCX_LP
+	leaq	1(%rsi, %r9), %rdi
+	subq	%rsi, %rcx
+	incq	%rsi
 	rep	movsb
-L(nop):
-	ret
+	VMOVU	%VEC(0), (%r8)
+# if VEC_SIZE != 64
+	VMOVU	%VEC(1), VEC_SIZE(%r8)
+# endif
+	VZEROUPPER_RETURN
 #endif
 
 
@@ -468,6 +526,7 @@ L(last_4x_vec):
 	VMOVU	%VEC(1), VEC_SIZE(%rdi)
 	VMOVU	%VEC(2), -VEC_SIZE(%rdi, %rdx)
 	VMOVU	%VEC(3), -(VEC_SIZE * 2)(%rdi, %rdx)
+L(nop):
 	VZEROUPPER_RETURN
 
 L(more_8x_vec):
-- 
2.25.1

