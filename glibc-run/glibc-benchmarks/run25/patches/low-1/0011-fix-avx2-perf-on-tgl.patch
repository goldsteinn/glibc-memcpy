From 4ebd0c7d5aae618bee7643366d047696de393638 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Mon, 6 Sep 2021 23:36:22 -0500
Subject: [PATCH 11/11] fix avx2 perf on tgl

---
 sysdeps/x86_64/multiarch/memmove-avx-unaligned-erms.S | 2 +-
 sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S | 9 ++++++++-
 2 files changed, 9 insertions(+), 2 deletions(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-avx-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-avx-unaligned-erms.S
index e195e93f15..edb4bd3e5f 100644
--- a/sysdeps/x86_64/multiarch/memmove-avx-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-avx-unaligned-erms.S
@@ -7,6 +7,6 @@
 
 # define SECTION(p)		p##.avx
 # define MEMMOVE_SYMBOL(p,s)	p##_avx_##s
-
+# define USE_WITH_AVX 1
 # include "memmove-vec-unaligned-erms.S"
 #endif
diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
index cccb0e3220..e2f2f74ffe 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
@@ -477,7 +477,13 @@ L(last_4x_vec):
 	VMOVU	%VEC(2), -VEC_SIZE(%rdi, %rdx)
 	VMOVU	%VEC(3), -(VEC_SIZE * 2)(%rdi, %rdx)
 	VZEROUPPER_RETURN
+#ifdef USE_WITH_AVX
+    .byte 0x66, 0x66, 0x0f, 0x1f, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00;
+    .byte 0x66, 0x0f, 0x1f, 0x44, 0x00, 0x00;
 
+    .byte 0x66, 0x66, 0x0f, 0x1f, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00;
+    .byte 0x66, 0x0f, 0x1f, 0x44, 0x00, 0x00;
+#endif
 	.p2align 4,, 10
 L(more_8x_vec):
 	movq	%rdi, %rcx
@@ -620,6 +626,7 @@ L(loop_4x_vec_backward):
 
 #if defined USE_MULTIARCH && IS_IN (libc)
 # if ALIGN_MOVSB
+    .p2align 4,,12
 L(skip_short_movsb_check):
 #  if MOVSB_ALIGN_TO > VEC_SIZE
 	VMOVU	VEC_SIZE(%rsi), %VEC(5)
@@ -653,7 +660,7 @@ L(skip_short_movsb_check):
 #  endif
 	VZEROUPPER_RETURN
 # endif
-	.p2align 4,, 6
+	.p2align 4,, 14
 L(movsb):
 	movq	%rdi, %rcx
 	subq	%rsi, %rcx
-- 
2.25.1

