From a0be38456c1db802d71ceee4717856aa6edec3d5 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Mon, 6 Sep 2021 16:52:36 -0500
Subject: [PATCH 05/11] final v0

---
 .../multiarch/memmove-vec-unaligned-erms.S    | 192 +++++++++++++-----
 1 file changed, 145 insertions(+), 47 deletions(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
index c0d9ccb3aa..1aff4bab8a 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
@@ -165,6 +165,17 @@
 # error Invalid LARGE_LOAD_SIZE
 #endif
 
+/* Whether to align before movsb. Ultimately we want 64 byte align
+   and not worth it to load 4x VEC for VEC_SIZE == 16.  */
+#define ALIGN_MOVSB	(VEC_SIZE > 16)
+
+/* Number of VECs to align movsb to.  */
+#if VEC_SIZE == 64
+# define MOVSB_ALIGN_TO	(VEC_SIZE)
+#else
+# define MOVSB_ALIGN_TO	(VEC_SIZE * 2)
+#endif
+
 /* Macro for copying inclusive power of 2 range with two register
    loads.  */
 #define COPY_BLOCK(mov_inst, src_reg, dst_reg, size_reg, len, tmp_reg0, tmp_reg1)	\
@@ -220,20 +231,19 @@ L(start):
 #else
 	jb	L(less_vec)
 #endif
+	VMOVU	(%rsi), %VEC(4)
 	cmp	$(VEC_SIZE * 2), %RDX_LP
 	ja	L(more_2x_vec)
 #if !defined USE_MULTIARCH || !IS_IN (libc)
 L(last_2x_vec):
 #endif
 	/* From VEC and to 2 * VEC.  No branch when size == VEC_SIZE.  */
-	VMOVU	(%rsi), %VEC(0)
 	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(1)
-	VMOVU	%VEC(0), (%rdi)
+	VMOVU	%VEC(4), (%rdi)
 	VMOVU	%VEC(1), -VEC_SIZE(%rdi, %rdx)
 	VZEROUPPER_RETURN
 #if defined USE_MULTIARCH && IS_IN (libc)
 END (MEMMOVE_SYMBOL (__memmove, unaligned))
-
 # if VEC_SIZE == 16
 ENTRY (__mempcpy_chk_erms)
 	cmp	%RDX_LP, %RCX_LP
@@ -319,14 +329,20 @@ L(start_erms):
 	jbe	L(less_vec)
 # else
 	jb	L(less_vec)
+# endif
+# if AVOID_SHORT_DISTANCE_REP_MOVSB || ALIGN_MOVSB
+	/* Load regardless.  */
+	VMOVU	(%rsi), %VEC(4)
 # endif
 	cmp	$(VEC_SIZE * 2), %RDX_LP
 	ja	L(movsb_more_2x_vec)
 L(last_2x_vec):
 	/* From VEC and to 2 * VEC.  No branch when size == VEC_SIZE.  */
-	VMOVU	(%rsi), %VEC(0)
+# if !AVOID_SHORT_DISTANCE_REP_MOVSB && !ALIGN_MOVSB
+	VMOVU	(%rsi), %VEC(4)
+# endif
 	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(1)
-	VMOVU	%VEC(0), (%rdi)
+	VMOVU	%VEC(4), (%rdi)
 	VMOVU	%VEC(1), -VEC_SIZE(%rdi, %rdx)
 L(return):
 # if VEC_SIZE > 16
@@ -406,7 +422,7 @@ L(less_quarter_vec):
 L(copy_0):
 	ret
 
-	.p2align 4
+	.p2align 4,, 6
 #if VEC_SIZE == 32
 L(copy_8_15):
 	COPY_8_16
@@ -415,44 +431,48 @@ L(copy_8_15):
 	   it wastes 15 bytes of code and 1 byte off is fine.  */
 #endif
 
+	.p2align 4,, 6
 #if defined USE_MULTIARCH && IS_IN (libc)
 L(movsb_more_2x_vec):
 	cmp	__x86_rep_movsb_threshold(%rip), %RDX_LP
 	ja	L(movsb)
+# if !AVOID_SHORT_DISTANCE_REP_MOVSB && !ALIGN_MOVSB
+	VMOVU	(%rsi), %VEC(4)
+# endif
 #endif
 L(more_2x_vec):
 	/* More than 2 * VEC and there may be overlap between destination
 	   and source.  */
 	cmpq	$(VEC_SIZE * 8), %rdx
 	ja	L(more_8x_vec)
-	/* Load regardless.  */
-	VMOVU	(%rsi), %VEC(0)
+	/* Load regardless of 4x branch.  */
 	VMOVU	VEC_SIZE(%rsi), %VEC(1)
 	cmpq	$(VEC_SIZE * 4), %rdx
 	jbe	L(last_4x_vec)
 	/* Copy from 4 * VEC + 1 to 8 * VEC, inclusively.  */
 	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(2)
 	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(3)
-	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(4)
+	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(0)
 	VMOVU	-(VEC_SIZE * 2)(%rsi, %rdx), %VEC(5)
+
 	VMOVU	-(VEC_SIZE * 3)(%rsi, %rdx), %VEC(6)
 	VMOVU	-(VEC_SIZE * 4)(%rsi, %rdx), %VEC(7)
-	VMOVU	%VEC(0), (%rdi)
+	VMOVU	%VEC(4), (%rdi)
 	VMOVU	%VEC(1), VEC_SIZE(%rdi)
 	VMOVU	%VEC(2), (VEC_SIZE * 2)(%rdi)
 	VMOVU	%VEC(3), (VEC_SIZE * 3)(%rdi)
-	VMOVU	%VEC(4), -VEC_SIZE(%rdi, %rdx)
+	VMOVU	%VEC(0), -VEC_SIZE(%rdi, %rdx)
 	VMOVU	%VEC(5), -(VEC_SIZE * 2)(%rdi, %rdx)
 	VMOVU	%VEC(6), -(VEC_SIZE * 3)(%rdi, %rdx)
 	VMOVU	%VEC(7), -(VEC_SIZE * 4)(%rdi, %rdx)
 	VZEROUPPER_RETURN
-
-	.p2align 4,, 11
+	.p2align 4,, 6
 L(last_4x_vec):
 	/* Copy from 2 * VEC + 1 to 4 * VEC, inclusively.  */
 	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(2)
 	VMOVU	-(VEC_SIZE * 2)(%rsi, %rdx), %VEC(3)
-	VMOVU	%VEC(0), (%rdi)
+
+	VMOVU	%VEC(4), (%rdi)
 	VMOVU	%VEC(1), VEC_SIZE(%rdi)
 	VMOVU	%VEC(2), -VEC_SIZE(%rdi, %rdx)
 	VMOVU	%VEC(3), -(VEC_SIZE * 2)(%rdi, %rdx)
@@ -462,9 +482,9 @@ L(last_4x_vec):
 L(more_8x_vec):
 	movq	%rdi, %rcx
 	subq	%rsi, %rcx
-	/* Go to backwards temporal copy if overlap no matter what as
-	   backward REP MOVSB is slow and we don't want to use NT stores if
-	   there is overlap.  */
+	/* Go to backwards temporal copy if dst > src and overlap no matter
+	   what as bwe don't want to use NT stores if there is overlap and
+	   cannot do forward 4x loop for correctness.  */
 	cmpq	%rdx, %rcx
 	/* L(more_8x_vec_backward_check_nop) checks for src == dst.  */
 	jb	L(more_8x_vec_backward_check_nop)
@@ -475,7 +495,6 @@ L(more_8x_vec):
 	cmp	__x86_shared_non_temporal_threshold(%rip), %RDX_LP
 	ja	L(large_memcpy_2x)
 #endif
-
 	/* To reach this point there cannot be overlap and dst > src. So
 	   check for overlap and src > dst in which case correctness requires
 	   forward copy. Otherwise decide between backward/forward copy
@@ -501,7 +520,6 @@ L(more_8x_vec):
 	   overlap or from short distance movsb.  */
 L(more_8x_vec_forward):
 	/* Load last 4 * VEC to support overlapping addresses.  */
-	VMOVU	(%rsi), %VEC(4)
 	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(5)
 	VMOVU	-(VEC_SIZE * 2)(%rsi, %rdx), %VEC(6)
 	/* Save begining of dst.  */
@@ -549,14 +567,21 @@ L(loop_4x_vec_forward):
 L(nop_backward):
 	VZEROUPPER_RETURN
 
-	.p2align 4,, 10
+	.p2align 4,, 6
+#if defined USE_MULTIARCH && IS_IN (libc)
+L(more_8x_vec_backward_check_nop_load):
+	/* Need a load here for potential entry from L(movsb) avoiding
+	   overlapping backward copy if VEC(4) wasnt already loaded.  */
+# if !AVOID_SHORT_DISTANCE_REP_MOVSB && !ALIGN_MOVSB
+	VMOVU	(%rsi), %VEC(4)
+# endif
+#endif
 L(more_8x_vec_backward_check_nop):
 	testq	%rcx, %rcx
 	jz	L(nop_backward)
 L(more_8x_vec_backward):
 	/* Load the first 4 * VEC and last VEC to support overlapping
 	   addresses.  */
-	VMOVU	(%rsi), %VEC(4)
 	VMOVU	VEC_SIZE(%rsi), %VEC(5)
 	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(6)
 	/* Begining of region for 4x backward copy stored in rcx.  */
@@ -596,24 +621,63 @@ L(loop_4x_vec_backward):
 	VZEROUPPER_RETURN
 
 #if defined USE_MULTIARCH && IS_IN (libc)
+# if ALIGN_MOVSB
+L(skip_short_movsb_check):
+#  if MOVSB_ALIGN_TO > VEC_SIZE
+	VMOVU	VEC_SIZE(%rsi), %VEC(5)
+#  endif
+#  if MOVSB_ALIGN_TO > (VEC_SIZE * 2)
+#   error Unsupported MOVSB_ALIGN_TO
+#  endif
+	/* If CPU does not have FSRM two options for aligning. Align src if
+	   dst and src 4k alias. Otherwise align dst.  */
+	testl	$(PAGE_SIZE - 512), %ecx
+	jnz	L(movsb_align_dst)
+	/* rcx already has dst - src.  */
+	movq	%rcx, %r9
+	/* Add src to len. Subtract back after src aligned. -1 because src
+	   is initially aligned to MOVSB_ALIGN_TO - 1.  */
+	leaq	-(1)(%rsi, %rdx), %rcx
+	/* Inclusively align src to MOVSB_ALIGN_TO - 1.  */
+	orq	$(MOVSB_ALIGN_TO - 1), %rsi
+	/* Restore dst and len adjusted with new values for aligned dst.  */
+	leaq	1(%rsi, %r9), %rdi
+	subq	%rsi, %rcx
+	/* Finish aligning src.  */
+	incq	%rsi
+
+	rep	movsb
+
+	/* Store VECs loaded for aligning.  */
+	VMOVU	%VEC(4), (%r8)
+#  if MOVSB_ALIGN_TO > VEC_SIZE
+	VMOVU	%VEC(5), VEC_SIZE(%r8)
+#  endif
+	VZEROUPPER_RETURN
+# endif
+	.p2align 4,, 6
 L(movsb):
 	movq	%rdi, %rcx
 	subq	%rsi, %rcx
-	/* Go to backwards temporal copy if overlap no matter what as
-	   backward REP MOVSB is slow and we don't want to use NT stores if
+	/* Go to backwards temporal copy dst > src if overlap no matter what
+	   as backward REP MOVSB is slow and we don't want to use NT stores if
 	   there is overlap.  */
 	cmpq	%rdx, %rcx
-	/* L(more_8x_vec_backward_check_nop) checks for src == dst.  */
-	jb	L(more_8x_vec_backward_check_nop)
-
-	/* If above x86_rep_movsb_stop_threshold most likely is candidate
+	/* L(more_8x_vec_backward_check_nop_load) checks for src == dst.  */
+	jb	L(more_8x_vec_backward_check_nop_load)
+# if ALIGN_MOVSB
+	/* Store dst for use after rep movsb.  */
+	movq	%rdi, %r8
+# endif
+	/* If above __x86_rep_movsb_stop_threshold most likely is candidate
 	   for NT moves aswell.  */
 	cmp	__x86_rep_movsb_stop_threshold(%rip), %RDX_LP
 	jae	L(large_memcpy_2x_check)
-# if AVOID_SHORT_DISTANCE_REP_MOVSB
+# if AVOID_SHORT_DISTANCE_REP_MOVSB || ALIGN_MOVSB
 	/* Only avoid short movsb if CPU has FSRM.  */
 	testl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
 	jz	L(skip_short_movsb_check)
+#  if AVOID_SHORT_DISTANCE_REP_MOVSB
 	/* Avoid "rep movsb" if RCX, the distance between source and
 	   destination, is N*4GB + [1..63] with N >= 0.  */
 
@@ -622,37 +686,72 @@ L(movsb):
 	   [-63, 0]. Use unsigned comparison with -64 check for that case.  */
 	cmpl	$-64, %ecx
 	ja	L(more_8x_vec_forward)
-L(skip_short_movsb_check):
+#  endif
 # endif
+# if ALIGN_MOVSB
+#  if MOVSB_ALIGN_TO > VEC_SIZE
+	VMOVU	VEC_SIZE(%rsi), %VEC(5)
+#  endif
+#  if MOVSB_ALIGN_TO > (VEC_SIZE * 2)
+#   error Unsupported MOVSB_ALIGN_TO
+#  endif
+	/* Fall through means cpu has FSRM. In that case exclusively align
+	   destination.  */
+L(movsb_align_dst):
+	/* Subtract dst from src. Add back after dst aligned.  */
+	subq	%rdi, %rsi
+	/* Exclusively align dst to MOVSB_ALIGN_TO (64).  */
+	addq	$(MOVSB_ALIGN_TO - 1), %rdi
+	/* Finish aligning dst.  */
+	andq	$-(MOVSB_ALIGN_TO), %rdi
+	/* Add dst to len. Subtract back after dst aligned.  */
+	leaq	(%r8, %rdx), %rcx
+	/* Restore src and len adjusted with new values for aligned dst.  */
+	addq	%rdi, %rsi
+	subq	%rdi, %rcx
+
+	rep	movsb
+
+	/* Store VECs loaded for aligning.  */
+	VMOVU	%VEC(4), (%r8)
+#  if MOVSB_ALIGN_TO > VEC_SIZE
+	VMOVU	%VEC(5), VEC_SIZE(%r8)
+#  endif
+	VZEROUPPER_RETURN
+# else
+L(skip_short_movsb_check):
 	mov	%RDX_LP, %RCX_LP
 	rep	movsb
 	ret
+# endif
 #endif
 
 #if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
 	.p2align 4
+# ifdef USE_MULTIARCH
+	/* L(large_memcpy_2x_check) has only entry point from L(movsb) so
+	   only include it for USE_MULTIARCH build.  */
 L(large_memcpy_2x_check):
+	/* Need a load here from L(movsb) entry if VEC(4) wasn't already
+	   loaded.  */
+#  if !AVOID_SHORT_DISTANCE_REP_MOVSB && !ALIGN_MOVSB
+	VMOVU	(%rsi), %VEC(4)
+#  endif
 	cmp	__x86_shared_non_temporal_threshold(%rip), %RDX_LP
 	jb	L(more_8x_vec_forward)
+# endif
 L(large_memcpy_2x):
-	/* Compute absolute value of difference between source and
-	   destination.  */
-	movq	%rdi, %r9
-	subq	%rsi, %r9
-	movq	%r9, %r8
-	leaq	-1(%r9), %rcx
-	sarq	$63, %r8
-	xorq	%r8, %r9
-	subq	%r8, %r9
-	/* Don't use non-temporal store if there is overlap between
-	   destination and source since destination may be in cache when source
-	   is loaded.  */
-	cmpq	%r9, %rdx
-	ja	L(more_8x_vec_check)
+	/* To reach this point it is impossible for dst > src and overlap.
+	   Remaining to check is src > dst and overlap. rcx already contains
+	   dst - src. Negate rcx to get src - dst. If length > rcx then there
+	   is overlap and forward copy is best.  */
+	negq	%rcx
+	cmpq	%rcx, %rdx
+	ja	L(more_8x_vec_forward)
 
 	/* Cache align destination. First store the first 64 bytes then
-	   adjust alignments.  */
-	VMOVU	(%rsi), %VEC(8)
+	   adjust alignments. Skip first VEC as must already have been loaded.
+	 */
 # if VEC_SIZE < 64
 	VMOVU	VEC_SIZE(%rsi), %VEC(9)
 #  if VEC_SIZE < 32
@@ -660,7 +759,7 @@ L(large_memcpy_2x):
 	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(11)
 #  endif
 # endif
-	VMOVU	%VEC(8), (%rdi)
+	VMOVU	%VEC(4), (%rdi)
 # if VEC_SIZE < 64
 	VMOVU	%VEC(9), VEC_SIZE(%rdi)
 #  if VEC_SIZE < 32
@@ -832,7 +931,6 @@ L(large_memcpy_4x_end):
 	VZEROUPPER_RETURN
 #endif
 END (MEMMOVE_SYMBOL (__memmove, unaligned_erms))
-
 #if IS_IN (libc)
 # ifdef USE_MULTIARCH
 strong_alias (MEMMOVE_SYMBOL (__memmove, unaligned_erms),
-- 
2.25.1

