From b96348dce18b735e8ef931ec0619e25b76225e94 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Sun, 22 Aug 2021 02:43:45 -0400
Subject: [PATCH 7/9] early align for movsb

---
 sysdeps/x86_64/multiarch/memmove-vec-unaligned.S | 10 +++++++++-
 1 file changed, 9 insertions(+), 1 deletion(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S
index 15e797d172..8cae69653d 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S
@@ -616,6 +616,12 @@ L(movsb):
 #  if VEC_SIZE > 16
 	cmpl	$-64, %ecx
 	jae	L(gt_8x_forward)
+#   if EARLY_ALIGN
+	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
+#    if MOVSB_ALIGN_TO > VEC_SIZE
+	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
+#    endif
+#   endif
 #  endif
 	subq	%rdi, %rsi
 	leaq	(%rdi, %rdx), %rcx
@@ -626,9 +632,11 @@ L(movsb):
 
 	rep	movsb
 
+#  if !EARLY_ALIGN || VEC_SIZE == 16
 	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
-#  if MOVSB_ALIGN_TO > VEC_SIZE
+#   if MOVSB_ALIGN_TO > VEC_SIZE
 	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
+#   endif
 #  endif
 	VZEROUPPER_RETURN
 # else
-- 
2.25.1

