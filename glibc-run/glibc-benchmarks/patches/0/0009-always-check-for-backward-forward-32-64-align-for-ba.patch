From 35b5879eb7e1a579afc56372c4ca10a0c006e226 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Sun, 22 Aug 2021 07:20:21 -0400
Subject: [PATCH 09/10] always check for backward/forward, 32/64 align for
 backward

---
 sysdeps/x86/dl-cacheinfo.h                    |  2 +-
 .../x86_64/multiarch/memmove-vec-unaligned.S  | 25 ++++++++-----------
 2 files changed, 11 insertions(+), 16 deletions(-)

diff --git a/sysdeps/x86/dl-cacheinfo.h b/sysdeps/x86/dl-cacheinfo.h
index e6c94dfd02..1d396a4dcd 100644
--- a/sysdeps/x86/dl-cacheinfo.h
+++ b/sysdeps/x86/dl-cacheinfo.h
@@ -894,7 +894,7 @@ dl_init_cacheinfo (struct cpu_features *cpu_features)
   /* NB: The default REP MOVSB threshold is 2112 on processors with fast
      short REP MOVSB (FSRM).  */
   if (CPU_FEATURE_USABLE_P (cpu_features, FSRM))
-    rep_movsb_threshold = 2112;
+    rep_movsb_threshold = 3112;
 
   unsigned long int rep_movsb_stop_threshold;
   /* ERMS feature is implemented from AMD Zen3 architecture and it is
diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S
index c084d6a313..69de48dbd2 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S
@@ -62,6 +62,7 @@
 
 #include "memmove-conf.S"
 #ifndef _BENCHMARK_CONF_H_
+# define VERSION	0
 # define ALIGN_MOVSB	1
 # define LOOP_ALIGN_CONF	64
 # define MOVSB_ALIGN_CONF	64
@@ -498,9 +499,11 @@ L(loop_4x_forward):
 #if (LOOP_4X_ALIGN_TO > VEC_SIZE)
 	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
 #endif
-
+L(nop2):
 	VZEROUPPER_RETURN
-
+L(gt_8x_backward_check_nop):
+	testq	%rcx, %rcx
+	jz	L(nop2)
 	.p2align 4
 L(gt_8x_backward):
 #if MOVSB_ALIGN_TO < (VEC_SIZE)
@@ -514,7 +517,7 @@ L(gt_8x_backward):
 
 	VMOVU	(VEC_SIZE * -1)(%rsi, %rdx), %VEC(15)
 
-#if VERSION == 1
+#if VERSION == 1 && LOOP_4X_ALIGN_TO > VEC_SIZE
 	VMOVU	(VEC_SIZE * -2)(%rsi, %rdx), %VEC(14)
 #endif
 
@@ -533,7 +536,6 @@ L(gt_8x_backward):
 	/* Readjust src (rsi).  */
 	addq	%rdi, %rsi
 	/* Loop 4x VEC at a time copy backward from src (rsi) to dst (rdi).  */
-
 	.p2align 4
 L(loop_4x_backward):
 	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(4)
@@ -561,7 +563,7 @@ L(loop_4x_backward):
 	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
 
 	/* Store VECs loaded for aligning dst (rdi).  */
-#if VERSION == 1
+#if VERSION == 1 && LOOP_4X_ALIGN_TO > VEC_SIZE
 	VMOVU	%VEC(14), (VEC_SIZE * -2)(%r8, %rdx)
 #endif
 	VMOVU	%VEC(15), (VEC_SIZE * -1)(%r8, %rdx)
@@ -573,7 +575,7 @@ L(movsb):
 	movq	%rdi, %rcx
 	subq	%rsi, %rcx
 	cmpq	%rdx, %rcx
-	jb	L(gt_8x_backward)
+	jb	L(gt_8x_backward_check_nop)
 
 	/* If size (rdx) > __x86_rep_movsb_stop_threshold go to large copy.  */
 
@@ -597,12 +599,6 @@ L(movsb):
 #  if VEC_SIZE > 16
 	cmpl	$-64, %ecx
 	jae	L(gt_8x_forward)
-#   if EARLY_ALIGN
-	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
-#    if MOVSB_ALIGN_TO > VEC_SIZE
-	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
-#    endif
-#   endif
 #  endif
 	subq	%rdi, %rsi
 	leaq	(%rdi, %rdx), %rcx
@@ -613,12 +609,11 @@ L(movsb):
 
 	rep	movsb
 
-#  if !EARLY_ALIGN || VEC_SIZE == 16
 	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
-#   if MOVSB_ALIGN_TO > VEC_SIZE
+#  if MOVSB_ALIGN_TO > VEC_SIZE
 	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
-#   endif
 #  endif
+
 	VZEROUPPER_RETURN
 # else
 	testl	$(PAGE_SIZE - 512), %ecx
-- 
2.25.1

