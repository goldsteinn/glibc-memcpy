From e9ef970808a4dd33557e1dff726f7de460160b68 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Sat, 21 Aug 2021 19:22:09 -0400
Subject: [PATCH 6/9] Trying with always backward

---
 .../x86_64/multiarch/memmove-vec-unaligned.S  | 69 ++++++++++---------
 1 file changed, 38 insertions(+), 31 deletions(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S
index bce14210fb..15e797d172 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned.S
@@ -65,7 +65,6 @@
 # define ALIGN_MOVSB	1
 # define LOOP_ALIGN_CONF	64
 # define MOVSB_ALIGN_CONF	64
-# define ORDER	0
 #endif
 #ifdef USE_WITH_FSRM
 # define ALIGN_MOVSB_EXCLUSIVE	1
@@ -435,19 +434,28 @@ L(copy_gt_2x_leq_4x):
 	VMOVU	%VEC(1), (VEC_SIZE * 1)(%rdi)
 	VMOVU	%VEC(14), (VEC_SIZE * -2)(%rdi, %rdx)
 	VMOVU	%VEC(15), (VEC_SIZE * -1)(%rdi, %rdx)
+#if VERSION == 0
+L(nop):
+#endif
 	VZEROUPPER_RETURN
 #if VEC_SIZE != 16
 	.p2align 4
 #endif
-
+#if VERSION == 1
+	.p2align 5
+#else
 	.p2align 4
+#endif
 L(gt_8x):
+#if VERSION == 1
 	movq	%rdi, %rcx
 	subq	%rsi, %rcx
 	movq	%rdi, %r8
 	cmpq	%rdx, %rcx
 	jb	L(gt_8x_backward)
-
+#else
+	movq	%rdi, %r8
+#endif
 #ifdef USE_WITH_ERMS
 	cmpq	__x86_rep_movsb_threshold(%rip), %rdx
 	ja	L(movsb)
@@ -459,23 +467,22 @@ L(gt_8x):
 	/* Entry if rdx is greater than non-temporal threshold but there is
 	   overlap.  */
 L(gt_8x_forward):
+#if VERSION == 0
+	cmpq	%rsi, %rdi
+	ja	L(gt_8x_backward)
+	je	L(nop)
+#endif
 #if MOVSB_ALIGN_TO < (VEC_SIZE)
 	VMOVU	(VEC_SIZE * 0)(%rsi), %VEC(0)
 #endif
 #if MOVSB_ALIGN_TO < (VEC_SIZE * 2) && (LOOP_4X_ALIGN_TO > VEC_SIZE)
 	VMOVU	(VEC_SIZE * 1)(%rsi), %VEC(1)
 #endif
-#if ORDER < 2
 	VMOVU	(VEC_SIZE * -4)(%rsi, %rdx), %VEC(12)
 	VMOVU	(VEC_SIZE * -3)(%rsi, %rdx), %VEC(13)
 	VMOVU	(VEC_SIZE * -2)(%rsi, %rdx), %VEC(14)
 	VMOVU	(VEC_SIZE * -1)(%rsi, %rdx), %VEC(15)
-#else
-	VMOVU	(VEC_SIZE * -1)(%rsi, %rdx), %VEC(15)
-	VMOVU	(VEC_SIZE * -2)(%rsi, %rdx), %VEC(14)
-	VMOVU	(VEC_SIZE * -3)(%rsi, %rdx), %VEC(13)
-	VMOVU	(VEC_SIZE * -4)(%rsi, %rdx), %VEC(12)
-#endif
+
 	subq	%rdi, %rsi
 	leaq	(VEC_SIZE * -4)(%rdi, %rdx), %rdx
 	orq	$(LOOP_4X_ALIGN_TO - 1), %rdi
@@ -501,32 +508,17 @@ L(loop_4x_forward):
 
 
 
-#if (ORDER & 1)
 	VMOVU	%VEC(12), (VEC_SIZE * 0)(%rdx)
 	VMOVU	%VEC(13), (VEC_SIZE * 1)(%rdx)
 	VMOVU	%VEC(14), (VEC_SIZE * 2)(%rdx)
 	VMOVU	%VEC(15), (VEC_SIZE * 3)(%rdx)
 	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
-# if (LOOP_4X_ALIGN_TO > VEC_SIZE)
-	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
-# endif
-#else
-	VMOVU	%VEC(15), (VEC_SIZE * 3)(%rdx)
-	VMOVU	%VEC(14), (VEC_SIZE * 2)(%rdx)
-	VMOVU	%VEC(13), (VEC_SIZE * 1)(%rdx)
-	VMOVU	%VEC(12), (VEC_SIZE * 0)(%rdx)
-	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
-# if (LOOP_4X_ALIGN_TO > VEC_SIZE)
+#if (LOOP_4X_ALIGN_TO > VEC_SIZE)
 	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
-# endif
-
-
-
 #endif
-
-
-
+#if VERSION == 1
 L(nop):
+#endif
 	VZEROUPPER_RETURN
 
 	.p2align 4
@@ -548,7 +540,9 @@ L(gt_8x_backward):
 	/* Subtract dst (rdi) from src (rsi) before aligning dst (rdi) so that
 	   src (rsi) can be properly adjusted.  */
 	subq	%rdi, %rsi
+#if VERSION == 1
 	jz	L(nop)
+#endif
 	/* Set dst (rdi) as end of region. Set 4x VEC from begining of region as
 	   those VECs where already loaded. -1 for aligning dst (rdi).  */
 	leaq	(VEC_SIZE * -4 + -1)(%rdi, %rdx), %rdi
@@ -557,6 +551,7 @@ L(gt_8x_backward):
 	/* Readjust src (rsi).  */
 	addq	%rdi, %rsi
 	/* Loop 4x VEC at a time copy backward from src (rsi) to dst (rdi).  */
+
 	.p2align 4
 L(loop_4x_backward):
 	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(4)
@@ -578,10 +573,10 @@ L(loop_4x_backward):
 	/* Store begining of region.  */
 
 	/* NB: rax was set to dst on function entry.  */
-	VMOVU	%VEC(3), (VEC_SIZE * 3)(%r8)
-	VMOVU	%VEC(2), (VEC_SIZE * 2)(%r8)
-	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
 	VMOVU	%VEC(0), (VEC_SIZE * 0)(%r8)
+	VMOVU	%VEC(1), (VEC_SIZE * 1)(%r8)
+	VMOVU	%VEC(2), (VEC_SIZE * 2)(%r8)
+	VMOVU	%VEC(3), (VEC_SIZE * 3)(%r8)
 
 	/* Store VECs loaded for aligning dst (rdi).  */
 #if (LOOP_4X_ALIGN_TO > VEC_SIZE)
@@ -593,6 +588,12 @@ L(loop_4x_backward):
 #ifdef USE_WITH_ERMS
 	.p2align 4
 L(movsb):
+# if VERSION == 0
+	movq	%rdi, %rcx
+	subq	%rsi, %rcx
+	cmpq	%rdx, %rcx
+	jb	L(gt_8x_backward)
+# endif
 	/* If size (rdx) > __x86_rep_movsb_stop_threshold go to large copy.  */
 
 	/* NB: __x86_rep_movsb_stop_threshold in range
@@ -674,6 +675,12 @@ L(large_memcpy_2x):
 # ifdef USE_WITH_ERMS
 	cmpq	__x86_shared_non_temporal_threshold(%rip), %rdx
 	jb	L(gt_8x_forward)
+# endif
+# if VERSION == 0
+	movq	%rdi, %rcx
+	subq	%rsi, %rcx
+	cmpq	%rdx, %rcx
+	jb	L(gt_8x_backward)
 # endif
 	/* We want to use temporal copies if there is overlap constructive cache
 	   interference between loading from src and storing to dst.  */
-- 
2.25.1

