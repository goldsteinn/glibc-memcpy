From 7d4f29d89c1f74fe519d2d1debddf3b8acc4d726 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Sun, 22 Aug 2021 07:43:58 -0400
Subject: [PATCH 5/8] with movsb skimmed down

---
 .../multiarch/memmove-vec-unaligned-erms.S    | 39 ++++++-------------
 1 file changed, 12 insertions(+), 27 deletions(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
index 0960ccbc0a..db2e029927 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
@@ -409,36 +409,17 @@ L(copy_8_15):
 
 #if defined USE_MULTIARCH && IS_IN (libc)
 L(movsb):
-	cmp	__x86_rep_movsb_stop_threshold(%rip), %RDX_LP
-	jae	L(more_8x_vec)
-	cmpq	%rsi, %rdi
-	jb	1f
-	/* Source == destination is less common.  */
-	je	L(nop)
-	leaq	(%rsi, %rdx), %r9
-	cmpq	%r9, %rdi
-	/* Avoid slow backward REP MOVSB.  */
-	jb	L(more_8x_vec_backward)
-# if AVOID_SHORT_DISTANCE_REP_MOVSB
-	andl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
-	jz	3f
 	movq	%rdi, %rcx
 	subq	%rsi, %rcx
-	jmp	2f
-# endif
-1:
+	cmpq	%rdx, %rcx
+	jb	L(more_8x_vec_backward_check_nop)
+	cmp	__x86_rep_movsb_stop_threshold(%rip), %RDX_LP
 # if AVOID_SHORT_DISTANCE_REP_MOVSB
 	andl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
-	jz	3f
-	movq	%rsi, %rcx
-	subq	%rdi, %rcx
-2:
-	/* Avoid "rep movsb" if RCX, the distance between source and
-	   destination, is N*4GB + [1..63] with N >= 0.  */
-	cmpl	$63, %ecx
-	/* Avoid "rep movsb" if ECX <= 63.  */
-	jbe	L(more_2x_vec)
-3:
+	jz	L(skip_short_movsb_check)
+	cmpl	$-64, %ecx
+	jae	L(more_8x_vec_forward)
+L(skip_short_movsb_check):
 # endif
 	mov	%RDX_LP, %RCX_LP
 	rep	movsb
@@ -503,6 +484,7 @@ L(more_8x_vec_check):
 	ja	L(more_8x_vec_backward)
 	/* Source == destination is less common.  */
 	je	L(nop)
+L(more_8x_vec_forward):
 	/* Load the first VEC and last 4 * VEC to support overlapping addresses.
 	 */
 	VMOVU	(%rsi), %VEC(4)
@@ -549,8 +531,11 @@ L(loop_4x_vec_forward):
 	VMOVU	%VEC(8), -(VEC_SIZE * 3)(%rcx)
 	/* Store the first VEC.  */
 	VMOVU	%VEC(4), (%r11)
+L(nop2):
 	VZEROUPPER_RETURN
-
+L(more_8x_vec_backward_check_nop):
+	testq	%rcx, %rcx
+	jz	L(nop2)
 L(more_8x_vec_backward):
 	/* Load the first 4 * VEC and last VEC to support overlapping addresses.
 	 */
-- 
2.25.1

