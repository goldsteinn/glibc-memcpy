From db60ca7d6cc399c9574c6af17b9e40a176c9f6f9 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Mon, 23 Aug 2021 00:41:52 -0400
Subject: [PATCH 13/13] align loop more intelligently

---
 .../x86_64/multiarch/memmove-vec-unaligned-erms.S    | 12 +++++++++++-
 1 file changed, 11 insertions(+), 1 deletion(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
index 8c7da8d2b6..a48c80935e 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
@@ -405,8 +405,11 @@ L(copy_8_15):
 	COPY_8_16
 	ret
 #endif
-	.p2align 4
+
 #if defined USE_MULTIARCH && IS_IN (libc)
+# if VEC_SIZE == 16
+	.p2align 5
+# endif
 L(movsb_more_2x_vec):
 	cmp	__x86_rep_movsb_threshold(%rip), %RDX_LP
 	ja	L(movsb)
@@ -436,6 +439,7 @@ L(more_2x_vec):
 	VMOVU	%VEC(6), -(VEC_SIZE * 3)(%rdi, %rdx)
 	VMOVU	%VEC(7), -(VEC_SIZE * 4)(%rdi, %rdx)
 	VZEROUPPER_RETURN
+	.p2align 4,, 6
 L(last_4x_vec):
 	/* Copy from 2 * VEC + 1 to 4 * VEC, inclusively.  */
 	VMOVU	(%rsi), %VEC(0)
@@ -448,7 +452,13 @@ L(last_4x_vec):
 	VMOVU	%VEC(3), -(VEC_SIZE * 2)(%rdi, %rdx)
 L(nop):
 	VZEROUPPER_RETURN
+#if VEC_SIZE == 32
+    .p2align 4
+    nop
+    .p2align 4
+#else
 	.p2align 4,, 10
+#endif
 L(more_8x_vec):
 	/* Check if non-temporal move candidate.  */
 #if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
-- 
2.25.1

