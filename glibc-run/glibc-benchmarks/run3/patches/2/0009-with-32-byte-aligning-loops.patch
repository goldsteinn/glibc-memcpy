From e4889e177037e596ad116558ff820084f82166c3 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Sun, 22 Aug 2021 18:47:18 -0400
Subject: [PATCH 09/11] with 32 byte aligning loops

---
 .../x86_64/multiarch/memmove-avx-unaligned-erms.S  |  2 +-
 .../multiarch/memmove-avx512-unaligned-erms.S      |  2 +-
 .../x86_64/multiarch/memmove-evex-unaligned-erms.S |  2 +-
 .../x86_64/multiarch/memmove-sse2-unaligned-erms.S |  2 +-
 .../x86_64/multiarch/memmove-vec-unaligned-erms.S  | 14 ++++++--------
 5 files changed, 10 insertions(+), 12 deletions(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-avx-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-avx-unaligned-erms.S
index e195e93f15..a9c999afa7 100644
--- a/sysdeps/x86_64/multiarch/memmove-avx-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-avx-unaligned-erms.S
@@ -4,7 +4,7 @@
 # define VMOVNT		vmovntdq
 # define VMOVU		vmovdqu
 # define VMOVA		vmovdqa
-
+# define USE_WITH_AVX		1
 # define SECTION(p)		p##.avx
 # define MEMMOVE_SYMBOL(p,s)	p##_avx_##s
 
diff --git a/sysdeps/x86_64/multiarch/memmove-avx512-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-avx512-unaligned-erms.S
index 848848ab39..9b2f3b69f1 100644
--- a/sysdeps/x86_64/multiarch/memmove-avx512-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-avx512-unaligned-erms.S
@@ -25,7 +25,7 @@
 # define VMOVU		vmovdqu64
 # define VMOVA		vmovdqa64
 # define VZEROUPPER
-
+# define USE_WITH_AVX512		1
 # define SECTION(p)		p##.evex512
 # define MEMMOVE_SYMBOL(p,s)	p##_avx512_##s
 
diff --git a/sysdeps/x86_64/multiarch/memmove-evex-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-evex-unaligned-erms.S
index 0cbce8f944..646fa04442 100644
--- a/sysdeps/x86_64/multiarch/memmove-evex-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-evex-unaligned-erms.S
@@ -25,7 +25,7 @@
 # define VMOVU		vmovdqu64
 # define VMOVA		vmovdqa64
 # define VZEROUPPER
-
+# define USE_WITH_EVEX		1
 # define SECTION(p)		p##.evex
 # define MEMMOVE_SYMBOL(p,s)	p##_evex_##s
 
diff --git a/sysdeps/x86_64/multiarch/memmove-sse2-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-sse2-unaligned-erms.S
index d10121db57..4f0d263505 100644
--- a/sysdeps/x86_64/multiarch/memmove-sse2-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-sse2-unaligned-erms.S
@@ -21,7 +21,7 @@
 #else
 weak_alias (__mempcpy, mempcpy)
 #endif
-
+# define USE_WITH_SSE2		1
 #include <sysdeps/x86_64/memmove.S>
 
 #if defined SHARED && IS_IN (libc)
diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
index 9d34091754..8656e0fd96 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
@@ -457,6 +457,7 @@ L(more_2x_vec):
 	VMOVU	%VEC(6), -(VEC_SIZE * 3)(%rdi, %rdx)
 	VMOVU	%VEC(7), -(VEC_SIZE * 4)(%rdi, %rdx)
 	VZEROUPPER_RETURN
+	.p2align 4,, 6
 L(last_4x_vec):
 	/* Copy from 2 * VEC + 1 to 4 * VEC, inclusively.  */
 	VMOVU	(%rsi), %VEC(0)
@@ -467,9 +468,9 @@ L(last_4x_vec):
 	VMOVU	%VEC(1), VEC_SIZE(%rdi)
 	VMOVU	%VEC(2), -VEC_SIZE(%rdi, %rdx)
 	VMOVU	%VEC(3), -(VEC_SIZE * 2)(%rdi, %rdx)
-L(nop):    
+L(nop):
 	VZEROUPPER_RETURN
-	.p2align 4
+	.p2align 4,,10
 L(more_8x_vec):
 	/* Check if non-temporal move candidate.  */
 #if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
@@ -524,11 +525,9 @@ L(loop_4x_vec_forward):
 	VMOVU	%VEC(4), (%rcx)
 L(nop2):
 	VZEROUPPER_RETURN
-
 L(more_8x_vec_backward_check_nop):
 	testq	%rcx, %rcx
 	jz	L(nop2)
-	.p2align 4
 L(more_8x_vec_backward):
 	/* Load the first 4 * VEC and last VEC to support overlapping addresses.
 	 */
@@ -538,14 +537,13 @@ L(more_8x_vec_backward):
 	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(7)
 	VMOVU	-VEC_SIZE(%rsi, %rdx), %VEC(8)
 
-    
+
 	subq	%rdi, %rsi
-    movq    %rdi, %rcx
+	movq	%rdi, %rcx
 	leaq	(VEC_SIZE * -4 + -1)(%rdi, %rdx), %rdi
 	andq	$-(VEC_SIZE), %rdi
 	addq	%rdi, %rsi
-
-	.p2align 4
+	.p2align 4,,11
 L(loop_4x_vec_backward):
 	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(0)
 	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(1)
-- 
2.25.1

