From 8dc977319b7bb92ca930fb483300043ffd2417cd Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Sun, 22 Aug 2021 20:27:52 -0400
Subject: [PATCH 10/11] fix bug in movsb

---
 sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S | 10 +++++++---
 1 file changed, 7 insertions(+), 3 deletions(-)

diff --git a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
index 8656e0fd96..95f3d10bd8 100644
--- a/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
+++ b/sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
@@ -407,12 +407,14 @@ L(copy_8_15):
 #endif
 
 #if defined USE_MULTIARCH && IS_IN (libc)
+	// .p2align 4
 L(movsb):
 	movq	%rdi, %rcx
 	subq	%rsi, %rcx
 	cmpq	%rdx, %rcx
 	jb	L(more_8x_vec_backward_check_nop)
 	cmp	__x86_rep_movsb_stop_threshold(%rip), %RDX_LP
+	jae	L(more_8x_vec)
 # if AVOID_SHORT_DISTANCE_REP_MOVSB
 	andl	$X86_STRING_CONTROL_AVOID_SHORT_DISTANCE_REP_MOVSB, __x86_string_control(%rip)
 	jz	L(skip_short_movsb_check)
@@ -423,7 +425,6 @@ L(skip_short_movsb_check):
 	mov	%RDX_LP, %RCX_LP
 	rep	movsb
 	ret
-	.p2align 4
 #endif
 
 
@@ -470,7 +471,10 @@ L(last_4x_vec):
 	VMOVU	%VEC(3), -(VEC_SIZE * 2)(%rdi, %rdx)
 L(nop):
 	VZEROUPPER_RETURN
-	.p2align 4,,10
+#ifdef USE_WITH_EVEX
+    .p2align 5
+#endif
+	.p2align 4,, 10
 L(more_8x_vec):
 	/* Check if non-temporal move candidate.  */
 #if (defined USE_MULTIARCH || VEC_SIZE == 16) && IS_IN (libc)
@@ -543,7 +547,7 @@ L(more_8x_vec_backward):
 	leaq	(VEC_SIZE * -4 + -1)(%rdi, %rdx), %rdi
 	andq	$-(VEC_SIZE), %rdi
 	addq	%rdi, %rsi
-	.p2align 4,,11
+	.p2align 4,, 11
 L(loop_4x_vec_backward):
 	VMOVU	(VEC_SIZE * 3)(%rsi), %VEC(0)
 	VMOVU	(VEC_SIZE * 2)(%rsi), %VEC(1)
-- 
2.25.1

